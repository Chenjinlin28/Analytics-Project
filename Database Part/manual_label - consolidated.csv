entity,sentence,paragraph,sentence_original,label,class_ID
retailer,which gives the supplier 's profit as and the retailer 's profit as Proposition 1 ( New Good Price and Supply - Chain Profits ) :,"which gives the supplier's profit as and the retailer's profit as Proposition 1 (New Good Price and Supply-Chain Profits): (i) The optimal new good price, P^^n, in the monopolistic ""used good"" market is lower than the new good price in a monopolistic ""no used good"" market, P'^^. (ii) While the supplier's profits decrease with the establishment of a secondary market, there exists a ku such that for all ky > k^l'q^ -h'))/q((2 -k^jh + k^qj, the retailer's profits increase in a secondary market. ",which gives the supplier's profit as and the retailer's profit as Proposition 1 (New Good Price and Supply-Chain Profits):,background_information,3
Proposition 1,which gives the supplier 's profit as and the retailer 's profit as Proposition 1 ( New Good Price and Supply - Chain Profits ) :,"which gives the supplier's profit as and the retailer's profit as Proposition 1 (New Good Price and Supply-Chain Profits): (i) The optimal new good price, P^^n, in the monopolistic ""used good"" market is lower than the new good price in a monopolistic ""no used good"" market, P'^^. (ii) While the supplier's profits decrease with the establishment of a secondary market, there exists a ku such that for all ky > k^l'q^ -h'))/q((2 -k^jh + k^qj, the retailer's profits increase in a secondary market. ",which gives the supplier's profit as and the retailer's profit as Proposition 1 (New Good Price and Supply-Chain Profits):,belongs_to_article,1
Supply-Chain,which gives the supplier 's profit as and the retailer 's profit as Proposition 1 ( New Good Price and Supply - Chain Profits ) :,"which gives the supplier's profit as and the retailer's profit as Proposition 1 (New Good Price and Supply-Chain Profits): (i) The optimal new good price, P^^n, in the monopolistic ""used good"" market is lower than the new good price in a monopolistic ""no used good"" market, P'^^. (ii) While the supplier's profits decrease with the establishment of a secondary market, there exists a ku such that for all ky > k^l'q^ -h'))/q((2 -k^jh + k^qj, the retailer's profits increase in a secondary market. ",which gives the supplier's profit as and the retailer's profit as Proposition 1 (New Good Price and Supply-Chain Profits):,background_information,3
Petri net,"which can be equivalently represented with a Petri net without OR - transition , as illustrated in Fig .","which can be equivalently represented with a Petri net without OR-transition, as illustrated in Fig. 4. Often, we express different meanings with an OR-transition: that is, even if both p and p have a token, the firing should j k produce exactly one token in p . Then, we represent the situation as follows: ","which can be equivalently represented with a Petri net without OR-transition, as illustrated in Fig.",background_information,3
business processes,"which are sent by business processes accounts receivable and case informational perspectives (what) represent the products respectively , request the recording of the system performance , thereby indicating that the goal has been accomplished .","A simple example of a synthesis process for IA modeling is given in Fig. 7: an IA model for a small law firm planning its new computing system. The firm currently has 20 lawyers and 4 secretaries; the accounts receivable system is manual, and information about cases is not shared. Two goals are set: charge to the customer promptly and share information about cases. Two major business processes closely related to the IS are accounts receivable and case information on-line access. Actors participating in the system are LAWYER and SECRETARY. The client-server descriptions include objects on Novell LAN as well as GUI objects of the accounting software package ACCPAC and the e-mail system Pegasus. Message 1, which is sent by object class SECRETARY to the CHARGE event, represents the completion of a lawsuit. It triggers an accounts receivable process. which are sent by business processes accounts receivable and case information respectively, request the recording of the system performance, thereby indicating that the goal has been accomplished. ","which are sent by business processes accounts receivable and case information respectively, request the recording of the system performance, thereby indicating that the goal has been accomplished.",background_information,3
system performance,"which are sent by business processes accounts receivable and case informational perspectives (what) represent the products respectively , request the recording of the system performance , thereby indicating that the goal has been accomplished .","A simple example of a synthesis process for IA modeling is given in Fig. 7: an IA model for a small law firm planning its new computing system. The firm currently has 20 lawyers and 4 secretaries; the accounts receivable system is manual, and information about cases is not shared. Two goals are set: charge to the customer promptly and share information about cases. Two major business processes closely related to the IS are accounts receivable and case information on-line access. Actors participating in the system are LAWYER and SECRETARY. The client-server descriptions include objects on Novell LAN as well as GUI objects of the accounting software package ACCPAC and the e-mail system Pegasus. Message 1, which is sent by object class SECRETARY to the CHARGE event, represents the completion of a lawsuit. It triggers an accounts receivable process. which are sent by business processes accounts receivable and case information respectively, request the recording of the system performance, thereby indicating that the goal has been accomplished. ","which are sent by business processes accounts receivable and case information respectively, request the recording of the system performance, thereby indicating that the goal has been accomplished.",belongs_to_article,1
front-end,"which a hypertext system is a front - end , can be For example , in our present system , contexts may thought of as making inferences in some rather be set in which only certain entities ( documents , different domain . In our implementation , Max , commands , or models ) are available .","The purpose of contexts (also called filters) is and very useful, to set or remove certain default assumptions. 5 For example, if the applicable context is that for Bridge laws. A bridge law is a universally naive users, then the user interface system may quantified material conditional in which the simplify the display in certain ways, perhaps by expressions in the antededent are all in one eliminating acronyms in favor of their full expreslanguage (or domain of discourse)and the sions by hiding technical information in docuexpressions in the consequent are all in ments, or by pruning the number of button or another language (or domain of discourse), commands. Logically, we represent contexts by adding an extra argument to our basic predicates. In the present case, we should think of a What was, for example, simply a node, becomes a hypertext system as working by making inferences node in a given context. Also, because contexts in the hypertext domain of discouse. The underly-are individuated it is possible to reason about ing hypertext language, as we have seen illus-them. We do so in two main ways. trated in §2, is designed for talking about nodes, First, contexts may be used for mapping (translinks, buttons, and so on. An application, for forming) and filtering data objects in the system. which a hypertext system is a front-end, can be For example, in our present system, contexts may thought of as making inferences in some rather be set in which only certain entities (documents, different domain. In our implementation, Max, commands, or models) are available. The entities that domain is mathematical models. There, the that are available are determined during run time application software makes inferences in alan-by mapping certain information to a set of possiguage that describes models, variables, data, and ble entities, then filtering the set using contextual so on. ","which a hypertext system is a front-end, can be For example, in our present system, contexts may thought of as making inferences in some rather be set in which only certain entities (documents, different domain. In our implementation, Max, commands, or models) are available.",belongs_to_article,1
hypertext,"which a hypertext system is a front - end , can be For example , in our present system , contexts may thought of as making inferences in some rather be set in which only certain entities ( documents , different domain . In our implementation , Max , commands , or models ) are available .","The purpose of contexts (also called filters) is and very useful, to set or remove certain default assumptions. 5 For example, if the applicable context is that for Bridge laws. A bridge law is a universally naive users, then the user interface system may quantified material conditional in which the simplify the display in certain ways, perhaps by expressions in the antededent are all in one eliminating acronyms in favor of their full expreslanguage (or domain of discourse)and the sions by hiding technical information in docuexpressions in the consequent are all in ments, or by pruning the number of button or another language (or domain of discourse), commands. Logically, we represent contexts by adding an extra argument to our basic predicates. In the present case, we should think of a What was, for example, simply a node, becomes a hypertext system as working by making inferences node in a given context. Also, because contexts in the hypertext domain of discouse. The underly-are individuated it is possible to reason about ing hypertext language, as we have seen illus-them. We do so in two main ways. trated in §2, is designed for talking about nodes, First, contexts may be used for mapping (translinks, buttons, and so on. An application, for forming) and filtering data objects in the system. which a hypertext system is a front-end, can be For example, in our present system, contexts may thought of as making inferences in some rather be set in which only certain entities (documents, different domain. In our implementation, Max, commands, or models) are available. The entities that domain is mathematical models. There, the that are available are determined during run time application software makes inferences in alan-by mapping certain information to a set of possiguage that describes models, variables, data, and ble entities, then filtering the set using contextual so on. ","which a hypertext system is a front-end, can be For example, in our present system, contexts may thought of as making inferences in some rather be set in which only certain entities (documents, different domain. In our implementation, Max, commands, or models) are available.",belongs_to_article,1
group decision support system,which a group decision support system GDSS intended to aid multiple cooperating decision makers has become an important topic .,"The increasing complexity of the socio-economic environment makes it increasingly difficult for a single Ž . decision maker DM to consider all relevant aspects of a problem. As a result, many organizations employ groups in decision making. This trend also has important consequences for research on decision support, in Ž . which a group decision support system GDSS intended to aid multiple cooperating decision makers has become an important topic. ",which a group decision support system GDSS intended to aid multiple cooperating decision makers has become an important topic.,background_information,3
GDSS,which a group decision support system GDSS intended to aid multiple cooperating decision makers has become an important topic .,"The increasing complexity of the socio-economic environment makes it increasingly difficult for a single Ž . decision maker DM to consider all relevant aspects of a problem. As a result, many organizations employ groups in decision making. This trend also has important consequences for research on decision support, in Ž . which a group decision support system GDSS intended to aid multiple cooperating decision makers has become an important topic. ",which a group decision support system GDSS intended to aid multiple cooperating decision makers has become an important topic.,background_information,3
IS professionals,"whether , in fact , information systems professionals find it more difficult to reach senior ranks and , if this is true , what creates this situation ( e.g. the characteristics that make them successful within information systems make them less successful in senior ranks ; negative views of information systems as a service rather than profit centre ; or that senior managers choose others that resemble themselves , with former marketing managers choosing marketing managers ) .","whether, in fact, IS professionals find it more difficult to reach senior ranks and, if this is true, what creates this situation (e.g. the characteristics that make them successful within IS make them less successful in senior ranks; negative views of IS as a service rather than profit centre; or that senior managers choose others that resemble themselves, with former marketing managers choosing marketing managers). Assignment for overseas tasks for IS Information syslems personnel as expalriates professionals may represent an opportunity to gather a broader range of skills, thus increasing the probability of eventual ascension into senior ranks. However, it could also represent a distancing from central management and years that are lost for lobbying existing senior management. Long term study of IS personnel reaching into senior management might profit from considering the impact of overseas posting on the probability of IS personnel reaching senior management ranks. ","whether, in fact, IS professionals find it more difficult to reach senior ranks and, if this is true, what creates this situation (e.g. the characteristics that make them successful within IS make them less successful in senior ranks; negative views of IS as a service rather than profit centre; or that senior managers choose others that resemble themselves, with former marketing managers choosing marketing managers).",belongs_to_article,1
marketing,"whether , in fact , information systems professionals find it more difficult to reach senior ranks and , if this is true , what creates this situation ( e.g. the characteristics that make them successful within information systems make them less successful in senior ranks ; negative views of information systems as a service rather than profit centre ; or that senior managers choose others that resemble themselves , with former marketing managers choosing marketing managers ) .","whether, in fact, IS professionals find it more difficult to reach senior ranks and, if this is true, what creates this situation (e.g. the characteristics that make them successful within IS make them less successful in senior ranks; negative views of IS as a service rather than profit centre; or that senior managers choose others that resemble themselves, with former marketing managers choosing marketing managers). Assignment for overseas tasks for IS Information syslems personnel as expalriates professionals may represent an opportunity to gather a broader range of skills, thus increasing the probability of eventual ascension into senior ranks. However, it could also represent a distancing from central management and years that are lost for lobbying existing senior management. Long term study of IS personnel reaching into senior management might profit from considering the impact of overseas posting on the probability of IS personnel reaching senior management ranks. ","whether, in fact, IS professionals find it more difficult to reach senior ranks and, if this is true, what creates this situation (e.g. the characteristics that make them successful within IS make them less successful in senior ranks; negative views of IS as a service rather than profit centre; or that senior managers choose others that resemble themselves, with former marketing managers choosing marketing managers).",belongs_to_article,1
methodologies,whether the use of different methodologies has different effects on system development .,"1. Who uses a methodology and how often; 2. whether the use of methodologies plays an important Fenton et al (1994) suggest that rigorous experimentation is needed to evaluate methodologies and their role in the determination of the economics of system development, and what role they play; and effects on organizations, processes and products. However, in practice (Chatzoglou & Macaulay, 1996) devel-3. whether the use of different methodologies has different effects on system development. opers usually choose specific methodologies for all sorts of reasons (availability, ease to use, found to work for The underlying conceptual model supporting this survey is presented in Figure 1. It assumes that there are other projects) except probably the right one (that is, suitability for the type of project they are currently work-some factors which affect developers' decisions about the use of a methodology. The characteristics of this ing on). The results can be disastrous (Lindstrom, 1993). This paper is the second of a series which present the methodology will, in turn, affect the development process, its economics and the outputs of this process. It findings of an in-depth survey of current practice associated with the requirements stage of a project. The main also assumes that all other parameters that affect the development process, its economics and its output (such objective of the survey was to obtain information about as resources, management, team members, users etc) are third, the effect of methodologies on the number of iterations of the RCA process is examined. In the fourth, the constant. Although, this is a strong assumption, it can help us isolate the effect of the issue under examination effect of methodologies on the economics of the whole development and on the RCA process is analysed, and as well as its implications. ",whether the use of different methodologies has different effects on system development.,related_work,2
system development,whether the use of different methodologies has different effects on system development .,"1. Who uses a methodology and how often; 2. whether the use of methodologies plays an important Fenton et al (1994) suggest that rigorous experimentation is needed to evaluate methodologies and their role in the determination of the economics of system development, and what role they play; and effects on organizations, processes and products. However, in practice (Chatzoglou & Macaulay, 1996) devel-3. whether the use of different methodologies has different effects on system development. opers usually choose specific methodologies for all sorts of reasons (availability, ease to use, found to work for The underlying conceptual model supporting this survey is presented in Figure 1. It assumes that there are other projects) except probably the right one (that is, suitability for the type of project they are currently work-some factors which affect developers' decisions about the use of a methodology. The characteristics of this ing on). The results can be disastrous (Lindstrom, 1993). This paper is the second of a series which present the methodology will, in turn, affect the development process, its economics and the outputs of this process. It findings of an in-depth survey of current practice associated with the requirements stage of a project. The main also assumes that all other parameters that affect the development process, its economics and its output (such objective of the survey was to obtain information about as resources, management, team members, users etc) are third, the effect of methodologies on the number of iterations of the RCA process is examined. In the fourth, the constant. Although, this is a strong assumption, it can help us isolate the effect of the issue under examination effect of methodologies on the economics of the whole development and on the RCA process is analysed, and as well as its implications. ",whether the use of different methodologies has different effects on system development.,related_work,2
individual-level,"whether the predictors display curvilinear effects on these outcomes , as has been shown with some of the Unified Theory of Acceptance and Use of Technology predictors on individual - level outcomes ( e.g. , START_CITE Brown et al . , 2012 END_CITE CITE_b11 START_CITE Brown et al . , , 2014) Liew et al. END_CITE CITE_b12 Venkatesh & Goyal , 2010) Zhou et al. ) .","whether the predictors display curvilinear effects on these outcomes, as has been shown with some of the UTAUT predictors on individual-level outcomes (e.g., Brown et al., 2012 Brown et al., , 2014 Venkatesh & Goyal, 2010). ","whether the predictors display curvilinear effects on these outcomes, as has been shown with some of the UTAUT predictors on individual-level outcomes (e.g., Brown et al., 2012 Brown et al., , 2014 Venkatesh & Goyal, 2010).",related_work,2
UTAUT,"whether the predictors display curvilinear effects on these outcomes , as has been shown with some of the Unified Theory of Acceptance and Use of Technology predictors on individual - level outcomes ( e.g. , START_CITE Brown et al . , 2012 END_CITE CITE_b11 START_CITE Brown et al . , , 2014) Liew et al. END_CITE CITE_b12 Venkatesh & Goyal , 2010) Zhou et al. ) .","whether the predictors display curvilinear effects on these outcomes, as has been shown with some of the UTAUT predictors on individual-level outcomes (e.g., Brown et al., 2012 Brown et al., , 2014 Venkatesh & Goyal, 2010). ","whether the predictors display curvilinear effects on these outcomes, as has been shown with some of the UTAUT predictors on individual-level outcomes (e.g., Brown et al., 2012 Brown et al., , 2014 Venkatesh & Goyal, 2010).",related_work,2
crowdfunding,"whether equity crowdfunding democratizes access to funding for user entrepreneurs , and","Previous qualitative studies document that user entrepreneurs face significant difficulties when commercializing their innovations (Baldwin et al., 2006; Shah & Mody, 2014) ; important among the contributing factors are the barriers they face when accessing capital for commercialization and growth. Crowdfunding is suggested as a promising solution (Mollick & Robb, 2016) because it provides access to a large and diverse pool of investors, some of whom may be open to nontraditional user entrepreneurs. Along with the increasingly important role that user entrepreneurs play in the economy (Baldwin & von Hippel, 2011) , examining the role of user entrepreneurs in the context of equity crowdfunding is thus highly salient. 2 More specifically, the two research questions of this study are (1) whether equity crowdfunding democratizes access to funding for user entrepreneurs, and (2) whether discrimination against user entrepreneurs, if it exists in equity crowdfunding, is statistical discrimination. 3 In statistical discrimination, decision makers treat groups differently as a rational response to limited information rather than prejudice (Guzman & Kacperczyk, 2019) . In contrast, in taste-based discrimination, the differences in the treatment of groups are a result of prejudice or idiosyncratic dislike for a particular group (Becker, 1957; Moser, 2012) . Identifying whether a lower preference for user innovator firms, if it exists, is statistical or taste-based discrimination has implications for fundraising strategies. Specifically, in statistical discrimination, the differential treatment of groups can be mitigated with more reliable signals of performance (either positive or negative), and the negative perceptions of the stereotyped group can be mitigated by positive performance information (Guzman & Kacperczyk, 2019 ). ","whether equity crowdfunding democratizes access to funding for user entrepreneurs, and",belongs_to_article,1
crowdfunding,"whether discrimination against user entrepreneurs , if it exists in equity crowdfunding , is statistical discrimination .","Previous qualitative studies document that user entrepreneurs face significant difficulties when commercializing their innovations (Baldwin et al., 2006; Shah & Mody, 2014) ; important among the contributing factors are the barriers they face when accessing capital for commercialization and growth. Crowdfunding is suggested as a promising solution (Mollick & Robb, 2016) because it provides access to a large and diverse pool of investors, some of whom may be open to nontraditional user entrepreneurs. Along with the increasingly important role that user entrepreneurs play in the economy (Baldwin & von Hippel, 2011) , examining the role of user entrepreneurs in the context of equity crowdfunding is thus highly salient. 2 More specifically, the two research questions of this study are (1) whether equity crowdfunding democratizes access to funding for user entrepreneurs, and (2) whether discrimination against user entrepreneurs, if it exists in equity crowdfunding, is statistical discrimination. 3 In statistical discrimination, decision makers treat groups differently as a rational response to limited information rather than prejudice (Guzman & Kacperczyk, 2019) . In contrast, in taste-based discrimination, the differences in the treatment of groups are a result of prejudice or idiosyncratic dislike for a particular group (Becker, 1957; Moser, 2012) . Identifying whether a lower preference for user innovator firms, if it exists, is statistical or taste-based discrimination has implications for fundraising strategies. Specifically, in statistical discrimination, the differential treatment of groups can be mitigated with more reliable signals of performance (either positive or negative), and the negative perceptions of the stereotyped group can be mitigated by positive performance information (Guzman & Kacperczyk, 2019 ). ","whether discrimination against user entrepreneurs, if it exists in equity crowdfunding, is statistical discrimination.",belongs_to_article,1
GSS,"whether data was collected , and epistemology , and Ž GSS factors such as GSS type , time and place dimensions , GSS software and tools , and research .","In this study the GSS papers in these journals were identified, analyzed, classified, coded and recorded according to a scheme which has evolved from the previous study and the work of a number of authors in other research of this type. In addition to the 'demographics' of journal name, year, authors and affiliations, each paper was classified according Ž to general research factors including research type, whether it was longitudinal, stage of research, . whether data was collected, and epistemology , and Ž GSS factors such as GSS type, time and place dimensions, GSS software and tools, and research . focus . In addition, though not presented in this Ž study, data factors such as data type, instruments used, dependent and independent variables studied, application task, data source, and various field and . laboratory characteristics were identified and collected. ","whether data was collected, and epistemology , and Ž GSS factors such as GSS type, time and place dimensions, GSS software and tools, and research .",belongs_to_article,1
epistemology,"whether data was collected , and epistemology , and Ž GSS factors such as GSS type , time and place dimensions , GSS software and tools , and research .","In this study the GSS papers in these journals were identified, analyzed, classified, coded and recorded according to a scheme which has evolved from the previous study and the work of a number of authors in other research of this type. In addition to the 'demographics' of journal name, year, authors and affiliations, each paper was classified according Ž to general research factors including research type, whether it was longitudinal, stage of research, . whether data was collected, and epistemology , and Ž GSS factors such as GSS type, time and place dimensions, GSS software and tools, and research . focus . In addition, though not presented in this Ž study, data factors such as data type, instruments used, dependent and independent variables studied, application task, data source, and various field and . laboratory characteristics were identified and collected. ","whether data was collected, and epistemology , and Ž GSS factors such as GSS type, time and place dimensions, GSS software and tools, and research .",background_information,3
India,"whether achieving cost - efficiencies , often gained by outsourcing development activities to low - cost locations such as India , is more important than the investment in a long - term development of shared knowledge and reuse capabilities .","whether achieving cost-efficiencies, often gained by outsourcing development activities to low-cost locations such as India, is more important than the investment in a long-term development of shared knowledge and reuse capabilities. A hybrid approach, in which some development activities will be carried out jointly across the remote sites and other activities will be centralized and accomplished in one location, is another possibility. In terms of the capabilities needed to support a joint development effort of a particular component across several sites, based on the evidence from TCS and LeCroy, Table 2 offers project managers some capabilities in the areas of the inter-site coordination, communications, and knowledge management required for the sharing of knowledge and the reuse of components in CBD environments. Last but not the least, we suggest that managers attempt to appoint project members based on their shared histories of collaboration in their area of expertise. In this way, members of a dispersed team will know each other, and their respective and shared communication and coordination routines, from past projects. Through such staffing strategies, the costs associated with inter-site communication and coordination activities can be reduced, and the challenges associated with inter-site coordination and communication activities can be overcome while knowledge sharing and component reuse can be improved. ","whether achieving cost-efficiencies, often gained by outsourcing development activities to low-cost locations such as India, is more important than the investment in a long-term development of shared knowledge and reuse capabilities.",background_information,3
outsourcing,"whether achieving cost - efficiencies , often gained by outsourcing development activities to low - cost locations such as India , is more important than the investment in a long - term development of shared knowledge and reuse capabilities .","whether achieving cost-efficiencies, often gained by outsourcing development activities to low-cost locations such as India, is more important than the investment in a long-term development of shared knowledge and reuse capabilities. A hybrid approach, in which some development activities will be carried out jointly across the remote sites and other activities will be centralized and accomplished in one location, is another possibility. In terms of the capabilities needed to support a joint development effort of a particular component across several sites, based on the evidence from TCS and LeCroy, Table 2 offers project managers some capabilities in the areas of the inter-site coordination, communications, and knowledge management required for the sharing of knowledge and the reuse of components in CBD environments. Last but not the least, we suggest that managers attempt to appoint project members based on their shared histories of collaboration in their area of expertise. In this way, members of a dispersed team will know each other, and their respective and shared communication and coordination routines, from past projects. Through such staffing strategies, the costs associated with inter-site communication and coordination activities can be reduced, and the challenges associated with inter-site coordination and communication activities can be overcome while knowledge sharing and component reuse can be improved. ","whether achieving cost-efficiencies, often gained by outsourcing development activities to low-cost locations such as India, is more important than the investment in a long-term development of shared knowledge and reuse capabilities.",belongs_to_article,1
standard deviation,whereσ k is a consistent estimate of the standard deviation of ffiffiffi T p r k .,"whereσ k is a consistent estimate of the standard deviation of ffiffiffi T p r k . To avoid using the least favorable configuration in White [25] (i.e. imposing μ = 0 in Eq. (2)) and to reduce the adverse influence of the rules with large negative average returns on the rejection rate of the null hypothesis, Hansen suggested a different way to bootstrap the distribution of Λ T SPA [13] . For the kth rule, let P z * ;b k denote the sample average of the bth bootstrapped realization of the centered returns: ",whereσ k is a consistent estimate of the standard deviation of ffiffiffi T p r k .,background_information,3
sharing market,"whereα;α À ð1=6Þ>0 : However , the assumption p Ã ðr ; τÞ > p 1 implies that ðα = αÞ > δ = ð1 þ δÞ ; which does not hold for any δ 2 ð0 ; 1 . Thus , by contradiction of the counterfactual we obtain that the clearing price of an active sharing market must be "" moderate "" in the sense that it lies in the interval ½p 0 ; p 1 : □ Proof of Lemma 4 . Aggregating owners with ν 2 ½p ; 1 yields the supply : while owners with ν 2 ½p 0 ; p constitute a supply of : This yields the sharing supply , S ¼ S 10 þ S 11 , for any combination of retail price and sharing tariff that allow for an active ( i.e. , positive - trading - volume ) peer - to - peer exchange in the secondary market .","whereα;α À ð1=6Þ>0: However, the assumption p Ã ðr; τÞ>p 1 implies that ðα=αÞ>δ=ð1 þ δÞ; which does not hold for any δ 2 ð0; 1. Thus, by contradiction of the counterfactual we obtain that the clearing price of an active sharing market must be ""moderate"" in the sense that it lies in the interval ½p 0 ; p 1 :□ Proof of Lemma 4. Aggregating owners with ν 2 ½p; 1 yields the supply: while owners with ν 2 ½p 0 ; p constitute a supply of: This yields the sharing supply, S ¼ S 10 þ S 11 , for any combination of retail price and sharing tariff that allow for an active (i.e., positive-trading-volume) peer-to-peer exchange in the secondary market. At the moderate sharing price p 2 ½p 0 ; p 1 , the supply from medium-value owners is: ","whereα;α À ð1=6Þ>0: However, the assumption p Ã ðr; τÞ>p 1 implies that ðα=αÞ>δ=ð1 þ δÞ; which does not hold for any δ 2 ð0; 1. Thus, by contradiction of the counterfactual we obtain that the clearing price of an active sharing market must be ""moderate"" in the sense that it lies in the interval ½p 0 ; p 1 :□ Proof of Lemma 4. Aggregating owners with ν 2 ½p; 1 yields the supply: while owners with ν 2 ½p 0 ; p constitute a supply of: This yields the sharing supply, S ¼ S 10 þ S 11 , for any combination of retail price and sharing tariff that allow for an active (i.e., positive-trading-volume) peer-to-peer exchange in the secondary market.",belongs_to_article,1
peer-to-peer,"whereα;α À ð1=6Þ>0 : However , the assumption p Ã ðr ; τÞ > p 1 implies that ðα = αÞ > δ = ð1 þ δÞ ; which does not hold for any δ 2 ð0 ; 1 . Thus , by contradiction of the counterfactual we obtain that the clearing price of an active sharing market must be "" moderate "" in the sense that it lies in the interval ½p 0 ; p 1 : □ Proof of Lemma 4 . Aggregating owners with ν 2 ½p ; 1 yields the supply : while owners with ν 2 ½p 0 ; p constitute a supply of : This yields the sharing supply , S ¼ S 10 þ S 11 , for any combination of retail price and sharing tariff that allow for an active ( i.e. , positive - trading - volume ) peer - to - peer exchange in the secondary market .","whereα;α À ð1=6Þ>0: However, the assumption p Ã ðr; τÞ>p 1 implies that ðα=αÞ>δ=ð1 þ δÞ; which does not hold for any δ 2 ð0; 1. Thus, by contradiction of the counterfactual we obtain that the clearing price of an active sharing market must be ""moderate"" in the sense that it lies in the interval ½p 0 ; p 1 :□ Proof of Lemma 4. Aggregating owners with ν 2 ½p; 1 yields the supply: while owners with ν 2 ½p 0 ; p constitute a supply of: This yields the sharing supply, S ¼ S 10 þ S 11 , for any combination of retail price and sharing tariff that allow for an active (i.e., positive-trading-volume) peer-to-peer exchange in the secondary market. At the moderate sharing price p 2 ½p 0 ; p 1 , the supply from medium-value owners is: ","whereα;α À ð1=6Þ>0: However, the assumption p Ã ðr; τÞ>p 1 implies that ðα=αÞ>δ=ð1 þ δÞ; which does not hold for any δ 2 ð0; 1. Thus, by contradiction of the counterfactual we obtain that the clearing price of an active sharing market must be ""moderate"" in the sense that it lies in the interval ½p 0 ; p 1 :□ Proof of Lemma 4. Aggregating owners with ν 2 ½p; 1 yields the supply: while owners with ν 2 ½p 0 ; p constitute a supply of: This yields the sharing supply, S ¼ S 10 þ S 11 , for any combination of retail price and sharing tariff that allow for an active (i.e., positive-trading-volume) peer-to-peer exchange in the secondary market.",belongs_to_article,1
retail,"whereα;α À ð1=6Þ>0 : However , the assumption p Ã ðr ; τÞ > p 1 implies that ðα = αÞ > δ = ð1 þ δÞ ; which does not hold for any δ 2 ð0 ; 1 . Thus , by contradiction of the counterfactual we obtain that the clearing price of an active sharing market must be "" moderate "" in the sense that it lies in the interval ½p 0 ; p 1 : □ Proof of Lemma 4 . Aggregating owners with ν 2 ½p ; 1 yields the supply : while owners with ν 2 ½p 0 ; p constitute a supply of : This yields the sharing supply , S ¼ S 10 þ S 11 , for any combination of retail price and sharing tariff that allow for an active ( i.e. , positive - trading - volume ) peer - to - peer exchange in the secondary market .","whereα;α À ð1=6Þ>0: However, the assumption p Ã ðr; τÞ>p 1 implies that ðα=αÞ>δ=ð1 þ δÞ; which does not hold for any δ 2 ð0; 1. Thus, by contradiction of the counterfactual we obtain that the clearing price of an active sharing market must be ""moderate"" in the sense that it lies in the interval ½p 0 ; p 1 :□ Proof of Lemma 4. Aggregating owners with ν 2 ½p; 1 yields the supply: while owners with ν 2 ½p 0 ; p constitute a supply of: This yields the sharing supply, S ¼ S 10 þ S 11 , for any combination of retail price and sharing tariff that allow for an active (i.e., positive-trading-volume) peer-to-peer exchange in the secondary market. At the moderate sharing price p 2 ½p 0 ; p 1 , the supply from medium-value owners is: ","whereα;α À ð1=6Þ>0: However, the assumption p Ã ðr; τÞ>p 1 implies that ðα=αÞ>δ=ð1 þ δÞ; which does not hold for any δ 2 ð0; 1. Thus, by contradiction of the counterfactual we obtain that the clearing price of an active sharing market must be ""moderate"" in the sense that it lies in the interval ½p 0 ; p 1 :□ Proof of Lemma 4. Aggregating owners with ν 2 ½p; 1 yields the supply: while owners with ν 2 ½p 0 ; p constitute a supply of: This yields the sharing supply, S ¼ S 10 þ S 11 , for any combination of retail price and sharing tariff that allow for an active (i.e., positive-trading-volume) peer-to-peer exchange in the secondary market.",belongs_to_article,1
polynomial regression,whereâ is the additive score of a polynomial regression of ξ on a andẑ is the outcome of a universal regression with the two latent variables j and k as regressors on z jk i .Here a and z can be given by Eqs .,"Likewise, when all predictor variables are tapped into the CART model, nine topmost important variables (WGT_KG_DON, HIST_CIG_ DON, HIST_IV_DRUG_DON, ABO_MAT, AMAT, AGE_DON, DRMAT, GINT, HLAMAT) also belong to the top-ranked two composite variables, namely donor's profile and match level with a 10-fold cross-validated variable importance ranking approach. In the USM model, nonlinear relations were sought, and at 0.05 significance level transplant success was revealed to have a significant nonlinear relationship with the donor's profile. In Fig. 4 Interaction effect (IE) of two independent latent/composite variables (ξ j and ξ k ) on ξ i (shortly IE jk i ) is expressed as the portion of variable ξ i 's explained variance that can be attributed to the interaction between ξ j and ξ k and is given by Eq. (20) [8] . whereâ is the additive score of a polynomial regression of ξ on a andẑ is the outcome of a universal regression with the two latent variables j and k as regressors on z jk i .Here a and z can be given by Eqs. (21) and (22), respectively. where a j i is the change in ξ i caused by the additive effect of ξ j , f is the neural network function, and ξ 1 and ξ n are the latent variables. By setting the value of ξ j to its mean value ( ξ j ), the change in ξ i which is provided by ξ j can be captured. Similarly, Eq. (22) represents the change in ξ i caused by the interactive effect of ξ j and ξ k . z i jk = f i ξ 1 ; …; ξ j ; …; ξ k ; …; ξ l −f i ξ 1 ; …;ξ j ; …;ξ k ; …; ξ l Þ ð22Þ ",whereâ is the additive score of a polynomial regression of ξ on a andẑ is the outcome of a universal regression with the two latent variables j and k as regressors on z jk i .Here a and z can be given by Eqs.,background_information,3
latent variables,whereâ is the additive score of a polynomial regression of ξ on a andẑ is the outcome of a universal regression with the two latent variables j and k as regressors on z jk i .Here a and z can be given by Eqs .,"Likewise, when all predictor variables are tapped into the CART model, nine topmost important variables (WGT_KG_DON, HIST_CIG_ DON, HIST_IV_DRUG_DON, ABO_MAT, AMAT, AGE_DON, DRMAT, GINT, HLAMAT) also belong to the top-ranked two composite variables, namely donor's profile and match level with a 10-fold cross-validated variable importance ranking approach. In the USM model, nonlinear relations were sought, and at 0.05 significance level transplant success was revealed to have a significant nonlinear relationship with the donor's profile. In Fig. 4 Interaction effect (IE) of two independent latent/composite variables (ξ j and ξ k ) on ξ i (shortly IE jk i ) is expressed as the portion of variable ξ i 's explained variance that can be attributed to the interaction between ξ j and ξ k and is given by Eq. (20) [8] . whereâ is the additive score of a polynomial regression of ξ on a andẑ is the outcome of a universal regression with the two latent variables j and k as regressors on z jk i .Here a and z can be given by Eqs. (21) and (22), respectively. where a j i is the change in ξ i caused by the additive effect of ξ j , f is the neural network function, and ξ 1 and ξ n are the latent variables. By setting the value of ξ j to its mean value ( ξ j ), the change in ξ i which is provided by ξ j can be captured. Similarly, Eq. (22) represents the change in ξ i caused by the interactive effect of ξ j and ξ k . z i jk = f i ξ 1 ; …; ξ j ; …; ξ k ; …; ξ l −f i ξ 1 ; …;ξ j ; …;ξ k ; …; ξ l Þ ð22Þ ",whereâ is the additive score of a polynomial regression of ξ on a andẑ is the outcome of a universal regression with the two latent variables j and k as regressors on z jk i .Here a and z can be given by Eqs.,background_information,3
effect size,"wherer is the bare - bones meta - analysis effect size , N i is the sample size of each study , and r i is the correlation of each study .","wherer is the bare-bones meta-analysis effect size, N i is the sample size of each study, and r i is the correlation of each study. We performed this analysis for each of the hypotheses. A bare-bones meta-analysis considers the sample size of each study and therefore, creates a weighted average of the correlations. ","wherer is the bare-bones meta-analysis effect size, N i is the sample size of each study, and r i is the correlation of each study.",belongs_to_article,1
meta-analysis,"wherer is the bare - bones meta - analysis effect size , N i is the sample size of each study , and r i is the correlation of each study .","wherer is the bare-bones meta-analysis effect size, N i is the sample size of each study, and r i is the correlation of each study. We performed this analysis for each of the hypotheses. A bare-bones meta-analysis considers the sample size of each study and therefore, creates a weighted average of the correlations. ","wherer is the bare-bones meta-analysis effect size, N i is the sample size of each study, and r i is the correlation of each study.",belongs_to_article,1
logistic regression,"whereas the performance of logistic regression , neural network , and linear discriminant analysis are relatively invariant across the training and test sets : • logistic reasoning yields the minimum cost in most situations and neural network in some ; • in general , logistic regression is the most robust against changes in priors , whereas decision tree is the most sensitive ; • the approach we used for tuning the models is effective .","• in general, neural network, logistic regression, and linear discriminant analysis dominate decision tree and case-based reasoning/^NN; • in terms of aggregate performance (AUC), neural network and logistic regression are the best; • decision tree and ANN are prone to overfttting. whereas the performance of logistic regression, neural network, and linear discriminant analysis are relatively invariant across the training and test sets: • logistic reasoning yields the minimum cost in most situations and neural network in some; • in general, logistic regression is the most robust against changes in priors, whereas decision tree is the most sensitive; • the approach we used for tuning the models is effective. ","whereas the performance of logistic regression, neural network, and linear discriminant analysis are relatively invariant across the training and test sets: • logistic reasoning yields the minimum cost in most situations and neural network in some; • in general, logistic regression is the most robust against changes in priors, whereas decision tree is the most sensitive; • the approach we used for tuning the models is effective.",belongs_to_article,1
logistic,"whereas the performance of logistic regression , neural network , and linear discriminant analysis are relatively invariant across the training and test sets : • logistic reasoning yields the minimum cost in most situations and neural network in some ; • in general , logistic regression is the most robust against changes in priors , whereas decision tree is the most sensitive ; • the approach we used for tuning the models is effective .","• in general, neural network, logistic regression, and linear discriminant analysis dominate decision tree and case-based reasoning/^NN; • in terms of aggregate performance (AUC), neural network and logistic regression are the best; • decision tree and ANN are prone to overfttting. whereas the performance of logistic regression, neural network, and linear discriminant analysis are relatively invariant across the training and test sets: • logistic reasoning yields the minimum cost in most situations and neural network in some; • in general, logistic regression is the most robust against changes in priors, whereas decision tree is the most sensitive; • the approach we used for tuning the models is effective. ","whereas the performance of logistic regression, neural network, and linear discriminant analysis are relatively invariant across the training and test sets: • logistic reasoning yields the minimum cost in most situations and neural network in some; • in general, logistic regression is the most robust against changes in priors, whereas decision tree is the most sensitive; • the approach we used for tuning the models is effective.",belongs_to_article,1
linear discriminant analysis,"whereas the performance of logistic regression , neural network , and linear discriminant analysis are relatively invariant across the training and test sets : • logistic reasoning yields the minimum cost in most situations and neural network in some ; • in general , logistic regression is the most robust against changes in priors , whereas decision tree is the most sensitive ; • the approach we used for tuning the models is effective .","• in general, neural network, logistic regression, and linear discriminant analysis dominate decision tree and case-based reasoning/^NN; • in terms of aggregate performance (AUC), neural network and logistic regression are the best; • decision tree and ANN are prone to overfttting. whereas the performance of logistic regression, neural network, and linear discriminant analysis are relatively invariant across the training and test sets: • logistic reasoning yields the minimum cost in most situations and neural network in some; • in general, logistic regression is the most robust against changes in priors, whereas decision tree is the most sensitive; • the approach we used for tuning the models is effective. ","whereas the performance of logistic regression, neural network, and linear discriminant analysis are relatively invariant across the training and test sets: • logistic reasoning yields the minimum cost in most situations and neural network in some; • in general, logistic regression is the most robust against changes in priors, whereas decision tree is the most sensitive; • the approach we used for tuning the models is effective.",belongs_to_article,1
neural network,"whereas the performance of logistic regression , neural network , and linear discriminant analysis are relatively invariant across the training and test sets : • logistic reasoning yields the minimum cost in most situations and neural network in some ; • in general , logistic regression is the most robust against changes in priors , whereas decision tree is the most sensitive ; • the approach we used for tuning the models is effective .","• in general, neural network, logistic regression, and linear discriminant analysis dominate decision tree and case-based reasoning/^NN; • in terms of aggregate performance (AUC), neural network and logistic regression are the best; • decision tree and ANN are prone to overfttting. whereas the performance of logistic regression, neural network, and linear discriminant analysis are relatively invariant across the training and test sets: • logistic reasoning yields the minimum cost in most situations and neural network in some; • in general, logistic regression is the most robust against changes in priors, whereas decision tree is the most sensitive; • the approach we used for tuning the models is effective. ","whereas the performance of logistic regression, neural network, and linear discriminant analysis are relatively invariant across the training and test sets: • logistic reasoning yields the minimum cost in most situations and neural network in some; • in general, logistic regression is the most robust against changes in priors, whereas decision tree is the most sensitive; • the approach we used for tuning the models is effective.",belongs_to_article,1
decision tree,"whereas the performance of logistic regression , neural network , and linear discriminant analysis are relatively invariant across the training and test sets : • logistic reasoning yields the minimum cost in most situations and neural network in some ; • in general , logistic regression is the most robust against changes in priors , whereas decision tree is the most sensitive ; • the approach we used for tuning the models is effective .","• in general, neural network, logistic regression, and linear discriminant analysis dominate decision tree and case-based reasoning/^NN; • in terms of aggregate performance (AUC), neural network and logistic regression are the best; • decision tree and ANN are prone to overfttting. whereas the performance of logistic regression, neural network, and linear discriminant analysis are relatively invariant across the training and test sets: • logistic reasoning yields the minimum cost in most situations and neural network in some; • in general, logistic regression is the most robust against changes in priors, whereas decision tree is the most sensitive; • the approach we used for tuning the models is effective. ","whereas the performance of logistic regression, neural network, and linear discriminant analysis are relatively invariant across the training and test sets: • logistic reasoning yields the minimum cost in most situations and neural network in some; • in general, logistic regression is the most robust against changes in priors, whereas decision tree is the most sensitive; • the approach we used for tuning the models is effective.",belongs_to_article,1
Hypothesis 2,whereas the main effects of content facilitation on knowledge acquisition ( Hypothesis 2 ) were supported only with the data on ktiowledge commonality .,"Botb qualitative and quantitative data were collected in this study to test the hypotheses and to better understand tbe main (Hypotheses 1 and 2) and interaction (Hypothesis 3) effects of GSS and content facilitation on knowledge acquisition in terms of the two group process gains {group participation and quality of feedback), two group process losses {domination and communication barrier), cooperation in learning, and three knowledge structures {knowledge complexity, integration, and commonality). The main prediction of this study (see Figure 1) is tbat GSS and content facilitation enhance the effectiveness of knowledge acquisition by positively affecting group processes, cooperation in learning and learners' knowledge structure. Table 4 presents means and standard deviations for eacb group process, cooperation in learning, and three knowledge structures, and Tables 5 and 6 summarize significant ANOVA, HANOVA [5, 76] , and Tukey test results. Tbe HANOVA test was used to detect significant main and interaction effects for each group process, cooperation in learning, and two knowledge structures {knowledge complexity and integration), which were individual-level measures. The main benefit of using the HANOVA test for individual-level measures is that it nests individual responses within groups and groups within treatments, hence adjusting for group-level effects [5] . The ANOVA test was used to detect significant main and interaction effects for knowledge commonality, which was a group-level measure. In addition, the results of our hypotheses tests are summarized in Table 7. The results indicate that the main effects of GSS on knowledge acquisition (Hypothesis 1) were supported with the data on quality of feedback, domination, communication barrier, and cooperation in leaming. whereas the main effects of content facilitation on knowledge acquisition (Hypothesis 2) were supported only with the data on ktiowledge commonality. However, the interaction effects of GSS and content facilitation on knowledge acquisition (Hypothesis 3) were not supported. ",whereas the main effects of content facilitation on knowledge acquisition (Hypothesis 2) were supported only with the data on ktiowledge commonality.,belongs_to_article,1
knowledge acquisition,whereas the main effects of content facilitation on knowledge acquisition ( Hypothesis 2 ) were supported only with the data on ktiowledge commonality .,"Botb qualitative and quantitative data were collected in this study to test the hypotheses and to better understand tbe main (Hypotheses 1 and 2) and interaction (Hypothesis 3) effects of GSS and content facilitation on knowledge acquisition in terms of the two group process gains {group participation and quality of feedback), two group process losses {domination and communication barrier), cooperation in learning, and three knowledge structures {knowledge complexity, integration, and commonality). The main prediction of this study (see Figure 1) is tbat GSS and content facilitation enhance the effectiveness of knowledge acquisition by positively affecting group processes, cooperation in learning and learners' knowledge structure. Table 4 presents means and standard deviations for eacb group process, cooperation in learning, and three knowledge structures, and Tables 5 and 6 summarize significant ANOVA, HANOVA [5, 76] , and Tukey test results. Tbe HANOVA test was used to detect significant main and interaction effects for each group process, cooperation in learning, and two knowledge structures {knowledge complexity and integration), which were individual-level measures. The main benefit of using the HANOVA test for individual-level measures is that it nests individual responses within groups and groups within treatments, hence adjusting for group-level effects [5] . The ANOVA test was used to detect significant main and interaction effects for knowledge commonality, which was a group-level measure. In addition, the results of our hypotheses tests are summarized in Table 7. The results indicate that the main effects of GSS on knowledge acquisition (Hypothesis 1) were supported with the data on quality of feedback, domination, communication barrier, and cooperation in leaming. whereas the main effects of content facilitation on knowledge acquisition (Hypothesis 2) were supported only with the data on ktiowledge commonality. However, the interaction effects of GSS and content facilitation on knowledge acquisition (Hypothesis 3) were not supported. ",whereas the main effects of content facilitation on knowledge acquisition (Hypothesis 2) were supported only with the data on ktiowledge commonality.,belongs_to_article,1
legislation,whereas the magnitude of the coefficient of first domestic cybercrime legislation is considerably smaller than that of the cumulative number of domestic cybercrime legislation,"The statistical results are largely similar. The coefficient of COC enforcement continues to be negative and significant. However, its magnitude has increased (-0.133 versus -0.125) whereas the magnitude of the coefficient of first domestic cybercrime legislation is considerably smaller than that of the cumulative number of domestic cybercrime legislation (-0.084 versus -0.231) in Table 7, column (1). This suggests, using an imprecise variable to measure domestic enforcement, the COC enforcement variable, K it , could have picked up some effects due to the domestic enforcement. In view of this potential bias, in the remaining analysis we revert to using the cumulative number of domestic cybercrime legislation, which provides a finer differentiation in domestic enforcement. ",whereas the magnitude of the coefficient of first domestic cybercrime legislation is considerably smaller than that of the cumulative number of domestic cybercrime legislation,belongs_to_article,1
cybercrime,whereas the magnitude of the coefficient of first domestic cybercrime legislation is considerably smaller than that of the cumulative number of domestic cybercrime legislation,"The statistical results are largely similar. The coefficient of COC enforcement continues to be negative and significant. However, its magnitude has increased (-0.133 versus -0.125) whereas the magnitude of the coefficient of first domestic cybercrime legislation is considerably smaller than that of the cumulative number of domestic cybercrime legislation (-0.084 versus -0.231) in Table 7, column (1). This suggests, using an imprecise variable to measure domestic enforcement, the COC enforcement variable, K it , could have picked up some effects due to the domestic enforcement. In view of this potential bias, in the remaining analysis we revert to using the cumulative number of domestic cybercrime legislation, which provides a finer differentiation in domestic enforcement. ",whereas the magnitude of the coefficient of first domestic cybercrime legislation is considerably smaller than that of the cumulative number of domestic cybercrime legislation,belongs_to_article,1
false positive rate,whereas specificity is the complement of the false positive rate :,whereas specificity is the complement of the false positive rate:,whereas specificity is the complement of the false positive rate:,background_information,3
SPSS,"whereas roughly 85 % of broad Software development organizations adopted middleware . These results provide evidence that broad Software development organizations innovated broadly across the board , not just with the 6 services we used to derive the clusters . Minimal Software development organizations innovated minimally across the board . And selective Software development organizations are somewhere in between . The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS . We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model , nor do they provide membership likelihood probabilities . Thus , while they provide information about membership , as well as significance values through F - tests ( ie , significance of the likelihood to discriminate between clusters ) , these values may be meaningless without information regarding the goodness of fit of the cases into clusters START_CITE ( Hair et al . , 2010 ) END_CITE CITE_b43 . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted ( which are not innovative for the client ) and organizational process tools adopted ( also not innovative for the client ) , whereas services are directly applied to ( or leveraged by ) a client and can therefore be seen as an "" innovation "" by the client . Additionally , our focus is on service innovation , and hence , it makes most sense to measure such innovation using services only .","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only. ","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only.",background_information,3
organizational process,"whereas roughly 85 % of broad Software development organizations adopted middleware . These results provide evidence that broad Software development organizations innovated broadly across the board , not just with the 6 services we used to derive the clusters . Minimal Software development organizations innovated minimally across the board . And selective Software development organizations are somewhere in between . The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS . We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model , nor do they provide membership likelihood probabilities . Thus , while they provide information about membership , as well as significance values through F - tests ( ie , significance of the likelihood to discriminate between clusters ) , these values may be meaningless without information regarding the goodness of fit of the cases into clusters START_CITE ( Hair et al . , 2010 ) END_CITE CITE_b43 . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted ( which are not innovative for the client ) and organizational process tools adopted ( also not innovative for the client ) , whereas services are directly applied to ( or leveraged by ) a client and can therefore be seen as an "" innovation "" by the client . Additionally , our focus is on service innovation , and hence , it makes most sense to measure such innovation using services only .","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only. ","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only.",belongs_to_article,1
software innovation,"whereas roughly 85 % of broad Software development organizations adopted middleware . These results provide evidence that broad Software development organizations innovated broadly across the board , not just with the 6 services we used to derive the clusters . Minimal Software development organizations innovated minimally across the board . And selective Software development organizations are somewhere in between . The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS . We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model , nor do they provide membership likelihood probabilities . Thus , while they provide information about membership , as well as significance values through F - tests ( ie , significance of the likelihood to discriminate between clusters ) , these values may be meaningless without information regarding the goodness of fit of the cases into clusters START_CITE ( Hair et al . , 2010 ) END_CITE CITE_b43 . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted ( which are not innovative for the client ) and organizational process tools adopted ( also not innovative for the client ) , whereas services are directly applied to ( or leveraged by ) a client and can therefore be seen as an "" innovation "" by the client . Additionally , our focus is on service innovation , and hence , it makes most sense to measure such innovation using services only .","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only. ","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only.",belongs_to_article,1
F-tests,"whereas roughly 85 % of broad Software development organizations adopted middleware . These results provide evidence that broad Software development organizations innovated broadly across the board , not just with the 6 services we used to derive the clusters . Minimal Software development organizations innovated minimally across the board . And selective Software development organizations are somewhere in between . The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS . We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model , nor do they provide membership likelihood probabilities . Thus , while they provide information about membership , as well as significance values through F - tests ( ie , significance of the likelihood to discriminate between clusters ) , these values may be meaningless without information regarding the goodness of fit of the cases into clusters START_CITE ( Hair et al . , 2010 ) END_CITE CITE_b43 . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted ( which are not innovative for the client ) and organizational process tools adopted ( also not innovative for the client ) , whereas services are directly applied to ( or leveraged by ) a client and can therefore be seen as an "" innovation "" by the client . Additionally , our focus is on service innovation , and hence , it makes most sense to measure such innovation using services only .","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only. ","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only.",belongs_to_article,1
service innovation,"whereas roughly 85 % of broad Software development organizations adopted middleware . These results provide evidence that broad Software development organizations innovated broadly across the board , not just with the 6 services we used to derive the clusters . Minimal Software development organizations innovated minimally across the board . And selective Software development organizations are somewhere in between . The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS . We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model , nor do they provide membership likelihood probabilities . Thus , while they provide information about membership , as well as significance values through F - tests ( ie , significance of the likelihood to discriminate between clusters ) , these values may be meaningless without information regarding the goodness of fit of the cases into clusters START_CITE ( Hair et al . , 2010 ) END_CITE CITE_b43 . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted ( which are not innovative for the client ) and organizational process tools adopted ( also not innovative for the client ) , whereas services are directly applied to ( or leveraged by ) a client and can therefore be seen as an "" innovation "" by the client . Additionally , our focus is on service innovation , and hence , it makes most sense to measure such innovation using services only .","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only. ","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only.",belongs_to_article,1
innovation,"whereas roughly 85 % of broad Software development organizations adopted middleware . These results provide evidence that broad Software development organizations innovated broadly across the board , not just with the 6 services we used to derive the clusters . Minimal Software development organizations innovated minimally across the board . And selective Software development organizations are somewhere in between . The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS . We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model , nor do they provide membership likelihood probabilities . Thus , while they provide information about membership , as well as significance values through F - tests ( ie , significance of the likelihood to discriminate between clusters ) , these values may be meaningless without information regarding the goodness of fit of the cases into clusters START_CITE ( Hair et al . , 2010 ) END_CITE CITE_b43 . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted ( which are not innovative for the client ) and organizational process tools adopted ( also not innovative for the client ) , whereas services are directly applied to ( or leveraged by ) a client and can therefore be seen as an "" innovation "" by the client . Additionally , our focus is on service innovation , and hence , it makes most sense to measure such innovation using services only .","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only. ","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only.",belongs_to_article,1
innovations,"whereas roughly 85 % of broad Software development organizations adopted middleware . These results provide evidence that broad Software development organizations innovated broadly across the board , not just with the 6 services we used to derive the clusters . Minimal Software development organizations innovated minimally across the board . And selective Software development organizations are somewhere in between . The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS . We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model , nor do they provide membership likelihood probabilities . Thus , while they provide information about membership , as well as significance values through F - tests ( ie , significance of the likelihood to discriminate between clusters ) , these values may be meaningless without information regarding the goodness of fit of the cases into clusters START_CITE ( Hair et al . , 2010 ) END_CITE CITE_b43 . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted ( which are not innovative for the client ) and organizational process tools adopted ( also not innovative for the client ) , whereas services are directly applied to ( or leveraged by ) a client and can therefore be seen as an "" innovation "" by the client . Additionally , our focus is on service innovation , and hence , it makes most sense to measure such innovation using services only .","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only. ","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only.",belongs_to_article,1
SAS,"whereas roughly 85 % of broad Software development organizations adopted middleware . These results provide evidence that broad Software development organizations innovated broadly across the board , not just with the 6 services we used to derive the clusters . Minimal Software development organizations innovated minimally across the board . And selective Software development organizations are somewhere in between . The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS . We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model , nor do they provide membership likelihood probabilities . Thus , while they provide information about membership , as well as significance values through F - tests ( ie , significance of the likelihood to discriminate between clusters ) , these values may be meaningless without information regarding the goodness of fit of the cases into clusters START_CITE ( Hair et al . , 2010 ) END_CITE CITE_b43 . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted ( which are not innovative for the client ) and organizational process tools adopted ( also not innovative for the client ) , whereas services are directly applied to ( or leveraged by ) a client and can therefore be seen as an "" innovation "" by the client . Additionally , our focus is on service innovation , and hence , it makes most sense to measure such innovation using services only .","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only. ","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only.",background_information,3
goodness of fit,"whereas roughly 85 % of broad Software development organizations adopted middleware . These results provide evidence that broad Software development organizations innovated broadly across the board , not just with the 6 services we used to derive the clusters . Minimal Software development organizations innovated minimally across the board . And selective Software development organizations are somewhere in between . The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS . We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model , nor do they provide membership likelihood probabilities . Thus , while they provide information about membership , as well as significance values through F - tests ( ie , significance of the likelihood to discriminate between clusters ) , these values may be meaningless without information regarding the goodness of fit of the cases into clusters START_CITE ( Hair et al . , 2010 ) END_CITE CITE_b43 . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted ( which are not innovative for the client ) and organizational process tools adopted ( also not innovative for the client ) , whereas services are directly applied to ( or leveraged by ) a client and can therefore be seen as an "" innovation "" by the client . Additionally , our focus is on service innovation , and hence , it makes most sense to measure such innovation using services only .","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only. ","whereas roughly 85% of broad SDOs adopted middleware. These results provide evidence that broad SDOs innovated broadly across the board, not just with the 6 services we used to derive the clusters. Minimal SDOs innovated minimally across the board. And selective SDOs are somewhere in between. The only exceptions are Ubiquitous 5 Detecting similarities in patterns of software innovation would also have been possible with traditional statistical packages like SPSS and SAS. We chose Latent Gold as available techniques in those other packages neither provide fit statistics for the estimated model, nor do they provide membership likelihood probabilities. Thus, while they provide information about membership, as well as significance values through F-tests (ie, significance of the likelihood to discriminate between clusters), these values may be meaningless without information regarding the goodness of fit of the cases into clusters (Hair et al., 2010) . 6 These services are a better measure of service innovation than if we were to use base and/or process systems because base innovations represent the infrastructures adopted (which are not innovative for the client) and organizational process tools adopted (also not innovative for the client), whereas services are directly applied to (or leveraged by) a client and can therefore be seen as an ""innovation"" by the client. Additionally, our focus is on service innovation, and hence, it makes most sense to measure such innovation using services only.",related_work,2
R 2,"where , R 2 included and R 2 excluded are the squared Rs for the dependent latent variable when the interaction term is included and omitted , respectively , in the main - effects model .","where, R 2 included and R 2 excluded are the squared Rs for the dependent latent variable when the interaction term is included and omitted, respectively, in the main-effects model. Values of 0.02, 0.15, and 0.35 are recommended as small, moderate, and large effects, respectively [18] . The f 2 value for the full model was .08, which does not necessarily connote an unimportant effect. As Chin, Marcolin, and Newsted explain, ""Even a small interaction can be significant under extreme moderating conditions, if the resulting beta changes are meaningful, then it is important to take these conditions into account"" [15, p. 211]. Q 2 is another measure of predictive relevance of the PLS (structural) model [43] . It is calculated using a blindfolding procedure that excludes a part of the data for a particular block of indicators during parameter estimations, and then tries to estimate the omitted part using the estimated parameter [15] . Q 2 > 0 means that the model has predictive relevance, whereas Q 2 < 0 suggests a lack of it. Q 2 values of 0.31, 0.38, and 0.41 were obtained for the controls-only model, main-effects model, and full model respectively, which suggests that all the models have predictive relevance. To account for multiple projects per firm, we performed two robustness checks. First, we tested for any significant firm-level effects using hierarchical linear modeling (HLM). As a first step, a one-way analysis of variance was performed to confirm that the variability in the outcome variable, that is, knowledge integration, by a level-2 group, that is, a firm, is significantly different from zero. This tests whether there are any differences at the firm level on the knowledge integration, and confirms whether HLM is necessary [81] . The chi-square test statistic was not significant (χ 2 = 4.225, p = 0.237), indicating that there is no variance in our outcome variable by firm-level groupings, and that there is no statistical justification for running HLM. ","where, R 2 included and R 2 excluded are the squared Rs for the dependent latent variable when the interaction term is included and omitted, respectively, in the main-effects model.",belongs_to_article,1
mean of 500,zy refers to the mean of 500 path estimates from the moderator variable z to criterion y ( true score = 0 50 ; below solid line denotes significance ) .,xy refers to the mean of 500 path estimates from predictor x to criterion y (true score = 0 30; below dashed line denotes significance). zy refers to the mean of 500 path estimates from the moderator variable z to criterion y (true score = 0 50; below solid line denotes significance).,zy refers to the mean of 500 path estimates from the moderator variable z to criterion y (true score = 0 50; below solid line denotes significance).,belongs_to_article,1
moderator variable,zy refers to the mean of 500 path estimates from the moderator variable z to criterion y ( true score = 0 50 ; below solid line denotes significance ) .,xy refers to the mean of 500 path estimates from predictor x to criterion y (true score = 0 30; below dashed line denotes significance). zy refers to the mean of 500 path estimates from the moderator variable z to criterion y (true score = 0 50; below solid line denotes significance).,zy refers to the mean of 500 path estimates from the moderator variable z to criterion y (true score = 0 50; below solid line denotes significance).,belongs_To_article,1
web services,zur Muehlen et al . examined the development of process management standards in the context of workflow development based on web services .,". zur Muehlen et al. examined the development of process management standards in the context of workflow development based on web services. They investigated two opposing types of standards: those based on SOAP, with tightly coupled designs similar to remote procedure calls, and those based on REST, with loosely coupled designs similar to the navigating of web links. In the context of SOAP versus REST, the authors illustrated the standardization process, clarified the technical underpinnings of the conflict, and analyzed the interests of stakeholders. ",zur Muehlen et al. examined the development of process management standards in the context of workflow development based on web services.,related_work,2
process management,zur Muehlen et al . examined the development of process management standards in the context of workflow development based on web services .,". zur Muehlen et al. examined the development of process management standards in the context of workflow development based on web services. They investigated two opposing types of standards: those based on SOAP, with tightly coupled designs similar to remote procedure calls, and those based on REST, with loosely coupled designs similar to the navigating of web links. In the context of SOAP versus REST, the authors illustrated the standardization process, clarified the technical underpinnings of the conflict, and analyzed the interests of stakeholders. ",zur Muehlen et al. examined the development of process management standards in the context of workflow development based on web services.,related_work,2
CIO,"zooming in on the two directions of understanding and their differential effects , our study shows that it is the chief information officer 's understanding of the chief executive officer - not the chief executive officer 's understanding of the chief information officer - that matters for improving and strengthening collaboration quality .","zooming in on the two directions of understanding and their differential effects, our study shows that it is the CIO's understanding of the CEO-not the CEO's understanding of the CIO-that matters for improving and strengthening collaboration quality. As such, our study is the first to highlight the relevance of the conceptual distinction into an active and passive form of understanding and its crucial implications for effective social alignment. Third, and more generally, our study contributes to interpersonal relations research by combining two hitherto largely separately applied theoretical models of intra-/ interpersonal perceptions and interdependence. In so doing, we are able to gain a more comprehensive picture of the bilateral nature and effects of CEO-CIO understanding, which would not be possible by focusing on either model in isolation. ","zooming in on the two directions of understanding and their differential effects, our study shows that it is the CIO's understanding of the CEO-not the CEO's understanding of the CIO-that matters for improving and strengthening collaboration quality.",belongs_to_article,1
online survey,"zoomeRaNG GeNeRated a RaNdom set of 1,200 sales / marketing managers employed in high - tech firms , who were then invited to complete the online survey .","zoomeRaNG GeNeRated a RaNdom set of 1,200 sales/marketing managers employed in high-tech firms, who were then invited to complete the online survey. Of these 1,200, we received 188 usable responses. 4 the demographics of our sample reveal that just over half were female (60 percent), middle aged (mean = 45 years), and well-educated (95 percent had at least some college experience). Respondents' average organizational tenure was 8.3 years, with an average of 5.1 years in their current position. they were highly active in formulation of marketing/sales policies for their firms at the time of the study (mean = 4.29 on a 5-point scale, with 5 = ""very active""). thus, the respondents were highly qualified to answer the questions. the median firm size was 400 employees, and the average firm age was 36.5 years. Our second survey was also deployed through Zoomerang. Of the 188 usable respondents from survey one, a total of 108 individuals completed the second survey for an effective response rate of 60 percent. ","zoomeRaNG GeNeRated a RaNdom set of 1,200 sales/marketing managers employed in high-tech firms, who were then invited to complete the online survey.",belongs_to_article,1
marketing,"zoomeRaNG GeNeRated a RaNdom set of 1,200 sales / marketing managers employed in high - tech firms , who were then invited to complete the online survey .","zoomeRaNG GeNeRated a RaNdom set of 1,200 sales/marketing managers employed in high-tech firms, who were then invited to complete the online survey. Of these 1,200, we received 188 usable responses. 4 the demographics of our sample reveal that just over half were female (60 percent), middle aged (mean = 45 years), and well-educated (95 percent had at least some college experience). Respondents' average organizational tenure was 8.3 years, with an average of 5.1 years in their current position. they were highly active in formulation of marketing/sales policies for their firms at the time of the study (mean = 4.29 on a 5-point scale, with 5 = ""very active""). thus, the respondents were highly qualified to answer the questions. the median firm size was 400 employees, and the average firm age was 36.5 years. Our second survey was also deployed through Zoomerang. Of the 188 usable respondents from survey one, a total of 108 individuals completed the second survey for an effective response rate of 60 percent. ","zoomeRaNG GeNeRated a RaNdom set of 1,200 sales/marketing managers employed in high-tech firms, who were then invited to complete the online survey.",belongs_to_article,1
virtual teams,"zones , organizational and national cultures , work practices , and technology all present specific challenges to people working in virtual teams .","zones, organizational and national cultures, work practices, and technology all present specific challenges to people working in virtual teams. For example, several empirical studies show that building trust across distance is difficult for distributed team members with no prior relationships (Jarvenpaa & Leidner, 1999; Paul & McDaniel, 2004; Zolin et al., 2004) and that team members are likely to base attributions about teammates on scant evidence (Cramton, 2001) . Also, temporal discontinuities arising from differences in time zones produce diverging perceptions of time and may reflect differences in team members' value systems (Saunders et al., 2004) . ","zones, organizational and national cultures, work practices, and technology all present specific challenges to people working in virtual teams.",background_information,3
national cultures,"zones , organizational and national cultures , work practices , and technology all present specific challenges to people working in virtual teams .","zones, organizational and national cultures, work practices, and technology all present specific challenges to people working in virtual teams. For example, several empirical studies show that building trust across distance is difficult for distributed team members with no prior relationships (Jarvenpaa & Leidner, 1999; Paul & McDaniel, 2004; Zolin et al., 2004) and that team members are likely to base attributions about teammates on scant evidence (Cramton, 2001) . Also, temporal discontinuities arising from differences in time zones produce diverging perceptions of time and may reflect differences in team members' value systems (Saunders et al., 2004) . ","zones, organizational and national cultures, work practices, and technology all present specific challenges to people working in virtual teams.",background_information,3
Internet,"zhao et al . [ 56 ] utilized nine years of data to calculate "" rate of Internet diffusion "" based on the logarithmic difference in Internet users per 1,000 .","Some studies are able to measure technology at two time points, yielding a diffusion rate, which indicates the rate of change [3, 28, 56]. kiiski and Pohjola [28] defined Internet diffusion as the logarithmic difference in Internet hosts per capita during a five-year period. Baliamoune-lutz [3] calculated two-year diffusion per capita of cell phones, Internet hosts, Internet users, and Pcs. zhao et al. [56] utilized nine years of data to calculate ""rate of Internet diffusion"" based on the logarithmic difference in Internet users per 1,000. Ono and zavodny [35] analyzed the diffusion of home computer ownership and number of Internet users per 100 people. Because the present study utilizes cross-sectional data, the definition of technology utilization resembles those in the nondiffusion studies [12, 39, 42] . However, it also resembles single time point constructs from the studies with diffusion indicators [3, 28, 35, 56]. ","zhao et al. [56] utilized nine years of data to calculate ""rate of Internet diffusion"" based on the logarithmic difference in Internet users per 1,000.",related_work,2
Internet diffusion,"zhao et al . [ 56 ] utilized nine years of data to calculate "" rate of Internet diffusion "" based on the logarithmic difference in Internet users per 1,000 .","Some studies are able to measure technology at two time points, yielding a diffusion rate, which indicates the rate of change [3, 28, 56]. kiiski and Pohjola [28] defined Internet diffusion as the logarithmic difference in Internet hosts per capita during a five-year period. Baliamoune-lutz [3] calculated two-year diffusion per capita of cell phones, Internet hosts, Internet users, and Pcs. zhao et al. [56] utilized nine years of data to calculate ""rate of Internet diffusion"" based on the logarithmic difference in Internet users per 1,000. Ono and zavodny [35] analyzed the diffusion of home computer ownership and number of Internet users per 100 people. Because the present study utilizes cross-sectional data, the definition of technology utilization resembles those in the nondiffusion studies [12, 39, 42] . However, it also resembles single time point constructs from the studies with diffusion indicators [3, 28, 35, 56]. ","zhao et al. [56] utilized nine years of data to calculate ""rate of Internet diffusion"" based on the logarithmic difference in Internet users per 1,000.",related_work,2
public goods,"z - Tree would have been useful for programming simpler settings , such as a standard public goods game , but for the crowdfunding setting we wanted to examine , along with physiological measures , Brownie turned out to be the better choice .","It would have been easier to find someone to program for z-Tree, especially for simpler experiments, but not for programming our experiment, which would have been a bigger challenge. z-Tree would have been useful for programming simpler settings, such as a standard public goods game, but for the crowdfunding setting we wanted to examine, along with physiological measures, Brownie turned out to be the better choice. In addition, on z-Tree, it is not possible to use common knowledge about programming, which makes one heavily dependent on the manual, whereas on Brownie, we could find help for new ideas online, for instance on forums, very easily. ","z-Tree would have been useful for programming simpler settings, such as a standard public goods game, but for the crowdfunding setting we wanted to examine, along with physiological measures, Brownie turned out to be the better choice.",background_information,3
crowdfunding,"z - Tree would have been useful for programming simpler settings , such as a standard public goods game , but for the crowdfunding setting we wanted to examine , along with physiological measures , Brownie turned out to be the better choice .","It would have been easier to find someone to program for z-Tree, especially for simpler experiments, but not for programming our experiment, which would have been a bigger challenge. z-Tree would have been useful for programming simpler settings, such as a standard public goods game, but for the crowdfunding setting we wanted to examine, along with physiological measures, Brownie turned out to be the better choice. In addition, on z-Tree, it is not possible to use common knowledge about programming, which makes one heavily dependent on the manual, whereas on Brownie, we could find help for new ideas online, for instance on forums, very easily. ","z-Tree would have been useful for programming simpler settings, such as a standard public goods game, but for the crowdfunding setting we wanted to examine, along with physiological measures, Brownie turned out to be the better choice.",belongs_to_article,1
physiological measures,"z - Tree would have been useful for programming simpler settings , such as a standard public goods game , but for the crowdfunding setting we wanted to examine , along with physiological measures , Brownie turned out to be the better choice .","It would have been easier to find someone to program for z-Tree, especially for simpler experiments, but not for programming our experiment, which would have been a bigger challenge. z-Tree would have been useful for programming simpler settings, such as a standard public goods game, but for the crowdfunding setting we wanted to examine, along with physiological measures, Brownie turned out to be the better choice. In addition, on z-Tree, it is not possible to use common knowledge about programming, which makes one heavily dependent on the manual, whereas on Brownie, we could find help for new ideas online, for instance on forums, very easily. ","z-Tree would have been useful for programming simpler settings, such as a standard public goods game, but for the crowdfunding setting we wanted to examine, along with physiological measures, Brownie turned out to be the better choice.",belongs_to_article,1
algorithms,"z ) cell locations returned from either of the algorithms , a GST morphogenesis change algorithm reduces to a simple counting procedure :","Within the GST-DigiDoc, the amount of morphogenesis change is represented by the total number of units represented within a component or within the c(xy-coordinates) returned by either of the algorithms. Since run-length coding is used for both, counting the total number of morphogenesis units requires a very simple procedure that accounts for the cells not explicitly stored in the structure. If the input xy-coordinates are represented by a run-length encoded list of georeferenced (x, y) or (x, y, z) cell locations returned from either of the algorithms, a GST morphogenesis change algorithm reduces to a simple counting procedure: ","z) cell locations returned from either of the algorithms, a GST morphogenesis change algorithm reduces to a simple counting procedure:",belongs_to_article,1
algorithm,"z ) cell locations returned from either of the algorithms , a GST morphogenesis change algorithm reduces to a simple counting procedure :","Within the GST-DigiDoc, the amount of morphogenesis change is represented by the total number of units represented within a component or within the c(xy-coordinates) returned by either of the algorithms. Since run-length coding is used for both, counting the total number of morphogenesis units requires a very simple procedure that accounts for the cells not explicitly stored in the structure. If the input xy-coordinates are represented by a run-length encoded list of georeferenced (x, y) or (x, y, z) cell locations returned from either of the algorithms, a GST morphogenesis change algorithm reduces to a simple counting procedure: ","z) cell locations returned from either of the algorithms, a GST morphogenesis change algorithm reduces to a simple counting procedure:",belongs_to_article,1
control variable,"z snwd ; z snwd is an exogenous regressor ( e.g. , Google Trends control variable ) ; snwd is the parameter for the influence of the exogenous regressor ; is the parameter for the strength of regressor error dependence ; and snwd is the latent instrument that has a discrete distribution with K support points 1 k and probabilities p 1 p k , where each k has a normal prior , the probabilities p k have a Dirichlet prior , and K is the number of latent segments for the shows and is empirically determined .","We expect viewerships of shows in the same time slot to be correlated with each other. z snwd ; z snwd is an exogenous regressor (e.g., Google Trends control variable); snwd is the parameter for the influence of the exogenous regressor; is the parameter for the strength of regressor error dependence; and snwd is the latent instrument that has a discrete distribution with K support points 1 k and probabilities p 1 p k , where each k has a normal prior, the probabilities p k have a Dirichlet prior, and K is the number of latent segments for the shows and is empirically determined. ","z snwd ; z snwd is an exogenous regressor (e.g., Google Trends control variable); snwd is the parameter for the influence of the exogenous regressor; is the parameter for the strength of regressor error dependence; and snwd is the latent instrument that has a discrete distribution with K support points 1 k and probabilities p 1 p k , where each k has a normal prior, the probabilities p k have a Dirichlet prior, and K is the number of latent segments for the shows and is empirically determined.",belongs_to_article,1
Google,"z snwd ; z snwd is an exogenous regressor ( e.g. , Google Trends control variable ) ; snwd is the parameter for the influence of the exogenous regressor ; is the parameter for the strength of regressor error dependence ; and snwd is the latent instrument that has a discrete distribution with K support points 1 k and probabilities p 1 p k , where each k has a normal prior , the probabilities p k have a Dirichlet prior , and K is the number of latent segments for the shows and is empirically determined .","We expect viewerships of shows in the same time slot to be correlated with each other. z snwd ; z snwd is an exogenous regressor (e.g., Google Trends control variable); snwd is the parameter for the influence of the exogenous regressor; is the parameter for the strength of regressor error dependence; and snwd is the latent instrument that has a discrete distribution with K support points 1 k and probabilities p 1 p k , where each k has a normal prior, the probabilities p k have a Dirichlet prior, and K is the number of latent segments for the shows and is empirically determined. ","z snwd ; z snwd is an exogenous regressor (e.g., Google Trends control variable); snwd is the parameter for the influence of the exogenous regressor; is the parameter for the strength of regressor error dependence; and snwd is the latent instrument that has a discrete distribution with K support points 1 k and probabilities p 1 p k , where each k has a normal prior, the probabilities p k have a Dirichlet prior, and K is the number of latent segments for the shows and is empirically determined.",belongs_to_article,1
control variables,"z i;j;t represents a vector of observed hospital - level time - variant control variables , and β m the vector of associated regression coefficients .","wherein, σ 2 t is the error variance of year t (t = 2010, 2011, . . ., 2015) and σ b;bþ1 is the error covariance between two adjacent years (b = 0 stands for 2010, b = 0, 1, . . ., 4). In Eq. 5, i indexes hospital, j indexes healthcare system, and t indexes year. β i;j;0 represents the intercept-a linear combination of the Level-2 intercept (u 0;j;0 ), a vector of observed hospital-level time-invariant control variables (x i;j ) and its regression coefficients (u n ), and unobserved hospital-level random effect (r i;j ). u 0;j;0 is further decomposed into an overall intercept (λ 0;0 ), a vector of observed system-level control variables (w j ) and its regression coefficients (λ p ), and unobserved system-level random effect (ν j ). Q i;j;tÀ 1 represents cumulative partners' experiences for hospital i in system j in year t; its regression coefficient is β 1 . z i;j;t represents a vector of observed hospital-level time-variant control variables, and β m the vector of associated regression coefficients. δ t represents year fixed effects. Finally, ε i;j;t is the random error for hospital i of system j in year t and is modeled as a banded-1 diagonal structure. 3 To examine the differential effects of the hospital-level EHR centralities (ehrR and ehrW) and the system-level interconnectedness (SysehrW and SysehrR) on the relationship between partners' experiences and cost efficiency, we added the within-level and cross-level interactions, respectively, with partners' experiences to the HLM and analyzed the following model: In Eq. 6, β i;j;1 is now a linear combination of the fixed term (λ 0;1 ), system-level interconnectedness for system j (SysehrR i;j and SysehrW i;j ), and their coefficients (λ 1;1 and λ 2;1 , respectively). β 2 and β 3 denote the respective effect of hospital's time-variant centrality on cost efficiency (ehrR i;j;tÀ 1 and ehrW i;j;tÀ 1 ). β 4 and β 5 denote the respective effect of withinlevel interactions between centrality and partners' experiences. All else is as defined in Eq. 6. The HLM model is integrated as follows: Before estimating the model, we grand mean-centered all hospital-level and system-level variables to mitigate multicollinearity [14] . We standardized all variables for better comparisons across the regression coefficients. Next, we used the maximum likelihood method of estimation, which is appropriate for unbalanced data sets like ours. We used Stata 15 [128] for all tests. ","z i;j;t represents a vector of observed hospital-level time-variant control variables, and β m the vector of associated regression coefficients.",belongs_to_article,1
regression coefficients,"z i;j;t represents a vector of observed hospital - level time - variant control variables , and β m the vector of associated regression coefficients .","wherein, σ 2 t is the error variance of year t (t = 2010, 2011, . . ., 2015) and σ b;bþ1 is the error covariance between two adjacent years (b = 0 stands for 2010, b = 0, 1, . . ., 4). In Eq. 5, i indexes hospital, j indexes healthcare system, and t indexes year. β i;j;0 represents the intercept-a linear combination of the Level-2 intercept (u 0;j;0 ), a vector of observed hospital-level time-invariant control variables (x i;j ) and its regression coefficients (u n ), and unobserved hospital-level random effect (r i;j ). u 0;j;0 is further decomposed into an overall intercept (λ 0;0 ), a vector of observed system-level control variables (w j ) and its regression coefficients (λ p ), and unobserved system-level random effect (ν j ). Q i;j;tÀ 1 represents cumulative partners' experiences for hospital i in system j in year t; its regression coefficient is β 1 . z i;j;t represents a vector of observed hospital-level time-variant control variables, and β m the vector of associated regression coefficients. δ t represents year fixed effects. Finally, ε i;j;t is the random error for hospital i of system j in year t and is modeled as a banded-1 diagonal structure. 3 To examine the differential effects of the hospital-level EHR centralities (ehrR and ehrW) and the system-level interconnectedness (SysehrW and SysehrR) on the relationship between partners' experiences and cost efficiency, we added the within-level and cross-level interactions, respectively, with partners' experiences to the HLM and analyzed the following model: In Eq. 6, β i;j;1 is now a linear combination of the fixed term (λ 0;1 ), system-level interconnectedness for system j (SysehrR i;j and SysehrW i;j ), and their coefficients (λ 1;1 and λ 2;1 , respectively). β 2 and β 3 denote the respective effect of hospital's time-variant centrality on cost efficiency (ehrR i;j;tÀ 1 and ehrW i;j;tÀ 1 ). β 4 and β 5 denote the respective effect of withinlevel interactions between centrality and partners' experiences. All else is as defined in Eq. 6. The HLM model is integrated as follows: Before estimating the model, we grand mean-centered all hospital-level and system-level variables to mitigate multicollinearity [14] . We standardized all variables for better comparisons across the regression coefficients. Next, we used the maximum likelihood method of estimation, which is appropriate for unbalanced data sets like ours. We used Stata 15 [128] for all tests. ","z i;j;t represents a vector of observed hospital-level time-variant control variables, and β m the vector of associated regression coefficients.",belongs_to_article,1
hospital,"z i;j;t represents a vector of observed hospital - level time - variant control variables , and β m the vector of associated regression coefficients .","wherein, σ 2 t is the error variance of year t (t = 2010, 2011, . . ., 2015) and σ b;bþ1 is the error covariance between two adjacent years (b = 0 stands for 2010, b = 0, 1, . . ., 4). In Eq. 5, i indexes hospital, j indexes healthcare system, and t indexes year. β i;j;0 represents the intercept-a linear combination of the Level-2 intercept (u 0;j;0 ), a vector of observed hospital-level time-invariant control variables (x i;j ) and its regression coefficients (u n ), and unobserved hospital-level random effect (r i;j ). u 0;j;0 is further decomposed into an overall intercept (λ 0;0 ), a vector of observed system-level control variables (w j ) and its regression coefficients (λ p ), and unobserved system-level random effect (ν j ). Q i;j;tÀ 1 represents cumulative partners' experiences for hospital i in system j in year t; its regression coefficient is β 1 . z i;j;t represents a vector of observed hospital-level time-variant control variables, and β m the vector of associated regression coefficients. δ t represents year fixed effects. Finally, ε i;j;t is the random error for hospital i of system j in year t and is modeled as a banded-1 diagonal structure. 3 To examine the differential effects of the hospital-level EHR centralities (ehrR and ehrW) and the system-level interconnectedness (SysehrW and SysehrR) on the relationship between partners' experiences and cost efficiency, we added the within-level and cross-level interactions, respectively, with partners' experiences to the HLM and analyzed the following model: In Eq. 6, β i;j;1 is now a linear combination of the fixed term (λ 0;1 ), system-level interconnectedness for system j (SysehrR i;j and SysehrW i;j ), and their coefficients (λ 1;1 and λ 2;1 , respectively). β 2 and β 3 denote the respective effect of hospital's time-variant centrality on cost efficiency (ehrR i;j;tÀ 1 and ehrW i;j;tÀ 1 ). β 4 and β 5 denote the respective effect of withinlevel interactions between centrality and partners' experiences. All else is as defined in Eq. 6. The HLM model is integrated as follows: Before estimating the model, we grand mean-centered all hospital-level and system-level variables to mitigate multicollinearity [14] . We standardized all variables for better comparisons across the regression coefficients. Next, we used the maximum likelihood method of estimation, which is appropriate for unbalanced data sets like ours. We used Stata 15 [128] for all tests. ","z i;j;t represents a vector of observed hospital-level time-variant control variables, and β m the vector of associated regression coefficients.",belongs_to_article,1
reflective construct,z We also conducted the analysis by treating intrinsic benefit as a reflective construct and found no significant differences in our conclusions .,.332 2.481 y The third EXB item was removed from the analysis because of its low loading. z We also conducted the analysis by treating intrinsic benefit as a reflective construct and found no significant differences in our conclusions.,z We also conducted the analysis by treating intrinsic benefit as a reflective construct and found no significant differences in our conclusions.,belongs_to_article,1
no significant differences,z We also conducted the analysis by treating intrinsic benefit as a reflective construct and found no significant differences in our conclusions .,.332 2.481 y The third EXB item was removed from the analysis because of its low loading. z We also conducted the analysis by treating intrinsic benefit as a reflective construct and found no significant differences in our conclusions.,z We also conducted the analysis by treating intrinsic benefit as a reflective construct and found no significant differences in our conclusions.,related_work,2
P 1,z 1 > z 0 and P 1 < P 0 .,"In Table 1, P 1 and P 0 denote the future and current probability of information systems breaches respectively. The variable z 1 and z 0 denote the target's future and current investment in IS security respectively. The variable M denotes the maximum instant loss in the incident of information systems breaches. The maximum instant loss includes any loss from incursions excluding the cost of security. Subscript 0 denotes the current state. Subscript 1 denotes the future state. z 1 > z 0 and P 1 < P 0 . ",z 1 > z 0 and P 1 < P 0 .,belongs_to_article,1
P 0 .,z 1 > z 0 and P 1 < P 0 .,"In Table 1, P 1 and P 0 denote the future and current probability of information systems breaches respectively. The variable z 1 and z 0 denote the target's future and current investment in IS security respectively. The variable M denotes the maximum instant loss in the incident of information systems breaches. The maximum instant loss includes any loss from incursions excluding the cost of security. Subscript 0 denotes the current state. Subscript 1 denotes the future state. z 1 > z 0 and P 1 < P 0 . ",z 1 > z 0 and P 1 < P 0 .,belongs_to_article,1
email,"ywhenever I went on to that site I got frustrated ( I 've got an email to them about that ) , to the point where I just thought forget it , just go to the local paper - based directory & it was quicker , because of the complications & a lot of them were technical complications and he was always ' Oh yeah , we 're looking into it ; we 're fixing that ; fixing this ( SME in the printing trade ) . ySupport was offered but again it 's the marketing officer who did thatyhere 's a phone number , if you need support you just call the number and that was it . I suppose the problem is that one person is trying to do all the things . Specifically , what should have been set up was some form of help desk that is equally business related , as it is technical . That does n't appear to be there . We ca n't get tender results . Even if its just technical support and stats or tech issuesyThere 's some things that are absolutely fundamentally wrong ( SME in IT business ) .","ywhenever I went on to that site I got frustrated (I've got an email to them about that), to the point where I just thought forget it, just go to the local paper-based directory & it was quicker, because of the complications & a lot of them were technical complications and he was always 'Oh yeah, we're looking into it; we're fixing that; fixing this (SME in the printing trade). ySupport was offered but again it's the marketing officer who did thatyhere's a phone number, if you need support you just call the number and that was it. I suppose the problem is that one person is trying to do all the things. Specifically, what should have been set up was some form of help desk that is equally business related, as it is technical. That doesn't appear to be there. We can't get tender results. Even if its just technical support and stats or tech issuesyThere's some things that are absolutely fundamentally wrong (SME in IT business). ","ywhenever I went on to that site I got frustrated (I've got an email to them about that), to the point where I just thought forget it, just go to the local paper-based directory & it was quicker, because of the complications & a lot of them were technical complications and he was always 'Oh yeah, we're looking into it; we're fixing that; fixing this (SME in the printing trade). ySupport was offered but again it's the marketing officer who did thatyhere's a phone number, if you need support you just call the number and that was it. I suppose the problem is that one person is trying to do all the things. Specifically, what should have been set up was some form of help desk that is equally business related, as it is technical. That doesn't appear to be there. We can't get tender results. Even if its just technical support and stats or tech issuesyThere's some things that are absolutely fundamentally wrong (SME in IT business).",belongs_to_article,1
SME,"ywhenever I went on to that site I got frustrated ( I 've got an email to them about that ) , to the point where I just thought forget it , just go to the local paper - based directory & it was quicker , because of the complications & a lot of them were technical complications and he was always ' Oh yeah , we 're looking into it ; we 're fixing that ; fixing this ( SME in the printing trade ) . ySupport was offered but again it 's the marketing officer who did thatyhere 's a phone number , if you need support you just call the number and that was it . I suppose the problem is that one person is trying to do all the things . Specifically , what should have been set up was some form of help desk that is equally business related , as it is technical . That does n't appear to be there . We ca n't get tender results . Even if its just technical support and stats or tech issuesyThere 's some things that are absolutely fundamentally wrong ( SME in IT business ) .","ywhenever I went on to that site I got frustrated (I've got an email to them about that), to the point where I just thought forget it, just go to the local paper-based directory & it was quicker, because of the complications & a lot of them were technical complications and he was always 'Oh yeah, we're looking into it; we're fixing that; fixing this (SME in the printing trade). ySupport was offered but again it's the marketing officer who did thatyhere's a phone number, if you need support you just call the number and that was it. I suppose the problem is that one person is trying to do all the things. Specifically, what should have been set up was some form of help desk that is equally business related, as it is technical. That doesn't appear to be there. We can't get tender results. Even if its just technical support and stats or tech issuesyThere's some things that are absolutely fundamentally wrong (SME in IT business). ","ywhenever I went on to that site I got frustrated (I've got an email to them about that), to the point where I just thought forget it, just go to the local paper-based directory & it was quicker, because of the complications & a lot of them were technical complications and he was always 'Oh yeah, we're looking into it; we're fixing that; fixing this (SME in the printing trade). ySupport was offered but again it's the marketing officer who did thatyhere's a phone number, if you need support you just call the number and that was it. I suppose the problem is that one person is trying to do all the things. Specifically, what should have been set up was some form of help desk that is equally business related, as it is technical. That doesn't appear to be there. We can't get tender results. Even if its just technical support and stats or tech issuesyThere's some things that are absolutely fundamentally wrong (SME in IT business).",belongs_to_article,1
IT,"ywhenever I went on to that site I got frustrated ( I 've got an email to them about that ) , to the point where I just thought forget it , just go to the local paper - based directory & it was quicker , because of the complications & a lot of them were technical complications and he was always ' Oh yeah , we 're looking into it ; we 're fixing that ; fixing this ( SME in the printing trade ) . ySupport was offered but again it 's the marketing officer who did thatyhere 's a phone number , if you need support you just call the number and that was it . I suppose the problem is that one person is trying to do all the things . Specifically , what should have been set up was some form of help desk that is equally business related , as it is technical . That does n't appear to be there . We ca n't get tender results . Even if its just technical support and stats or tech issuesyThere 's some things that are absolutely fundamentally wrong ( SME in IT business ) .","ywhenever I went on to that site I got frustrated (I've got an email to them about that), to the point where I just thought forget it, just go to the local paper-based directory & it was quicker, because of the complications & a lot of them were technical complications and he was always 'Oh yeah, we're looking into it; we're fixing that; fixing this (SME in the printing trade). ySupport was offered but again it's the marketing officer who did thatyhere's a phone number, if you need support you just call the number and that was it. I suppose the problem is that one person is trying to do all the things. Specifically, what should have been set up was some form of help desk that is equally business related, as it is technical. That doesn't appear to be there. We can't get tender results. Even if its just technical support and stats or tech issuesyThere's some things that are absolutely fundamentally wrong (SME in IT business). ","ywhenever I went on to that site I got frustrated (I've got an email to them about that), to the point where I just thought forget it, just go to the local paper-based directory & it was quicker, because of the complications & a lot of them were technical complications and he was always 'Oh yeah, we're looking into it; we're fixing that; fixing this (SME in the printing trade). ySupport was offered but again it's the marketing officer who did thatyhere's a phone number, if you need support you just call the number and that was it. I suppose the problem is that one person is trying to do all the things. Specifically, what should have been set up was some form of help desk that is equally business related, as it is technical. That doesn't appear to be there. We can't get tender results. Even if its just technical support and stats or tech issuesyThere's some things that are absolutely fundamentally wrong (SME in IT business).",belongs_to_article,1
help desk,"ywhenever I went on to that site I got frustrated ( I 've got an email to them about that ) , to the point where I just thought forget it , just go to the local paper - based directory & it was quicker , because of the complications & a lot of them were technical complications and he was always ' Oh yeah , we 're looking into it ; we 're fixing that ; fixing this ( SME in the printing trade ) . ySupport was offered but again it 's the marketing officer who did thatyhere 's a phone number , if you need support you just call the number and that was it . I suppose the problem is that one person is trying to do all the things . Specifically , what should have been set up was some form of help desk that is equally business related , as it is technical . That does n't appear to be there . We ca n't get tender results . Even if its just technical support and stats or tech issuesyThere 's some things that are absolutely fundamentally wrong ( SME in IT business ) .","ywhenever I went on to that site I got frustrated (I've got an email to them about that), to the point where I just thought forget it, just go to the local paper-based directory & it was quicker, because of the complications & a lot of them were technical complications and he was always 'Oh yeah, we're looking into it; we're fixing that; fixing this (SME in the printing trade). ySupport was offered but again it's the marketing officer who did thatyhere's a phone number, if you need support you just call the number and that was it. I suppose the problem is that one person is trying to do all the things. Specifically, what should have been set up was some form of help desk that is equally business related, as it is technical. That doesn't appear to be there. We can't get tender results. Even if its just technical support and stats or tech issuesyThere's some things that are absolutely fundamentally wrong (SME in IT business). ","ywhenever I went on to that site I got frustrated (I've got an email to them about that), to the point where I just thought forget it, just go to the local paper-based directory & it was quicker, because of the complications & a lot of them were technical complications and he was always 'Oh yeah, we're looking into it; we're fixing that; fixing this (SME in the printing trade). ySupport was offered but again it's the marketing officer who did thatyhere's a phone number, if you need support you just call the number and that was it. I suppose the problem is that one person is trying to do all the things. Specifically, what should have been set up was some form of help desk that is equally business related, as it is technical. That doesn't appear to be there. We can't get tender results. Even if its just technical support and stats or tech issuesyThere's some things that are absolutely fundamentally wrong (SME in IT business).",belongs_to_article,1
marketing,"ywhenever I went on to that site I got frustrated ( I 've got an email to them about that ) , to the point where I just thought forget it , just go to the local paper - based directory & it was quicker , because of the complications & a lot of them were technical complications and he was always ' Oh yeah , we 're looking into it ; we 're fixing that ; fixing this ( SME in the printing trade ) . ySupport was offered but again it 's the marketing officer who did thatyhere 's a phone number , if you need support you just call the number and that was it . I suppose the problem is that one person is trying to do all the things . Specifically , what should have been set up was some form of help desk that is equally business related , as it is technical . That does n't appear to be there . We ca n't get tender results . Even if its just technical support and stats or tech issuesyThere 's some things that are absolutely fundamentally wrong ( SME in IT business ) .","ywhenever I went on to that site I got frustrated (I've got an email to them about that), to the point where I just thought forget it, just go to the local paper-based directory & it was quicker, because of the complications & a lot of them were technical complications and he was always 'Oh yeah, we're looking into it; we're fixing that; fixing this (SME in the printing trade). ySupport was offered but again it's the marketing officer who did thatyhere's a phone number, if you need support you just call the number and that was it. I suppose the problem is that one person is trying to do all the things. Specifically, what should have been set up was some form of help desk that is equally business related, as it is technical. That doesn't appear to be there. We can't get tender results. Even if its just technical support and stats or tech issuesyThere's some things that are absolutely fundamentally wrong (SME in IT business). ","ywhenever I went on to that site I got frustrated (I've got an email to them about that), to the point where I just thought forget it, just go to the local paper-based directory & it was quicker, because of the complications & a lot of them were technical complications and he was always 'Oh yeah, we're looking into it; we're fixing that; fixing this (SME in the printing trade). ySupport was offered but again it's the marketing officer who did thatyhere's a phone number, if you need support you just call the number and that was it. I suppose the problem is that one person is trying to do all the things. Specifically, what should have been set up was some form of help desk that is equally business related, as it is technical. That doesn't appear to be there. We can't get tender results. Even if its just technical support and stats or tech issuesyThere's some things that are absolutely fundamentally wrong (SME in IT business).",belongs_to_article,1
technical support,"ywhenever I went on to that site I got frustrated ( I 've got an email to them about that ) , to the point where I just thought forget it , just go to the local paper - based directory & it was quicker , because of the complications & a lot of them were technical complications and he was always ' Oh yeah , we 're looking into it ; we 're fixing that ; fixing this ( SME in the printing trade ) . ySupport was offered but again it 's the marketing officer who did thatyhere 's a phone number , if you need support you just call the number and that was it . I suppose the problem is that one person is trying to do all the things . Specifically , what should have been set up was some form of help desk that is equally business related , as it is technical . That does n't appear to be there . We ca n't get tender results . Even if its just technical support and stats or tech issuesyThere 's some things that are absolutely fundamentally wrong ( SME in IT business ) .","ywhenever I went on to that site I got frustrated (I've got an email to them about that), to the point where I just thought forget it, just go to the local paper-based directory & it was quicker, because of the complications & a lot of them were technical complications and he was always 'Oh yeah, we're looking into it; we're fixing that; fixing this (SME in the printing trade). ySupport was offered but again it's the marketing officer who did thatyhere's a phone number, if you need support you just call the number and that was it. I suppose the problem is that one person is trying to do all the things. Specifically, what should have been set up was some form of help desk that is equally business related, as it is technical. That doesn't appear to be there. We can't get tender results. Even if its just technical support and stats or tech issuesyThere's some things that are absolutely fundamentally wrong (SME in IT business). ","ywhenever I went on to that site I got frustrated (I've got an email to them about that), to the point where I just thought forget it, just go to the local paper-based directory & it was quicker, because of the complications & a lot of them were technical complications and he was always 'Oh yeah, we're looking into it; we're fixing that; fixing this (SME in the printing trade). ySupport was offered but again it's the marketing officer who did thatyhere's a phone number, if you need support you just call the number and that was it. I suppose the problem is that one person is trying to do all the things. Specifically, what should have been set up was some form of help desk that is equally business related, as it is technical. That doesn't appear to be there. We can't get tender results. Even if its just technical support and stats or tech issuesyThere's some things that are absolutely fundamentally wrong (SME in IT business).",belongs_to_article,1
EDI,"ywhen they come to see the benefits of full cycle [ ie . the next major implementation cycle ] , convincing them is not going to be a problem , because as there is an electronic data interchange culture already built , they will be very used to doing electronic data interchange .","ywhen they come to see the benefits of full cycle [ie. the next major implementation cycle], convincing them is not going to be a problem, because as there is an EDI culture already built, they will be very used to doing EDI. ","ywhen they come to see the benefits of full cycle [ie. the next major implementation cycle], convincing them is not going to be a problem, because as there is an EDI culture already built, they will be very used to doing EDI.",belongs_to_article,1
airlines,"yuqing Ren , Sara kiesler , and Susan R. Fussell address issues of cognitive limitations among people working in complex and dynamic environments such as hospitals , airlines , and disaster response teams in their paper , "" Multiple group Coordination in Complex and Dynamic task Environments : Interruptions , Coping Mechanisms , and technology Recommendations . "" using a hospital operating room as a microcosmic exemplar of such environments , they examine the sources , coping mechanisms , and consequences of coordination breakdowns , and identify three factors whose absence may impede effective responses to unexpected interruptions : ( 1 ) trajectory awareness of what is going on beyond a person 's immediate workspace , ( 2 ) information systems integration , and ( 3 ) information pooling and learning at the organizational level .","the move to online coordination and interaction creates challenges for individuals as well as for organizations. In the 1960s, Nobel laureate herbert Simon famously prophesied, ""In the future, the scarcest resource will be human attention."" In the knowledge economy, that future is upon us. One of the most important constraints on an organization's ability to create value for its stakeholders is its ability to bring the minds of its experts to bear on the complexities of the task at hand. Demands for human attention resources exceed supply. yuqing Ren, Sara kiesler, and Susan R. Fussell address issues of cognitive limitations among people working in complex and dynamic environments such as hospitals, airlines, and disaster response teams in their paper, ""Multiple group Coordination in Complex and Dynamic task Environments: Interruptions, Coping Mechanisms, and technology Recommendations."" using a hospital operating room as a microcosmic exemplar of such environments, they examine the sources, coping mechanisms, and consequences of coordination breakdowns, and identify three factors whose absence may impede effective responses to unexpected interruptions: (1) trajectory awareness of what is going on beyond a person's immediate workspace, (2) IS integration, and (3) information pooling and learning at the organizational level. based on the findings of their in-depth case study, they make technological recommendations to promote trajectory awareness and to automate information gathering and monitoring, so as to facilitate multiple group coordination in complex and dynamic task environments. ","yuqing Ren, Sara kiesler, and Susan R. Fussell address issues of cognitive limitations among people working in complex and dynamic environments such as hospitals, airlines, and disaster response teams in their paper, ""Multiple group Coordination in Complex and Dynamic task Environments: Interruptions, Coping Mechanisms, and technology Recommendations."" using a hospital operating room as a microcosmic exemplar of such environments, they examine the sources, coping mechanisms, and consequences of coordination breakdowns, and identify three factors whose absence may impede effective responses to unexpected interruptions: (1) trajectory awareness of what is going on beyond a person's immediate workspace, (2) IS integration, and (3) information pooling and learning at the organizational level.",related_work,2
hospital,"yuqing Ren , Sara kiesler , and Susan R. Fussell address issues of cognitive limitations among people working in complex and dynamic environments such as hospitals , airlines , and disaster response teams in their paper , "" Multiple group Coordination in Complex and Dynamic task Environments : Interruptions , Coping Mechanisms , and technology Recommendations . "" using a hospital operating room as a microcosmic exemplar of such environments , they examine the sources , coping mechanisms , and consequences of coordination breakdowns , and identify three factors whose absence may impede effective responses to unexpected interruptions : ( 1 ) trajectory awareness of what is going on beyond a person 's immediate workspace , ( 2 ) information systems integration , and ( 3 ) information pooling and learning at the organizational level .","the move to online coordination and interaction creates challenges for individuals as well as for organizations. In the 1960s, Nobel laureate herbert Simon famously prophesied, ""In the future, the scarcest resource will be human attention."" In the knowledge economy, that future is upon us. One of the most important constraints on an organization's ability to create value for its stakeholders is its ability to bring the minds of its experts to bear on the complexities of the task at hand. Demands for human attention resources exceed supply. yuqing Ren, Sara kiesler, and Susan R. Fussell address issues of cognitive limitations among people working in complex and dynamic environments such as hospitals, airlines, and disaster response teams in their paper, ""Multiple group Coordination in Complex and Dynamic task Environments: Interruptions, Coping Mechanisms, and technology Recommendations."" using a hospital operating room as a microcosmic exemplar of such environments, they examine the sources, coping mechanisms, and consequences of coordination breakdowns, and identify three factors whose absence may impede effective responses to unexpected interruptions: (1) trajectory awareness of what is going on beyond a person's immediate workspace, (2) IS integration, and (3) information pooling and learning at the organizational level. based on the findings of their in-depth case study, they make technological recommendations to promote trajectory awareness and to automate information gathering and monitoring, so as to facilitate multiple group coordination in complex and dynamic task environments. ","yuqing Ren, Sara kiesler, and Susan R. Fussell address issues of cognitive limitations among people working in complex and dynamic environments such as hospitals, airlines, and disaster response teams in their paper, ""Multiple group Coordination in Complex and Dynamic task Environments: Interruptions, Coping Mechanisms, and technology Recommendations."" using a hospital operating room as a microcosmic exemplar of such environments, they examine the sources, coping mechanisms, and consequences of coordination breakdowns, and identify three factors whose absence may impede effective responses to unexpected interruptions: (1) trajectory awareness of what is going on beyond a person's immediate workspace, (2) IS integration, and (3) information pooling and learning at the organizational level.",related_work,2
organizational level,"yuqing Ren , Sara kiesler , and Susan R. Fussell address issues of cognitive limitations among people working in complex and dynamic environments such as hospitals , airlines , and disaster response teams in their paper , "" Multiple group Coordination in Complex and Dynamic task Environments : Interruptions , Coping Mechanisms , and technology Recommendations . "" using a hospital operating room as a microcosmic exemplar of such environments , they examine the sources , coping mechanisms , and consequences of coordination breakdowns , and identify three factors whose absence may impede effective responses to unexpected interruptions : ( 1 ) trajectory awareness of what is going on beyond a person 's immediate workspace , ( 2 ) information systems integration , and ( 3 ) information pooling and learning at the organizational level .","the move to online coordination and interaction creates challenges for individuals as well as for organizations. In the 1960s, Nobel laureate herbert Simon famously prophesied, ""In the future, the scarcest resource will be human attention."" In the knowledge economy, that future is upon us. One of the most important constraints on an organization's ability to create value for its stakeholders is its ability to bring the minds of its experts to bear on the complexities of the task at hand. Demands for human attention resources exceed supply. yuqing Ren, Sara kiesler, and Susan R. Fussell address issues of cognitive limitations among people working in complex and dynamic environments such as hospitals, airlines, and disaster response teams in their paper, ""Multiple group Coordination in Complex and Dynamic task Environments: Interruptions, Coping Mechanisms, and technology Recommendations."" using a hospital operating room as a microcosmic exemplar of such environments, they examine the sources, coping mechanisms, and consequences of coordination breakdowns, and identify three factors whose absence may impede effective responses to unexpected interruptions: (1) trajectory awareness of what is going on beyond a person's immediate workspace, (2) IS integration, and (3) information pooling and learning at the organizational level. based on the findings of their in-depth case study, they make technological recommendations to promote trajectory awareness and to automate information gathering and monitoring, so as to facilitate multiple group coordination in complex and dynamic task environments. ","yuqing Ren, Sara kiesler, and Susan R. Fussell address issues of cognitive limitations among people working in complex and dynamic environments such as hospitals, airlines, and disaster response teams in their paper, ""Multiple group Coordination in Complex and Dynamic task Environments: Interruptions, Coping Mechanisms, and technology Recommendations."" using a hospital operating room as a microcosmic exemplar of such environments, they examine the sources, coping mechanisms, and consequences of coordination breakdowns, and identify three factors whose absence may impede effective responses to unexpected interruptions: (1) trajectory awareness of what is going on beyond a person's immediate workspace, (2) IS integration, and (3) information pooling and learning at the organizational level.",related_work,2
IS integration,"yuqing Ren , Sara kiesler , and Susan R. Fussell address issues of cognitive limitations among people working in complex and dynamic environments such as hospitals , airlines , and disaster response teams in their paper , "" Multiple group Coordination in Complex and Dynamic task Environments : Interruptions , Coping Mechanisms , and technology Recommendations . "" using a hospital operating room as a microcosmic exemplar of such environments , they examine the sources , coping mechanisms , and consequences of coordination breakdowns , and identify three factors whose absence may impede effective responses to unexpected interruptions : ( 1 ) trajectory awareness of what is going on beyond a person 's immediate workspace , ( 2 ) information systems integration , and ( 3 ) information pooling and learning at the organizational level .","the move to online coordination and interaction creates challenges for individuals as well as for organizations. In the 1960s, Nobel laureate herbert Simon famously prophesied, ""In the future, the scarcest resource will be human attention."" In the knowledge economy, that future is upon us. One of the most important constraints on an organization's ability to create value for its stakeholders is its ability to bring the minds of its experts to bear on the complexities of the task at hand. Demands for human attention resources exceed supply. yuqing Ren, Sara kiesler, and Susan R. Fussell address issues of cognitive limitations among people working in complex and dynamic environments such as hospitals, airlines, and disaster response teams in their paper, ""Multiple group Coordination in Complex and Dynamic task Environments: Interruptions, Coping Mechanisms, and technology Recommendations."" using a hospital operating room as a microcosmic exemplar of such environments, they examine the sources, coping mechanisms, and consequences of coordination breakdowns, and identify three factors whose absence may impede effective responses to unexpected interruptions: (1) trajectory awareness of what is going on beyond a person's immediate workspace, (2) IS integration, and (3) information pooling and learning at the organizational level. based on the findings of their in-depth case study, they make technological recommendations to promote trajectory awareness and to automate information gathering and monitoring, so as to facilitate multiple group coordination in complex and dynamic task environments. ","yuqing Ren, Sara kiesler, and Susan R. Fussell address issues of cognitive limitations among people working in complex and dynamic environments such as hospitals, airlines, and disaster response teams in their paper, ""Multiple group Coordination in Complex and Dynamic task Environments: Interruptions, Coping Mechanisms, and technology Recommendations."" using a hospital operating room as a microcosmic exemplar of such environments, they examine the sources, coping mechanisms, and consequences of coordination breakdowns, and identify three factors whose absence may impede effective responses to unexpected interruptions: (1) trajectory awareness of what is going on beyond a person's immediate workspace, (2) IS integration, and (3) information pooling and learning at the organizational level.",related_work,2
hospitals,"yuqing Ren , Sara kiesler , and Susan R. Fussell address issues of cognitive limitations among people working in complex and dynamic environments such as hospitals , airlines , and disaster response teams in their paper , "" Multiple group Coordination in Complex and Dynamic task Environments : Interruptions , Coping Mechanisms , and technology Recommendations . "" using a hospital operating room as a microcosmic exemplar of such environments , they examine the sources , coping mechanisms , and consequences of coordination breakdowns , and identify three factors whose absence may impede effective responses to unexpected interruptions : ( 1 ) trajectory awareness of what is going on beyond a person 's immediate workspace , ( 2 ) information systems integration , and ( 3 ) information pooling and learning at the organizational level .","the move to online coordination and interaction creates challenges for individuals as well as for organizations. In the 1960s, Nobel laureate herbert Simon famously prophesied, ""In the future, the scarcest resource will be human attention."" In the knowledge economy, that future is upon us. One of the most important constraints on an organization's ability to create value for its stakeholders is its ability to bring the minds of its experts to bear on the complexities of the task at hand. Demands for human attention resources exceed supply. yuqing Ren, Sara kiesler, and Susan R. Fussell address issues of cognitive limitations among people working in complex and dynamic environments such as hospitals, airlines, and disaster response teams in their paper, ""Multiple group Coordination in Complex and Dynamic task Environments: Interruptions, Coping Mechanisms, and technology Recommendations."" using a hospital operating room as a microcosmic exemplar of such environments, they examine the sources, coping mechanisms, and consequences of coordination breakdowns, and identify three factors whose absence may impede effective responses to unexpected interruptions: (1) trajectory awareness of what is going on beyond a person's immediate workspace, (2) IS integration, and (3) information pooling and learning at the organizational level. based on the findings of their in-depth case study, they make technological recommendations to promote trajectory awareness and to automate information gathering and monitoring, so as to facilitate multiple group coordination in complex and dynamic task environments. ","yuqing Ren, Sara kiesler, and Susan R. Fussell address issues of cognitive limitations among people working in complex and dynamic environments such as hospitals, airlines, and disaster response teams in their paper, ""Multiple group Coordination in Complex and Dynamic task Environments: Interruptions, Coping Mechanisms, and technology Recommendations."" using a hospital operating room as a microcosmic exemplar of such environments, they examine the sources, coping mechanisms, and consequences of coordination breakdowns, and identify three factors whose absence may impede effective responses to unexpected interruptions: (1) trajectory awareness of what is going on beyond a person's immediate workspace, (2) IS integration, and (3) information pooling and learning at the organizational level.",related_work,2
implementation of IT,"ythe ability to maximize the benefits realized from the implementation of information technology investments through the effective use of information , applications and information technology services","ythe ability to maximize the benefits realized from the implementation of IT investments through the effective use of information, applications and IT services ","ythe ability to maximize the benefits realized from the implementation of IT investments through the effective use of information, applications and IT services",belongs_to_article,1
IT,"ythe ability to maximize the benefits realized from the implementation of information technology investments through the effective use of information , applications and information technology services","ythe ability to maximize the benefits realized from the implementation of IT investments through the effective use of information, applications and IT services ","ythe ability to maximize the benefits realized from the implementation of IT investments through the effective use of information, applications and IT services",belongs_to_article,1
106 companies,yielded 246 unique blogs from 106 companies across numerous industries .,"The sample comprised Fortune 500 companies (using the 2011 Fortune 500 list) that used blogs for communication. We identified the blogs through three search methods. First, we searched each company's website for the term ""blog"". Next, we searched Google for the term ""blog"" and restricted the results to each company's domain. This method served as a backup in case the company's website did not have search capabilities or only searched certain parts of the site. Finally, we performed another Google search for the term ""[Company name] blog"". Some Fortune 500 companies hosted dozens of blogs, while others had no apparent company-sanctioned blog. As a result, our search (conducted over a one-month period in autumn 2011) yielded 246 unique blogs from 106 companies across numerous industries. Some companies had more than one company-sanctioned blog that often had different target audiences. ",yielded 246 unique blogs from 106 companies across numerous industries.,belongs_to_article,1
blogs,yielded 246 unique blogs from 106 companies across numerous industries .,"The sample comprised Fortune 500 companies (using the 2011 Fortune 500 list) that used blogs for communication. We identified the blogs through three search methods. First, we searched each company's website for the term ""blog"". Next, we searched Google for the term ""blog"" and restricted the results to each company's domain. This method served as a backup in case the company's website did not have search capabilities or only searched certain parts of the site. Finally, we performed another Google search for the term ""[Company name] blog"". Some Fortune 500 companies hosted dozens of blogs, while others had no apparent company-sanctioned blog. As a result, our search (conducted over a one-month period in autumn 2011) yielded 246 unique blogs from 106 companies across numerous industries. Some companies had more than one company-sanctioned blog that often had different target audiences. ",yielded 246 unique blogs from 106 companies across numerous industries.,belongs_to_article,1
hospital,"yet , as researchers , we do not often delve into key contextual questions such as "" Why does a hospital adopt information technology ? ""","the business vAlue of it literAture is replete with exAMples describing relationships between It and various types of value and suggestions as to what to control, how to measure, and when to measure [5, 32, 33, 54,55]. What seems apparent from this stream of research is that ""context"" matters. yet, as researchers, we do not often delve into key contextual questions such as ""Why does a hospital adopt information technology?"" Is it out of competitive necessity, mimetic behavior, or the belief that there is value associated with the use of It? Or is it simply the federal and state mandates that drive adoption? these and other explanations have been used to explain why firms, in general, adopt It; but as we noted earlier, the intent of the adopter or context under which the It was adopted determines what ""success"" really is. Our paper builds on the premise that financial viability is a key attribute of performance for hospitals but an even more important criterion for decision makers is how It affects quality of patient care-but what is quality patient care? ","yet, as researchers, we do not often delve into key contextual questions such as ""Why does a hospital adopt information technology?""",background_information,3
information technology,"yet , as researchers , we do not often delve into key contextual questions such as "" Why does a hospital adopt information technology ? ""","the business vAlue of it literAture is replete with exAMples describing relationships between It and various types of value and suggestions as to what to control, how to measure, and when to measure [5, 32, 33, 54,55]. What seems apparent from this stream of research is that ""context"" matters. yet, as researchers, we do not often delve into key contextual questions such as ""Why does a hospital adopt information technology?"" Is it out of competitive necessity, mimetic behavior, or the belief that there is value associated with the use of It? Or is it simply the federal and state mandates that drive adoption? these and other explanations have been used to explain why firms, in general, adopt It; but as we noted earlier, the intent of the adopter or context under which the It was adopted determines what ""success"" really is. Our paper builds on the premise that financial viability is a key attribute of performance for hospitals but an even more important criterion for decision makers is how It affects quality of patient care-but what is quality patient care? ","yet, as researchers, we do not often delve into key contextual questions such as ""Why does a hospital adopt information technology?""",background_information,3
e-procurement,"yet when we further address the nature of goods purchased , we find that number of suppliers is positively associated with procurement - procurement for custom goods and negatively associated with procurement - procurement for standard goods .","We examine empirically the relationship of IT with number of suppliers, while prior research has been mainly focused on conceptualization (e.g., [26,33, 37] ) or case research (e.g., [13]). We find no direct relationship between the volume of goods purchased via e-procurement and the number of suppliers at the aggregate level. yet when we further address the nature of goods purchased, we find that number of suppliers is positively associated with e-procurement for custom goods and negatively associated with e-procurement for standard goods. To our knowledge, this is the first time such a distinction has been made. The different relationship is consistent with a TcE-based notion that custom goods procurement involves more asset-specific supplier relationships with greater potential for opportunism, and e-procurement enables buyers to use more suppliers and thus avoid vendor holdup. For commodity goods, in contrast, an efficiently functioning transparent market reduces the risk of opportunism, so e-procurement can be used to automate frequent transactions with fewer suppliers. ","yet when we further address the nature of goods purchased, we find that number of suppliers is positively associated with e-procurement for custom goods and negatively associated with e-procurement for standard goods.",related_work,2
Service Quality,"yet we found no studies that considered the determinants of Service Quality for a specific information systems . the few studies that did identify determinants of Service Quality considered the overall quality of the service provided by the information systems department for all applications and services rather than for a specific information systems . responsiveness and empathy are likely to be important to customer / users when they need help with a specific system ; therefore , more attention should be given to the study of variables that have an impact on Service Quality of an individual information systems .","Model [37] refers to the service quality provided by the IS department across all of its services. While IS support has expanded to include a portfolio of customer-facing e-commerce and Web 2.0 systems, Service Quality becomes an important dimension of IS success [129] . yet we found no studies that considered the determinants of Service Quality for a specific IS. the few studies that did identify determinants of Service Quality considered the overall quality of the service provided by the IS department for all applications and services rather than for a specific IS. responsiveness and empathy are likely to be important to customer/users when they need help with a specific system; therefore, more attention should be given to the study of variables that have an impact on Service Quality of an individual IS. ","yet we found no studies that considered the determinants of Service Quality for a specific IS. the few studies that did identify determinants of Service Quality considered the overall quality of the service provided by the IS department for all applications and services rather than for a specific IS. responsiveness and empathy are likely to be important to customer/users when they need help with a specific system; therefore, more attention should be given to the study of variables that have an impact on Service Quality of an individual IS.",background_information,3
national cultures,yet we also test our conceptual model using data collected from the united States and China and demonstrate some interesting differences between the two national cultures in the national Masculinity → level model .,"The most commonly adopted cross-cultural dimensions come from Hofstede's [43] cross-cultural study, which presents data from more than 100,000 IBM employees in 40 countries. Hofstede defines culture as ""the collective programming of the mind which distinguishes the members of one human group from another"" [43, p. 5]. From Hofstede's study, four cultural dimensions emerged: masculinity/femininity, uncertainty avoidance, power distance, and individualism/collectivism. These four dimensions of cultural values have been widely validated and adopted in more than 100 cross-cultural studies undertaken in various disciplines [38]. Most multicultural IS research has been conducted on the national level, as was done by Kim [55] . However, national-level research may be too broad in that this level of research and ignores the possibility of individual differences among cultural dimensions. addressing this criticism much earlier in other domains, researchers reconceptualized Hofstede's cultural dimensions from a national level to an individual level of analysis and, in doing so, improved the factorial validity of the scales [11, 26] . Thus, as was done in Cao and Everard [11] , we chose to conceptualize and measure culture from an individual-level perspective. yet we also test our conceptual model using data collected from the united States and China and demonstrate some interesting differences between the two national cultures in the national-level model. we now describe these four cultural dimensions. ",yet we also test our conceptual model using data collected from the united States and China and demonstrate some interesting differences between the two national cultures in the national-level model.,background_information,3
China,yet we also test our conceptual model using data collected from the united States and China and demonstrate some interesting differences between the two national cultures in the national Masculinity → level model .,"The most commonly adopted cross-cultural dimensions come from Hofstede's [43] cross-cultural study, which presents data from more than 100,000 IBM employees in 40 countries. Hofstede defines culture as ""the collective programming of the mind which distinguishes the members of one human group from another"" [43, p. 5]. From Hofstede's study, four cultural dimensions emerged: masculinity/femininity, uncertainty avoidance, power distance, and individualism/collectivism. These four dimensions of cultural values have been widely validated and adopted in more than 100 cross-cultural studies undertaken in various disciplines [38]. Most multicultural IS research has been conducted on the national level, as was done by Kim [55] . However, national-level research may be too broad in that this level of research and ignores the possibility of individual differences among cultural dimensions. addressing this criticism much earlier in other domains, researchers reconceptualized Hofstede's cultural dimensions from a national level to an individual level of analysis and, in doing so, improved the factorial validity of the scales [11, 26] . Thus, as was done in Cao and Everard [11] , we chose to conceptualize and measure culture from an individual-level perspective. yet we also test our conceptual model using data collected from the united States and China and demonstrate some interesting differences between the two national cultures in the national-level model. we now describe these four cultural dimensions. ",yet we also test our conceptual model using data collected from the united States and China and demonstrate some interesting differences between the two national cultures in the national-level model.,background_information,3
conceptual model,yet we also test our conceptual model using data collected from the united States and China and demonstrate some interesting differences between the two national cultures in the national Masculinity → level model .,"The most commonly adopted cross-cultural dimensions come from Hofstede's [43] cross-cultural study, which presents data from more than 100,000 IBM employees in 40 countries. Hofstede defines culture as ""the collective programming of the mind which distinguishes the members of one human group from another"" [43, p. 5]. From Hofstede's study, four cultural dimensions emerged: masculinity/femininity, uncertainty avoidance, power distance, and individualism/collectivism. These four dimensions of cultural values have been widely validated and adopted in more than 100 cross-cultural studies undertaken in various disciplines [38]. Most multicultural IS research has been conducted on the national level, as was done by Kim [55] . However, national-level research may be too broad in that this level of research and ignores the possibility of individual differences among cultural dimensions. addressing this criticism much earlier in other domains, researchers reconceptualized Hofstede's cultural dimensions from a national level to an individual level of analysis and, in doing so, improved the factorial validity of the scales [11, 26] . Thus, as was done in Cao and Everard [11] , we chose to conceptualize and measure culture from an individual-level perspective. yet we also test our conceptual model using data collected from the united States and China and demonstrate some interesting differences between the two national cultures in the national-level model. we now describe these four cultural dimensions. ",yet we also test our conceptual model using data collected from the united States and China and demonstrate some interesting differences between the two national cultures in the national-level model.,background_information,3
unanswered questions,"yet several of the other antecedents of Individual Impact had mixed results , leaving many unanswered questions .","yet several of the other antecedents of Individual Impact had mixed results, leaving many unanswered questions. Self-efficacy received mixed support, with two studies by the same author finding support for this relationship [29,30] and three other studies finding no support [30, 82, 134] . One study [30], which contained conflicting results, found that the relationship between self-efficacy and Individual Impact is dependent on the kind of training given to users. Although technology experience is a predictor of other dimensions of success, such as System Quality, this variable has little to no effect on Individual Impact in the literature. Education as a determinant of Individual Impact was, at best, mixed. however, these variables did not exhibit enough support to be identified as moderate or strong antecedents of Individual Impact. ","yet several of the other antecedents of Individual Impact had mixed results, leaving many unanswered questions.",background_information,3
Individual Impact,"yet several of the other antecedents of Individual Impact had mixed results , leaving many unanswered questions .","yet several of the other antecedents of Individual Impact had mixed results, leaving many unanswered questions. Self-efficacy received mixed support, with two studies by the same author finding support for this relationship [29,30] and three other studies finding no support [30, 82, 134] . One study [30], which contained conflicting results, found that the relationship between self-efficacy and Individual Impact is dependent on the kind of training given to users. Although technology experience is a predictor of other dimensions of success, such as System Quality, this variable has little to no effect on Individual Impact in the literature. Education as a determinant of Individual Impact was, at best, mixed. however, these variables did not exhibit enough support to be identified as moderate or strong antecedents of Individual Impact. ","yet several of the other antecedents of Individual Impact had mixed results, leaving many unanswered questions.",background_information,3
virtual groups,yet research on what factors of the interface could enhance individual cognition in virtual groups remains limited .,"Most prior research on EBS (and virtual group performance in general) has focused primarily on social interactions and how technology can help or hurt the sharing of ideas to improve or impair performance [37, 59] . More recently, research has begun to examine individual characteristics and individual cognition as important factors influencing the success of EBS and other group processes [2, 40, 42] . For example, EBS performance is affected by group member characteristics such as domain knowledge, cognitive ability, personality type, and creative skill [54] . Similarly, group performance has been shown to be influenced by the interface of an EBS tool [62] . yet research on what factors of the interface could enhance individual cognition in virtual groups remains limited. ",yet research on what factors of the interface could enhance individual cognition in virtual groups remains limited.,related_work,2
Wikipedia,"yet many of the studies on Wikipedia success , and specifically its article quality , have been rather anecdotal START_CITE [ 27 , END_CITE CITE_b22 START_CITE 79 ] END_CITE CITE_b63 , and our understanding of the factors driving Wikipedia content quality is still limited .","As shown in figure 1, a substantial amount of variance in information quality was explained by our model (R² = 32 percent). the effects of all paths but one were statistically significant. the results of the PLS analysis demonstrate that the interaction between cognitive diversity and task conflict was statistically significant (H1) 13 ; task conflict had a direct negative effect on information quality (H2); and the presence of group members' orientation had direct negative effects on both task conflict (H3) and information quality (H4). the only control variable to have a significant (positive) effect on the outcome variable was article length. the only control path to have a significant , and Wikipedia has been investigated extensively in recent years [39, 88] . yet many of the studies on Wikipedia success, and specifically its article quality, have been rather anecdotal [27, 79] , and our understanding of the factors driving Wikipedia content quality is still limited. In the current study, we explored the relationships between groups' cognitive diversity, the conflict that arises during the collaborative authoring process, group members' orientation, and the quality of Wikipedia articles created by these groups. the composition of Wikipedia has been described in previous studies, demonstrating that articles' editor groups are characterized by a composition of a few highly active members and many occasional editors that make a few contributions [39] , similar to the core-periphery structure that characterizes online communities and OSS projects [19, 47, 59, 65] . However, it is not clear how the core-periphery composition affects collaborative work processes and group performance. We argue that the main limitation of research on group composition in community-based projects is the underspecification of the dimensions on which composition is analyzed. to address this gap, we investigate two dimensions of Wikipedia groups' composition: group members' orientation and diversity in members' knowledge. 15 We contribute to the literature of members' roles by formulating the constructs of group members' (administrative/content) orientation and by proposing measures for this construct. Our model demonstrates three ways in which these group composition constructs, through their interaction with task conflict, impact the quality of the article produced by the group. first, the presence of administrative-oriented members enables the group to diffuse and restrain task conflict [11, 14, 79, 84] . Second, the inclusion of content-oriented members draws on their domain-specific expertise [4] and directly affects article quality. third, the interaction between cognitive diversity and task conflict generates a creative abrasion [55] that positively affects information quality. ","yet many of the studies on Wikipedia success, and specifically its article quality, have been rather anecdotal [27, 79] , and our understanding of the factors driving Wikipedia content quality is still limited.",related_work,2
unified modeling language,"yet experts in the traditional mind - set seem to have difficulty transitioning to object-oriented development : "" prior knowledge of a procedural and functional paradigm may interfere with the learning of UMl [ unified modeling language ] , which is based on an object - oriented paradigm "" [ 65 , p 47 ] .","In addition to the focus transition (from programmer to analyst), traditional and OO software development represent distinct types of approaches [33] . 3 Developers who are experts in traditional development must make a revolutionary change in mind-set as they transition to OO [9, 48] . there has been a considerable amount of research concerning the acquisition of expertise in general (e.g., [6, 68] ), in software development (e.g., [41, 64] ), and in the factors that influence the acquisition of software development expertise (e.g., [53] ). However, little is known about the process and the problems encountered as expert traditional developers transition to OO development techniques. the gap in our understanding of the transition itself leads to our research question: ""How does software development thinking (cognition) with respect to concept understanding change as individuals transition from the traditional to the OO software development approach?"" 4 In order to answer this research question, we used cognitive fit theory as our foundation. From this theory, we developed an instrument that maps an individual's thinking to constructs in either the traditional or the OO software development mind-set. this instrument can determine whether a developer, supposedly working within the OO mind-set, actually has the OO mind-set in place, or is still mentally operating under the traditional mind-set. Note that the research instrument does not measure learning, but rather it measures the expert's mind-set (i.e., approach, thinking) regarding software development concepts. the results of this study have implications for both theory and practice. For theory, it reveals how a software developer's mind-set changes throughout the transition process and identifies problems that may be encountered due to a lack of cognitive fit leading to confusion and falling back on inappropriate mind-sets. For practice, this study provides insights for the development of interventions that may help ease these more radical transitions. theory tHe aDoptioN of oo and the realization of its advantages has been slow, many believe due to its long and difficult learning curve [3, 8, 63, 70] . While the individual transition involves both motivational (e.g., [58] ) and cognitive components, this research is focused on the cognitive aspects of the transition. Novice programmers seem to have a great deal of difficulty understanding and applying OO concepts. this problem is especially acute for developers who are experts in traditional development who are transitioning to the OO development approach [42, 48, 49] . Expert traditional developers should have an easy transition as they have a deep understanding of the semantics of software development and a good understanding of the problem domain. yet experts in the traditional mind-set seem to have difficulty transitioning to OO development: ""prior knowledge of a procedural and functional paradigm may interfere with the learning of UMl [unified modeling language], which is based on an object-oriented paradigm"" [65, p 47]. this apparent paradox is the motivation for the research. ","yet experts in the traditional mind-set seem to have difficulty transitioning to OO development: ""prior knowledge of a procedural and functional paradigm may interfere with the learning of UMl [unified modeling language], which is based on an object-oriented paradigm"" [65, p 47].",related_work,2
Algorithm,"x Proposition 3 . For any given problem instance , if there exists an integer h , 0F h F log BrB q 1 such that k Algorithm I stops at iteration h , the following condition holds :","x Proposition 3. For any given problem instance, if there exists an integer h, 0F h F log BrB q 1 such that k Algorithm I stops at iteration h, the following condition holds:","x Proposition 3. For any given problem instance, if there exists an integer h, 0F h F log BrB q 1 such that k Algorithm I stops at iteration h, the following condition holds:",belongs_to_article,1
marketing,"x According to literature 9,27 and the mail quesw x tionnaire survey 29,32 , marketing strategy development involves a high degree of uncertainty and ambiguity .","x According to literature 9,27 and the mail quesw x tionnaire survey 29,32 , marketing strategy development involves a high degree of uncertainty and ambiguity. Imprecise measures, fuzziness and uncertainty of strategic factors all present challenges to computer-based support. Subjectivity of the decision making process and unclear decision rules make the application of computer support in this field more difficult.","x According to literature 9,27 and the mail quesw x tionnaire survey 29,32 , marketing strategy development involves a high degree of uncertainty and ambiguity.",related_work,2
repeated measure,"x 2 x 2 mixed - model design with one repeated measure ( interruption salience ) START_CITE ( Cohen et al . , 2003 ) END_CITE CITE_b22 15 .","x 2 x 2 mixed-model design with one repeated measure (interruption salience) (Cohen et al., 2003) 15 . RM-ANCOVA effectively separates variation into between-subjects and within-subjects components and develops separate error terms for various segments of the within-subject effects (Cohen et al., 2003; Tabachnick & Fidell, 2007). RM-ANCOVA was particularly appropriate for our study because this study consisted mainly of experimental selections and manipulations as well as observed variables that were easily separable into three analytical steps (see Figure 3 in Appendix) (Kline, 2005) . The relationships in each step were tested simultaneously rather than separately to appropriately estimate the standard errors (Cohen et al., 2003) .","x 2 x 2 mixed-model design with one repeated measure (interruption salience) (Cohen et al., 2003) 15 .",related_work,2
CFI was 0.924,"x 2 /df was 3.67 , which was below the acceptable limit of 5 , RMSEA was 0.054 and the CFI was 0.924 .","The measurement model was estimated by using the robust maximum likelihood method from the asymptotic variancecovariance matrix. EQS 6.1. The fit indices obtained in the measurement model estimation showed that the variables converged towards the factors established in the CFA (see Table 5). x 2 Satorra-Bentler was 750, with 205 degrees of freedom and a p-value of 0.000. x 2 /df was 3.67, which was below the acceptable limit of 5, RMSEA was 0.054 and the CFI was 0.924. Taking the significance of the robust x 2 statistic with caution, and noting the global indicators, the global fit was acceptable.","x 2 /df was 3.67, which was below the acceptable limit of 5, RMSEA was 0.054 and the CFI was 0.924.",belongs_to_article,1
SRMR = 0.039,"x 2 /df = 3.45 ; CFI = 0.947 ; TLI = 0.931 ; RMSEA = 0.062 ; SRMR = 0.039 ; CD = 1.000 , START_CITE [ 29 , END_CITE CITE_b42 START_CITE 75 ] END_CITE CITE_b88 .","x 2 /df = 3.45; CFI = 0.947; TLI = 0.931; RMSEA = 0.062; SRMR = 0.039; CD = 1.000, [29, 75] . Because numerous indices provide moderate to adequate results, we report the results of our CB-SEM-based model in Fig. 3. The results of our hypotheses, as based on the model testing, are shown in Table 3.","x 2 /df = 3.45; CFI = 0.947; TLI = 0.931; RMSEA = 0.062; SRMR = 0.039; CD = 1.000, [29, 75] .",belongs_to_article,1
goodness-of-fit index,"x 2 , GFI ( goodness - of - fit index ) , AGFI ( adjusted GFI ) , NFI ( normalized fit index ) , CFI ( an incremental fit index of improved NFI ) and RMSEA ( root - mean - square error of approximation ) were used to test the goodness of fit of the proposed model .","The fitness measures for the measurement models are shown in Table 5. x 2 , GFI (goodness-of-fit index), AGFI (adjusted GFI), NFI (normalized fit index), CFI (an incremental fit index of improved NFI) and RMSEA (root-mean-square error of approximation) were used to test the goodness of fit of the proposed model. It is suggested x 2 /d.f. should not exceed 3 [5] while GFI and AGFI should be greater than the recommended value of 0.8 [36, 38] . Bentler further suggested that model fit indices should be used, and scores of 0.9 or higher on NFI and CFI should be considered evidence of good fit. Accordingly, all the fitness measures in the study fell into acceptable ranges. Consequently, the proposed model provided a suitable fit.","x 2 , GFI (goodness-of-fit index), AGFI (adjusted GFI), NFI (normalized fit index), CFI (an incremental fit index of improved NFI) and RMSEA (root-mean-square error of approximation) were used to test the goodness of fit of the proposed model.",belongs_to_article,1
goodness of fit,"x 2 , GFI ( goodness - of - fit index ) , AGFI ( adjusted GFI ) , NFI ( normalized fit index ) , CFI ( an incremental fit index of improved NFI ) and RMSEA ( root - mean - square error of approximation ) were used to test the goodness of fit of the proposed model .","The fitness measures for the measurement models are shown in Table 5. x 2 , GFI (goodness-of-fit index), AGFI (adjusted GFI), NFI (normalized fit index), CFI (an incremental fit index of improved NFI) and RMSEA (root-mean-square error of approximation) were used to test the goodness of fit of the proposed model. It is suggested x 2 /d.f. should not exceed 3 [5] while GFI and AGFI should be greater than the recommended value of 0.8 [36, 38] . Bentler further suggested that model fit indices should be used, and scores of 0.9 or higher on NFI and CFI should be considered evidence of good fit. Accordingly, all the fitness measures in the study fell into acceptable ranges. Consequently, the proposed model provided a suitable fit.","x 2 , GFI (goodness-of-fit index), AGFI (adjusted GFI), NFI (normalized fit index), CFI (an incremental fit index of improved NFI) and RMSEA (root-mean-square error of approximation) were used to test the goodness of fit of the proposed model.",belongs_to_article,1
user interfaces,"x 11,18,27,33,37,55 has been done in developing adaptive user interfaces to accommodate the heterogeneity and evolution of user characteristics .","x 11,18,27,33,37,55 has been done in developing adaptive user interfaces to accommodate the heterogeneity and evolution of user characteristics. One of the underlying assumptions for adapting a user interface to the user's preferences is that the fit between both brings greater satisfaction to the user and thus improves the decision quality and the performance of the user. Based on the same rationale, an ADSS should demonstrate similar behavior in this respect.","x 11,18,27,33,37,55 has been done in developing adaptive user interfaces to accommodate the heterogeneity and evolution of user characteristics.",belongs_to_article,1
two maximal admissible groups,"x 1 = R 1 R 2 R 3 R 4 , and the two maximal admissible groups corresponding to x 1 are 1 = R 1 R 4 , and 2 = R 2 R 3 .","x 1 = R 1 R 2 R 3 R 4 , and the two maximal admissible groups corresponding to x 1 are 1 = R 1 R 4 , and 2 = R 2 R 3 .","x 1 = R 1 R 2 R 3 R 4 , and the two maximal admissible groups corresponding to x 1 are 1 = R 1 R 4 , and 2 = R 2 R 3 .",belongs_to_article,1
explanatory variables,"x 0 ij is a K - dimensional row vector , which represents the postspecific characteristics and includes explanatory variables such as tie strength ( 0 - 4 ) , "" likes "" ( ), comments ) , comments ( ), comments ) , post length ( ), comments ) , its quadratic term , length 2 , post type_photo ( 1 = post is a photo , 0 = otherwise ) , and post type_link ( 1 = post is a link , 0 = otherwise ) , as well as the interaction terms : "" likes "" * tie strength , comments*tie strength , length*tie strength , and length 2 * tie strength .","x 0 ij is a K-dimensional row vector, which represents the postspecific characteristics and includes explanatory variables such as tie strength (0-4), ""likes"" (number), comments (number), post length (number), its quadratic term, length 2 , post type_photo (1 = post is a photo, 0 = otherwise), and post type_link (1 = post is a link, 0 = otherwise), as well as the interaction terms: ""likes""* tie strength, comments*tie strength, length*tie strength, and length 2 *tie strength.","x 0 ij is a K-dimensional row vector, which represents the postspecific characteristics and includes explanatory variables such as tie strength (0-4), ""likes"" (number), comments (number), post length (number), its quadratic term, length 2 , post type_photo (1 = post is a photo, 0 = otherwise), and post type_link (1 = post is a link, 0 = otherwise), as well as the interaction terms: ""likes""* tie strength, comments*tie strength, length*tie strength, and length 2 *tie strength.",belongs_to_article,1
standard deviation,x 0 had a cumulative mean of 7.28 with a cumulative standard deviation of 3.19 .,"In the final step, we compared the power-law fit to an alternative distribution using a likelihood-ratio test. We chose the lognormal distribution as an alternative fit as it is the alternative distribution most commonly compared to and tested against a power-law distribution. We used the exact same five steps to fit our data to a lognormal distribution. First, the estimates for the lognormal distribution are log-mean μ = 0.56 and scale σ = 0.72 for all x > x 0 = 1.0. Second, we found the lower bound of x 0 = 10.0, and the tail was fitted to all x > x 0 = 10.0. This led to new estimates: μ= 1.32 and σ = 1.76. Third, the bootstrapping procedure was used to assess the uncertainty of the parameters with 1,000 iterations. x 0 had a cumulative mean of 7.28 with a cumulative standard deviation of 3.19. μ had a cumulative mean of -11.95 with a cumulative standard deviation of 17.18. σ had a cumulative mean of 3.42 with a cumulative standard deviation of 1.97. The number of observations in the tail, i.e. x > x 0 was on average 102.19, with a cumulative standard deviation of 54.54. Next, we conducted two separate analyses, one using the Anderson-Darling statistic and one using the Kolmogorov-Smirnov statistic. The two analyses yielded p-values of 0.02 (Anderson-Darling) and 0.03 (Kolmogorov-Smirnov), suggesting that our data is significantly different from a lognormal distribution.",x 0 had a cumulative mean of 7.28 with a cumulative standard deviation of 3.19.,belongs_to_article,1
IT,"x * PC units of IT services are demanded as a function of the price and quality offered by the IT department , and a surplus to the consuming unit of S I * PC is generated net of price for IT services . START_CITE 3 END_CITE CITE_b2 A profit of π IT is generated by the IT department .","x * PC units of IT services are demanded as a function of the price and quality offered by the IT department, and a surplus to the consuming unit of S I * PC is generated net of price for IT services. 3 A profit of π IT is generated by the IT department. 4","x * PC units of IT services are demanded as a function of the price and quality offered by the IT department, and a surplus to the consuming unit of S I * PC is generated net of price for IT services. 3 A profit of π IT is generated by the IT department.",belongs_to_article,1
TAM,www START_CITE [ 90 ] END_CITE CITE_b82 START_CITE [ 91 ] END_CITE CITE_b83 START_CITE [ 92 ] END_CITE CITE_b84 START_CITE [ 93 ] END_CITE CITE_b85 START_CITE [ 94 ] END_CITE CITE_b86 START_CITE [ 95 ] END_CITE CITE_b87 [ 96 ] START_CITE [ 97 ] END_CITE CITE_b88 START_CITE [ 98 ] END_CITE CITE_b89 START_CITE [ 99 ] END_CITE CITE_b90 START_CITE [ 100 ] END_CITE CITE_b91 [ 101 ] START_CITE [ 102 ] END_CITE CITE_b92 [ 103 ] The aim of our study was to examine the convergence or divergence of technology acceptance model relationships across different settings to make better claims on and give an objective picture of results of research using technology acceptance model in recent years .,"Many studies have replicated, extended, and used TAM but there are some aspects which remain unclear. First, the subjective norm has had a mixed and inconclusive role; it has been defined as ''a person's perception that most people who are important to him think he should or should not perform the behavior in question'' [24] . Some studies found considerable impacts of it on the dependent variables [14, 39, 71] . However, others did not find significant effects [45, 72] . Second, few conclusions have been drawn on the different settings used in testing the model. TAM has been tested with students as subjects [23, 85] and with non-students [3, 20] . It has been applied to microcomputer technologies [36, 38, 52, 53] and other technologies [12, 21, 43, 91] . And it has been used in Western cultures [2, 37, 63] and others [33, 51, 87] . www [90] [91] [92] [93] [94] [95] [96] [97] [98] [99] [100] [101] [102] [103] The aim of our study was to examine the convergence or divergence of TAM relationships across different settings to make better claims on and give an objective picture of results of research using TAM in recent years. Since the field has been dominated by quantitative research approaches, we conducted a metaanalysis on the literature, integrating a large volume of results to determine whether research findings were homogeneous. We thus added to two previous metastudies. Ma and Liu [58] also provided a quantitative meta-analysis, but only focused on three relationships: (1) perceived usefulness ! perceived ease of use, (2) perceived usefulness ! technology acceptance (use), and (3) perceived ease of use ! technology acceptance (use). Legris et al. [49] performed a qualitative metaanalysis and concluded that TAM was a useful model, but had to include human and social change process variables (e.g. the subjective norm). In addressing their limitations, we included the subjective norm in our analysis and additionally considered the impact of three types of settings as moderating variables. We also used structural equation modeling to assess overall model fit and identified additional paths to improve the model.",www [90] [91] [92] [93] [94] [95] [96] [97] [98] [99] [100] [101] [102] [103] The aim of our study was to examine the convergence or divergence of TAM relationships across different settings to make better claims on and give an objective picture of results of research using TAM in recent years.,belongs_to_article,1
experiments,"writing a report , statement conclusion , e - mail , etc . ) Integration of experiments 37 .","writing a report, statement conclusion, e-mail, etc.) Integration of experiments 37. You integrate the experiences of each partner to improve your practices","writing a report, statement conclusion, e-mail, etc.) Integration of experiments 37.",belongs_to_article,1
economic theory,"writes that "" the broadening of economic theory has coincided with new empirical research by economists on social interactions . Unfortunately , the empirical literature has not shown much progress "" .","Second, we conducted this study in part to answer Manski's (2000) call to analyze social interactions in an economics paradigm. Specifically, in reviewing the intersection between economics and sociology, Manski (2000, p. 117) writes that ""the broadening of economic theory has coincided with new empirical research by economists on social interactions. Unfortunately, the empirical literature has not shown much progress"". He supports this view with two reasons: 1) the ""dearth of clear thinking in the empirical literature"" and 2) ""the inherent difficulty of drawing inferences from the data"". In line with his perspective, we believe empirical modeling of constraint, expectation, and preference interactions with a clear, standard setup that affords clear conceptualization of the interaction process (e.g., GPI and our focal manipulations) could be a way to understand interactions in a market setting. The results from this study allow us to observe the nature and outcome of expectations and preferences interactions in a cooperative gaming mode.","writes that ""the broadening of economic theory has coincided with new empirical research by economists on social interactions. Unfortunately, the empirical literature has not shown much progress"".",related_work,2
decision-making,"write : "" technology acceptance model presumes that [ BI ] is formed as a result of conscious decision - making processes . ""","A second approach to verifying accuracy is to try to verify the underlying process that an empirical relationship is assumed to reflect. For instance, TAM researchers typically assume that the relationships they estimate reflect real-world processes. Kim and Malhotra (2005, p. 743) write: ""TAM presumes that [BI] is formed as a result of conscious decision-making processes."" Thus, one way to test the accuracy of the relationships is to verify the mediating mechanism(s) (Mathieu et al. 2008 )-to check if users do engage in conscious decision-making processes before forming their usage intentions or instead follow a different process, e.g., informed by habit or emotion. This can be checked by using a method that sheds light on the process. Experimental researchers have long used process tracing to check their assumptions about the processes that individuals engage in (Todd and Benbasat 1987) . We are not familiar with any process tracing studies of TAM, but such an approach could be used. Researchers with a preference for lab settings could use functional magnetic resonance imaging (fMRI) to examine if/how individuals engage in conscious decision-making processes when forming their intentions to use an IT (Dimoka et al. 2007) , and researchers with a preference for field settings could use experience sampling methods to learn the cognitive and emotional processes that lead people to use IT in specific ways (Ortiz de Guinea and Webster 2013).","write: ""TAM presumes that [BI] is formed as a result of conscious decision-making processes.""",related_work,2
IS planning,"would still have made sense [ therefore ] an opportunity was missed for such studies to offer more grounded insights into IS phenomena by including and articulating the role of information technology , for example , in . . . the process of IS planning . ""","would still have made sense [therefore] an opportunity was missed for such studies to offer more grounded insights into IS phenomena by including and articulating the role of information technology, for example, in . . . the process of IS planning.""","would still have made sense [therefore] an opportunity was missed for such studies to offer more grounded insights into IS phenomena by including and articulating the role of information technology, for example, in . . . the process of IS planning.""",background_information,3
IT investment,"would imply increases of 20 - 30 % per year in constant dollar information technology stock , which is consistent with press reports of dramatically increasing information technology investment .","The VIC is bounded below by zero (single industry or no vertical linkages) and above by one, although the upper bound would not be attainable in our sample given the structure of the I-O matrix. See Maddigan (1981) for more discussion of the properties of this index. would imply increases of 20-30% per year in constant dollar IT stock, which is consistent with press reports of dramatically increasing IT investment.","would imply increases of 20-30% per year in constant dollar IT stock, which is consistent with press reports of dramatically increasing IT investment.",background_information,3
retailers,would have important implications for retailers ' commerce - commerce product colocation practices .,"would have important implications for retailers' e-commerce product colocation practices. Alternatively, it would be fruitful to examine the interplay between these automated recommendations (co-view and co-purchase) and the consumer-generated WOM recommendations. One could thus identify whether these two recommendation mechanisms are substitutable, complementary, or independent in driving product demand. The findings would have important implications for online retailers' marketing strategies and the design of e-commerce websites.",would have important implications for retailers' e-commerce product colocation practices.,background_information,3
crowdfunding,"would be similar to angel investors compared to those in reward - based crowdfunding , implying that success at reward - based crowdfunding will send a more distinct signal from receiving angel investing than success at equity - based crowdfunding .","would be similar to angel investors compared to those in reward-based crowdfunding, implying that success at reward-based crowdfunding will send a more distinct signal from receiving angel investing than success at equity-based crowdfunding. 3 Our unit of analysis is the startup, the entity of decision-making, given that each startup would choose either crowdfunding or angels as its initial funding source. Here, the campaign is a means for raising funds through crowdfunding, on which startups would spend time and resources. It is comparable to startups' efforts in preparing their materials and communicating them to angels. In our paper, we limit the use of the term ""campaign"" to the specific crowdfunding context to mitigate potential confusion.","would be similar to angel investors compared to those in reward-based crowdfunding, implying that success at reward-based crowdfunding will send a more distinct signal from receiving angel investing than success at equity-based crowdfunding.",background_information,3
decision-making,"would be a spatial decision support system SDSS , such systems have been shown to facilitate w x decision - making 13 .",". population data . The growth of GIS has lead to the increasing availability of spatially related data, with digital mapping data being readily available for most regions. As this data becomes widely used, its cost will fall, providing a valuable source of additional information for vehicle routing decision support software. In developed countries digital data is being w x prepared for use by car navigation software 19 and this data is relevant to a wide range of routing problems. Therefore, a useful contribution can be made, by GIS techniques, to the design of a decision support system for vehicle routing. Such a system Ž . would be a spatial decision support system SDSS , such systems have been shown to facilitate w x decision-making 13 . Various types of SDSS are being used which combine GIS information with w x appropriate algorithms. Muller 28 identified SDSS as a growth area in the application of GIS technology. A recognised existing application of SDSS that is relevant to vehicle routing is in facility location w x applications 3 . Facility location is an area of application of traditional management science techniques where the spatial dimension to the problem is obvious.","would be a spatial decision support system SDSS , such systems have been shown to facilitate w x decision-making 13 .",background_information,3
theories,"works of START_CITE Okhuysen and END_CITE CITE_b84 START_CITE Bonardi ( 2011 ) and END_CITE CITE_b84 START_CITE Murray , Evers , and END_CITE CITE_b81 START_CITE Janda ( 1995 ) END_CITE CITE_b81 who provide important considerations when combining theories START_CITE ( Lowry et al . , 2019 ) END_CITE CITE_b73 . START_CITE Okhuysen and Bonardi ( 2011 ) END_CITE CITE_b84 suggest to consider the proximity and compatibility between underlying assumptions between the two theories .","works of Okhuysen and Bonardi (2011) and Murray, Evers, and Janda (1995) who provide important considerations when combining theories (Lowry et al., 2019) . Okhuysen and Bonardi (2011) suggest to consider the proximity and compatibility between underlying assumptions between the two theories. Proximity refers to the 'conceptual distance that exists between the phenomena that the lenses address in their original conception' (Okhuysen & Bonardi, 2011, p. 7). For example, as Okhuysen and Bonardi (2011) Smith et al. (1995) , are rooted in cognitive psychology and seek to explain the processes underpinning the awareness held in the minds of individuals and their interaction with the environment (Stanton et al., 2017) . This focus incorporates the psychological processes involved (eg, mental models, schema, perception) and the nature of situation awareness itself (eg, the person's situation awareness comprising knowledge about something). Conversely, PMT is a major health psychology theory aimed at explaining the cognitive mediation process of behavioural change in terms of threat and coping appraisal based on processing of information from environmental and intrapersonal sources (Maddux & Rogers, 1983) . Thus, although they have been applied in different research fields, PCM of situation awareness and PMT are proximal theories and, as such, it is appropriate to combine situation awareness and PMT from this perspective.","works of Okhuysen and Bonardi (2011) and Murray, Evers, and Janda (1995) who provide important considerations when combining theories (Lowry et al., 2019) . Okhuysen and Bonardi (2011) suggest to consider the proximity and compatibility between underlying assumptions between the two theories.",related_work,2
first-order logic,"works for comparing the features of various hy - We were motivated to generalize the concept pertext systems . It is hoped that a hypertext interof hypertext by , among other things , the need we change format will emerge permitting hypertext saw for automatic linking and for more general systems to share their data ( see START_CITE [ 44 ] END_CITE CITE_b26 ) . operation upon application objects ( any system with logic, and called While our model of generalized hypertext for the most part can be mapped to these meta - level tween pairs of nodes ( called the source and sink reference models , our focus differs in two impor - link endpoints ) , are also declared in a data base . tant ways . First , instead of mapping among hy - Embedded in nodes -as part of the declarations pertext systems , we are concerned with mapping establishing the nodes -are link anchors or butnon - hypertext information system applications to tons , which indicate the presence of a link and a hypertext interface for the reasons expressed which usually are highlighted in some manner earlier . Here , our major contribution is the tech - when the node embedding them is displayed . The nique of bridge laws ( see § 6 ) , which model this number of buttons at a node , and the number of integration . Second , in the existing models of links in a document , is essentially unlimited . hypertext virtual structures and computation are A hypertext system is software for creating , ad hoc , as are mechanisms most current systems editing and maintaining hyperdocuments anduse to implement them ( e.g. , [ 51 ] ) . Because most more interestingly and to the point for present of our applications are dynamic in nature ( e.g. , purposes -for browsing through , or exploring , decision support system which execute decision models on the fly ) , hyperdocuments . The browsing concept is central we provide a structure modeling hypertext inter - to the hypertext idea [ 14 , START_CITE 39 , END_CITE CITE_b41 START_CITE 46 ] END_CITE CITE_b30 , and it is the action , which can only be instantiated at run - time , hypertext browsing functions that are the subject Virtual structures and filtered computation START_CITE [ 21 ] END_CITE CITE_b6 of this paper and of our logic models . Our generdefined by bridge laws -both are at the core of alization of hypertext generalizes these browsing our hypertext model . Our system - level hypertext functions . A user browses a hyperdocument by engine performs inference for every hypertext viewing a displayed hypertext node , and selecting browsing command in a principled , well struc - a link to another node . Typically this is achieved tured manner , by pointing with a mouse to the button high - The strategy and organization for the remain - lighted in the node 's text that names a link emader of the paper is this . We assume the reader is nating from that node , and by clicking with the familiar with hypertext and first - order logic . In § 2 mouse . Selecting a link causes the system to trawe develop a particular concept of hypertext , verse the link , i.e. , to determine which node is at which we call basic hypertext . We do not claim the link 's distal endpoint , here the sink . In basic that basic hypertext is an adequate representa - hypertext ( as we are characterizing the concept ) , tion of all or most hypertext systems . Rather , we once a link is traversed the destination node is claim that it is reasonably close to , and very much displayed . Clicking on a button , then , produces a in the spirit of , the mainstream of first - generation new display of a node . hypertext ideas . Our purpose in presenting basic","works for comparing the features of various hy-We were motivated to generalize the concept pertext systems. It is hoped that a hypertext interof hypertext by, among other things, the need we change format will emerge permitting hypertext saw for automatic linking and for more general systems to share their data (see [44] ). operation upon application objects (often called While our model of generalized hypertext for the most part can be mapped to these meta-level tween pairs of nodes (called the source and sink reference models, our focus differs in two impor-link endpoints), are also declared in a data base. tant ways. First, instead of mapping among hy-Embedded in nodes -as part of the declarations pertext systems, we are concerned with mapping establishing the nodes -are link anchors or butnon-hypertext information system applications to tons, which indicate the presence of a link and a hypertext interface for the reasons expressed which usually are highlighted in some manner earlier. Here, our major contribution is the tech-when the node embedding them is displayed. The nique of bridge laws (see §6), which model this number of buttons at a node, and the number of integration. Second, in the existing models of links in a document, is essentially unlimited. hypertext virtual structures and computation are A hypertext system is software for creating, ad hoc, as are mechanisms most current systems editing and maintaining hyperdocuments anduse to implement them (e.g., [51]). Because most more interestingly and to the point for present of our applications are dynamic in nature (e.g., purposes -for browsing through, or exploring, DSS which execute decision models on the fly), hyperdocuments. The browsing concept is central we provide a structure modeling hypertext inter-to the hypertext idea [14, 39, 46] , and it is the action, which can only be instantiated at run-time, hypertext browsing functions that are the subject Virtual structures and filtered computation [21] of this paper and of our logic models. Our generdefined by bridge laws -both are at the core of alization of hypertext generalizes these browsing our hypertext model. Our system-level hypertext functions. A user browses a hyperdocument by engine performs inference for every hypertext viewing a displayed hypertext node, and selecting browsing command in a principled, well struc-a link to another node. Typically this is achieved tured manner, by pointing with a mouse to the button high-The strategy and organization for the remain-lighted in the node's text that names a link emader of the paper is this. We assume the reader is nating from that node, and by clicking with the familiar with hypertext and first-order logic. In §2 mouse. Selecting a link causes the system to trawe develop a particular concept of hypertext, verse the link, i.e., to determine which node is at which we call basic hypertext. We do not claim the link's distal endpoint, here the sink. In basic that basic hypertext is an adequate representa-hypertext (as we are characterizing the concept), tion of all or most hypertext systems. Rather, we once a link is traversed the destination node is claim that it is reasonably close to, and very much displayed. Clicking on a button, then, produces a in the spirit of, the mainstream of first-generation new display of a node. hypertext ideas. Our purpose in presenting basic","works for comparing the features of various hy-We were motivated to generalize the concept pertext systems. It is hoped that a hypertext interof hypertext by, among other things, the need we change format will emerge permitting hypertext saw for automatic linking and for more general systems to share their data (see [44] ). operation upon application objects (often called While our model of generalized hypertext for the most part can be mapped to these meta-level tween pairs of nodes (called the source and sink reference models, our focus differs in two impor-link endpoints), are also declared in a data base. tant ways. First, instead of mapping among hy-Embedded in nodes -as part of the declarations pertext systems, we are concerned with mapping establishing the nodes -are link anchors or butnon-hypertext information system applications to tons, which indicate the presence of a link and a hypertext interface for the reasons expressed which usually are highlighted in some manner earlier. Here, our major contribution is the tech-when the node embedding them is displayed. The nique of bridge laws (see §6), which model this number of buttons at a node, and the number of integration. Second, in the existing models of links in a document, is essentially unlimited. hypertext virtual structures and computation are A hypertext system is software for creating, ad hoc, as are mechanisms most current systems editing and maintaining hyperdocuments anduse to implement them (e.g., [51]). Because most more interestingly and to the point for present of our applications are dynamic in nature (e.g., purposes -for browsing through, or exploring, DSS which execute decision models on the fly), hyperdocuments. The browsing concept is central we provide a structure modeling hypertext inter-to the hypertext idea [14, 39, 46] , and it is the action, which can only be instantiated at run-time, hypertext browsing functions that are the subject Virtual structures and filtered computation [21] of this paper and of our logic models. Our generdefined by bridge laws -both are at the core of alization of hypertext generalizes these browsing our hypertext model. Our system-level hypertext functions. A user browses a hyperdocument by engine performs inference for every hypertext viewing a displayed hypertext node, and selecting browsing command in a principled, well struc-a link to another node. Typically this is achieved tured manner, by pointing with a mouse to the button high-The strategy and organization for the remain-lighted in the node's text that names a link emader of the paper is this. We assume the reader is nating from that node, and by clicking with the familiar with hypertext and first-order logic. In §2 mouse. Selecting a link causes the system to trawe develop a particular concept of hypertext, verse the link, i.e., to determine which node is at which we call basic hypertext. We do not claim the link's distal endpoint, here the sink. In basic that basic hypertext is an adequate representa-hypertext (as we are characterizing the concept), tion of all or most hypertext systems. Rather, we once a link is traversed the destination node is claim that it is reasonably close to, and very much displayed. Clicking on a button, then, produces a in the spirit of, the mainstream of first-generation new display of a node. hypertext ideas. Our purpose in presenting basic",belongs_to_article,1
job satisfaction,"workplace and one 's career trajectory , and should lead to higher job satisfaction .","workplace and one's career trajectory, and should lead to higher job satisfaction. We explore:","workplace and one's career trajectory, and should lead to higher job satisfaction.",belongs_to_article,1
career,workload ) and hindrances ( negative : career - family conflict ) .,"Perceived workload (PW) has often been identified as a predictor of job-focused exhaustion, particularly for IS professionals (e.g., Kouvonen et al. 2005; Rigas 2009 ). PW is applicable to the context of ISCE, because the IS work environment has been characterized as having work practices that often include long hours, late nights, after-hour meetings, on-call duty, and a continual state of rush or crisis (Ahuja 2002) . The IS field, by its very nature and reliance on information and communication technologies, allows work to always be accessible. In a study of IS professionals conducted by Riemenschneider et al. (2006) , one participant stated, ""I was on call 24 hours a day 7 days a week, worked every Sunday, got calls in the middle of the night"" (p. 68). workload) and hindrances (negative: career-family conflict). Challenge demands are stressors that have the potential to promote mastery and can be perceived as growth opportunities, whereas hindrance demands are stressors that have the potential to limit mastery and can be perceived as barriers Due to the high frequency of this construct's occurrence over a variety of studies, the PW characteristic may not be job specific but may apply to many areas of the IS field. One of our focus group participants stated, ""the demands from customers who don't understand what's required or involved behind a new [feature] request…can be very stressful."" This example may also illustrate that perceptions regarding workload are not necessarily linked to one job (e.g., programmer, consultant) or one organization (e.g., IBM, KPMG) but may permeate the culture of the IS profession. If an individual moves through his/her career in IS and perceives a consistent demand for an excessive amount of work to be accomplished in the allotted time, the individual may become exhausted with the IS profession.",workload) and hindrances (negative: career-family conflict).,belongs_to_article,1
software risk,"working with centralised architectures avoided gold plating significantly better than those developing systems First , Boehm 's ' top - ten ' list of software risks may ignore some software risk items that are important for man - with distributed architectures .","(2) What factors in the development process and environment correlate significantly with the improved associated with the late delivery times in external projects. It also lends support to the popular argument that risk management performance? We observed that risk management performance depends on several environ-outsourcing improves development control. Organizations preferring obligatory use of development methods mental contingencies. These include the size of the IS department, the project size, project management train-scored significantly better in achieving stable resource consumption than organizations relying on voluntary ing, project managers' experience, and the use of systems development methods. This suggests that risk man-use. This finding is in line with Humphrey's (1989) theory in that a disciplined systems development agement performance can be improved by leveraging on the experience of the project managers, by improving environment leads to better control of the development process. The extent of project management training and management training, by disciplining the development process, by minimising the project size, and by stan-applied hardware architecture both reduced the negative effect of gold plating. Well trained project managers suc-dardising and controlling essential components in the application development (such as user interfaces). ceeded significantly better in that they could focus on essential aspects of the design. Finally, project managers Our findings need to be interpreted with caution because the study suffers from a number of limitations. working with centralised architectures avoided gold plating significantly better than those developing systems First, Boehm's 'top-ten' list of software risks may ignore some software risk items that are important for man-with distributed architectures. This can be explained by two salient features of distributed systems. For one thing, agers, or end users. It is important to remember that Boehm's studies focused exclusively on project man-they offer more 'bells and whistles' to develop fancy user interfaces than the earlier text based centralised sol-agers' perceptions of software development (Lyytinen et al, 1996b) . Another problem stemming from Boehm's utions. Second, less managerial control can be exercised in distributed environments.","working with centralised architectures avoided gold plating significantly better than those developing systems First, Boehm's 'top-ten' list of software risks may ignore some software risk items that are important for man-with distributed architectures.",background_information,3
personal computer,"working on and personal computer ( measures END_PARAGRAPH_TAG START_PARAGRAPH_TAG Computer self-efficacy I feel confident. . . working on a personal computer ) getting software handling a disk correctly learning to use a variety of programs up and running using the user 's guide when help is needed entering and saving data ( numbers or words ) into and file escaping / exiting from and program or software handling a disk correctly learning to use a variety of programs calling up and data file to view on the monitor screen understanding terms / words relating to computer hardware understanding terms / words relating to computer software handling a disk correctly learning to use a variety of programs handling and disk correctly learning to use and variety of programs ( software handling a disk correctly learning to use a variety of programs ) making selections from an on - screen menu using and printer to make and ' ' hardcopy ' ' of my work copying and disk copying an individual file adding and deleting information from and data file moving the cursor around the monitor screen writing simple programs for the computers using the computer to write and letter or essay describing the function of computer hardware ( keyboard , monitor , disk drives , computer processing unit ) understanding the three stages of data processing : input , processing , output getting help for problems in the computer systems using the computer to organize information getting rid of files when they are no longer needed organizing and managing files troubleshooting computer problems Internet self - efficacy I feel confident","Computer self-efficacy I feel confident . . . working on a personal computer (microcomputer) getting software up and running using the user's guide when help is needed entering and saving data (numbers or words) into a file escaping/exiting from a program or software calling up a data file to view on the monitor screen understanding terms/words relating to computer hardware understanding terms/words relating to computer software handling a disk correctly learning to use a variety of programs (software) making selections from an on-screen menu using a printer to make a ''hardcopy'' of my work copying a disk copying an individual file adding and deleting information from a data file moving the cursor around the monitor screen writing simple programs for the computers using the computer to write a letter or essay describing the function of computer hardware (keyboard, monitor, disk drives, computer processing unit) understanding the three stages of data processing: input, processing, output getting help for problems in the computer systems using the computer to organize information getting rid of files when they are no longer needed organizing and managing files troubleshooting computer problems Internet self-efficacy I feel confident . . . browsing the World Wide Web (WWW) surfing the World Wide Web (WWW) encrypting my email messages encrypting my email messages before sending them over the Internet decrypting email messages that I receive decrypting email messages creating a home page for the World Wide Web (WWW) making changes on a home page downloading from another computer scanning pictures to save on the computer sending a fax via the computer receiving a fax on my computer recovering a file I accidentally deleted editing (size, color) a scanned picture finding information on the World Wide Web (WWW) User attitude I like working with computers. I look forward to those aspects of my job that require me to use a computer Once I start working on the computer, I find it hard to stop Using a computer is frustrating for me I get bored quickly when working on a computer Computer anxiety I feel apprehensive about using computers It scares me to think that I could cause the computer to destroy a large amount of information by hitting the wrong key I hesitate to use a computer for fear of making mistakes I cannot correct Computers are somewhat intimidating to me Appendix A (Continued )","working on a personal computer (microcomputer) getting software up and running using the user's guide when help is needed entering and saving data (numbers or words) into a file escaping/exiting from a program or software calling up a data file to view on the monitor screen understanding terms/words relating to computer hardware understanding terms/words relating to computer software handling a disk correctly learning to use a variety of programs (software) making selections from an on-screen menu using a printer to make a ''hardcopy'' of my work copying a disk copying an individual file adding and deleting information from a data file moving the cursor around the monitor screen writing simple programs for the computers using the computer to write a letter or essay describing the function of computer hardware (keyboard, monitor, disk drives, computer processing unit) understanding the three stages of data processing: input, processing, output getting help for problems in the computer systems using the computer to organize information getting rid of files when they are no longer needed organizing and managing files troubleshooting computer problems Internet self-efficacy I feel confident",background_information,3
local area network,work that provides direct connectivity to anyone over a local area network ( local area network ) or Internet service provider ( Internet service provider ) .,work that provides direct connectivity to anyone over a local area network (LAN) or Internet service provider (ISP). 4) Intranet. A corporate LAN or wide area network (WAN) that uses Internet technology and is secured behind an organization's firewalls. The intranet supports and promotes more effective internal information sharing and an organization's internal business processes. 5) Extranet. A collaborated network that uses Internet technology to link businesses with their supply chain and provides a degree of security and privacy from competitors.,work that provides direct connectivity to anyone over a local area network (LAN) or Internet service provider (ISP).,background_information,3
hardware,"work sheets , will be produced by each participant or group of participants . Recording of the decision making steps is the simplest approach ; Artificial Intelligence offers alternatives with intelligent guidance . 6 . 9 . Multimedia hardware requirements 6.6 . Application deue,!opment features in the MS - GDS tool kit Applications are oriented toward data types . The tool kit and actual applications should not different greatly . Both would have similar user interface displays . The exceptions would be production applications and physical sensor data collection . The applications would be developed by "" learn modes "" in which the designer goes through the steps , and the sy ~ tems copies the process .","work sheets, will be produced by each participant or group of participants. Recording of the decision making steps is the simplest approach; AI offers alternatives with intelligent guidance. 6. 9. Multimedia hardware requirements 6.6. Application deue,!opment features in the MS-GDS tool kit Applications are oriented toward data types. The tool kit and actual applications should not different greatly. Both would have similar user interface displays. The exceptions would be production applications and physical sensor data collection. The applications would be developed by ""learn modes"" in which the designer goes through the steps, and the sy~tems copies the process.","work sheets, will be produced by each participant or group of participants. Recording of the decision making steps is the simplest approach; AI offers alternatives with intelligent guidance. 6. 9. Multimedia hardware requirements 6.6. Application deue,!opment features in the MS-GDS tool kit Applications are oriented toward data types. The tool kit and actual applications should not different greatly. Both would have similar user interface displays. The exceptions would be production applications and physical sensor data collection. The applications would be developed by ""learn modes"" in which the designer goes through the steps, and the sy~tems copies the process.",belongs_to_article,1
data collection,"work sheets , will be produced by each participant or group of participants . Recording of the decision making steps is the simplest approach ; Artificial Intelligence offers alternatives with intelligent guidance . 6 . 9 . Multimedia hardware requirements 6.6 . Application deue,!opment features in the MS - GDS tool kit Applications are oriented toward data types . The tool kit and actual applications should not different greatly . Both would have similar user interface displays . The exceptions would be production applications and physical sensor data collection . The applications would be developed by "" learn modes "" in which the designer goes through the steps , and the sy ~ tems copies the process .","work sheets, will be produced by each participant or group of participants. Recording of the decision making steps is the simplest approach; AI offers alternatives with intelligent guidance. 6. 9. Multimedia hardware requirements 6.6. Application deue,!opment features in the MS-GDS tool kit Applications are oriented toward data types. The tool kit and actual applications should not different greatly. Both would have similar user interface displays. The exceptions would be production applications and physical sensor data collection. The applications would be developed by ""learn modes"" in which the designer goes through the steps, and the sy~tems copies the process.","work sheets, will be produced by each participant or group of participants. Recording of the decision making steps is the simplest approach; AI offers alternatives with intelligent guidance. 6. 9. Multimedia hardware requirements 6.6. Application deue,!opment features in the MS-GDS tool kit Applications are oriented toward data types. The tool kit and actual applications should not different greatly. Both would have similar user interface displays. The exceptions would be production applications and physical sensor data collection. The applications would be developed by ""learn modes"" in which the designer goes through the steps, and the sy~tems copies the process.",belongs_to_article,1
longitudinal studies,work on the attempts to replace physical ( and costly ) car crashes in safety design in the automotive industry with simulated crashes is one of few longitudinal studies of simulations in organizations .,"This insight overlaps with several other studies. work on the attempts to replace physical (and costly) car crashes in safety design in the automotive industry with simulated crashes is one of few longitudinal studies of simulations in organizations. In his study, Leonardi (2012, p. 14) underscores the sociotechnical conditions for digital representations in the form of simulations becoming 'real' as ""there is no guarantee that people will use the new information [from simulation] or that they will believe it"". Simulation models are interesting and relevant to our analysis as they theoretically offer the possibility of decoupling (further) the tie between digital representation and its physical referent. However, a consistent theme in Leonardi (2012, p. 62) analysis is how the simulation models need ongoing validation with physical car crashes ""to verify the accuracy of their simulations by confirming them with physical tests [of car crashes]"". As Bailey et al. (2012 Bailey et al. ( , p. 1500, drawing on Leonardi's study, point out ""this tight coupling in simulation means that people who create representations are highly dependent on physical referents"". Similarly, in their study of Gehry's use of simulation models in architecture, Boland, Lyytinen, and Yoo (2007) underscore the close link maintained with a physical model of the building in question. Our mechanism of material tethering spells out how the link digital/physical is forged. For instance, during the bi-annual maintenance shutdown, the values for the wear on pipes and chokes predicted by the algorithm are compared directly with the physical measurements of the equipment during maintenance.",work on the attempts to replace physical (and costly) car crashes in safety design in the automotive industry with simulated crashes is one of few longitudinal studies of simulations in organizations.,belongs_to_article,1
automotive industry,work on the attempts to replace physical ( and costly ) car crashes in safety design in the automotive industry with simulated crashes is one of few longitudinal studies of simulations in organizations .,"This insight overlaps with several other studies. work on the attempts to replace physical (and costly) car crashes in safety design in the automotive industry with simulated crashes is one of few longitudinal studies of simulations in organizations. In his study, Leonardi (2012, p. 14) underscores the sociotechnical conditions for digital representations in the form of simulations becoming 'real' as ""there is no guarantee that people will use the new information [from simulation] or that they will believe it"". Simulation models are interesting and relevant to our analysis as they theoretically offer the possibility of decoupling (further) the tie between digital representation and its physical referent. However, a consistent theme in Leonardi (2012, p. 62) analysis is how the simulation models need ongoing validation with physical car crashes ""to verify the accuracy of their simulations by confirming them with physical tests [of car crashes]"". As Bailey et al. (2012 Bailey et al. ( , p. 1500, drawing on Leonardi's study, point out ""this tight coupling in simulation means that people who create representations are highly dependent on physical referents"". Similarly, in their study of Gehry's use of simulation models in architecture, Boland, Lyytinen, and Yoo (2007) underscore the close link maintained with a physical model of the building in question. Our mechanism of material tethering spells out how the link digital/physical is forged. For instance, during the bi-annual maintenance shutdown, the values for the wear on pipes and chokes predicted by the algorithm are compared directly with the physical measurements of the equipment during maintenance.",work on the attempts to replace physical (and costly) car crashes in safety design in the automotive industry with simulated crashes is one of few longitudinal studies of simulations in organizations.,belongs_to_article,1
Linux,work on the Linux community and open source .,"To help maintain coherence -to remain collected and to accept a powerful machination that might help -the members of the collective (human and nonhuman) needed to draw on a common set of understandings about how things are to be done, what is allowed, and what is not -what we call the Constitution. We draw this concept of a constitution directly from Weber's (2004 Weber's ( , p. 179, 2005) work on the Linux community and open source. Specifically, he proposes that the open source license (in the Linux case the General Public Licence or GPL) serves as a de facto constitution of the Linux collective: ""... the core statement of the social structure that defines the community of open source developers who participate in the project"" (Weber, 2004) . In a similar way, Demil and Lecocq (2006) propose the copy-left license as fundamental to understanding the governance of open source. They quote Bonaccorsi and Rossi (2003) : ""Licenses are the most important institution in the governance structure of open source projects"" (see also O'Mahony, 2003) . Weber (2005) suggests that the core constitutional value that the GPL license proposes is that developers will be treated fairly if they join the collective (community), suggesting that fairness is basic to the open source movement's social contract that offers a mix of freedom, non-discrimination and pragmatism. Similarly, Lanzara and Morner (2005) speak of the ""rules of reciprocity"" that the license inscribes. Indeed, being treated fairly can be understood as central to participation in open source in the sense that participants can believe that their work will not be expropriated and exploited by others (Benkler, 2004; Henkel, 2004) . In this paper, we go a little further and ask that the other, nonhuman, participants in the collective be treated fairly under the Constitution, including code, patches, and software actors. As we see below, if this core value appears to be violated, then some members of the collective, and their spokespersons, become very concerned.",work on the Linux community and open source.,background_information,3
open source,work on the Linux community and open source .,"To help maintain coherence -to remain collected and to accept a powerful machination that might help -the members of the collective (human and nonhuman) needed to draw on a common set of understandings about how things are to be done, what is allowed, and what is not -what we call the Constitution. We draw this concept of a constitution directly from Weber's (2004 Weber's ( , p. 179, 2005) work on the Linux community and open source. Specifically, he proposes that the open source license (in the Linux case the General Public Licence or GPL) serves as a de facto constitution of the Linux collective: ""... the core statement of the social structure that defines the community of open source developers who participate in the project"" (Weber, 2004) . In a similar way, Demil and Lecocq (2006) propose the copy-left license as fundamental to understanding the governance of open source. They quote Bonaccorsi and Rossi (2003) : ""Licenses are the most important institution in the governance structure of open source projects"" (see also O'Mahony, 2003) . Weber (2005) suggests that the core constitutional value that the GPL license proposes is that developers will be treated fairly if they join the collective (community), suggesting that fairness is basic to the open source movement's social contract that offers a mix of freedom, non-discrimination and pragmatism. Similarly, Lanzara and Morner (2005) speak of the ""rules of reciprocity"" that the license inscribes. Indeed, being treated fairly can be understood as central to participation in open source in the sense that participants can believe that their work will not be expropriated and exploited by others (Benkler, 2004; Henkel, 2004) . In this paper, we go a little further and ask that the other, nonhuman, participants in the collective be treated fairly under the Constitution, including code, patches, and software actors. As we see below, if this core value appears to be violated, then some members of the collective, and their spokespersons, become very concerned.",work on the Linux community and open source.,background_information,3
telework,work in the telework area .,"applied to which determine how the work or organization form functions. A person with duties requiring fre-Without a clear frame of reference unnecessary confusion and imprecision may arise around what is actually quent and close personal contact with colleagues should not work at home full-time. It is difficult to say anything meant, and occurrences and effects cannot be discussed in an appropriate manner. This has been clear in Sweden in general on which duties are suitable for the various forms of work or organization. The importance of the where a number of statistical investigations regarding telework has shown very differing results. The confusion duties for the chosen work form has been discussed by several authors; see, for instance, Huws et al (1990), or caused by this may be traced back to different investigations using different definitions of the same concepts. Lindström and Rapp (1996). Work duties are also linked to another dimension From a practical point of view a classification which is as precise as possible is of great importance for how which is significant for the effectiveness of the chosen form, viz. the need for learning. There are duties of such easy it is to identify and judge potential effects of a particular form of work or organizational form. a nature which require continuous learning, a type of learning where it is important that the individual spends From the discussion above, both with regard to the form of work and the workplace, it is evident that pre-time in an environment with others doing similar jobs. In this way each individual is given continuous opport-requisites for telework differ in many ways. There can be major differences between different 'telework situ-unities to learn not only from their own experience but also from that of others. ations'. Many factors come into play to determine whether a 'telework form' is successful or not and what As can be seen from this discussion, there are a number of interesting and important questions for future should be taken into consideration in various situations to make any possible improvements. work in the telework area. We have shown that there are many factors at various levels that must be considered, One example of an important question is the individual's freedom to choose when and/or where he or she is such as management, type of duties, social contact and learning. It is a difficult task for an organization to to work. The more freedom the individual has to choose decide on a certain telework form. It can be seen as a principle' proposed here, that it is able to adapt to such changes. large jigsaw puzzle where a set of different variables must be considered simultaneously.",work in the telework area.,background_information,3
user acceptance,"work acknowledges this potential limitation : "" While being very powerful in helping us predict acceptance , one of the limitations of Technology Acceptance Model is that it does not help understand and explain acceptance in ways that guide development beyond suggesting that system characteristics impact ease of use . . . . This places a damper on our ability to meaningfully design interventions to foster acceptance . In order to be able to explain user acceptance and use , it is important to understand the antecedents of the key Technology Acceptance Model constructs , perceived ease of use and usefulness "" ( Venkatesh and Davis 1996 , pp . 472 - 473 ) .","The parsimony of TAM combined with its predictive power make it easy to apply to different situations. However, while parsimony is TAM's strength, it is also the model's key limitation. TAM is predictive but its generality does not provide sufficient understanding from the standpoint of providing system designers with the information necessary to create user acceptance for new systems (Mathieson 1991) . Specifically, it is important to emphasize that although perceived ease of use has been employed extensively in user acceptance research in general and TAM research in particular, very little has been done to understand the determinants of perceived ease of use. Davis' more recent 1 The development and testing of TAM was based on studies conducted to examine potential acceptance of products of IBM, Canada. work acknowledges this potential limitation: ""While being very powerful in helping us predict acceptance, one of the limitations of TAM is that it does not help understand and explain acceptance in ways that guide development beyond suggesting that system characteristics impact ease of use. . . . This places a damper on our ability to meaningfully design interventions to foster acceptance. In order to be able to explain user acceptance and use, it is important to understand the antecedents of the key TAM constructs, perceived ease of use and usefulness"" (Venkatesh and Davis 1996, pp. 472-473). Understanding the determinants of perceived ease of use is further underscored by the two mechanisms by which it influences intention: (1) perceived ease of use has a direct effect on intention, and an indirect effect on intention via perceived usefulness, and (2) it is an initial hurdle that users have to overcome for acceptance, adoption, and usage of a system (see Davis et al. 1989 ).","work acknowledges this potential limitation: ""While being very powerful in helping us predict acceptance, one of the limitations of TAM is that it does not help understand and explain acceptance in ways that guide development beyond suggesting that system characteristics impact ease of use. . . . This places a damper on our ability to meaningfully design interventions to foster acceptance. In order to be able to explain user acceptance and use, it is important to understand the antecedents of the key TAM constructs, perceived ease of use and usefulness"" (Venkatesh and Davis 1996, pp. 472-473).",related_work,2
TAM,"work acknowledges this potential limitation : "" While being very powerful in helping us predict acceptance , one of the limitations of Technology Acceptance Model is that it does not help understand and explain acceptance in ways that guide development beyond suggesting that system characteristics impact ease of use . . . . This places a damper on our ability to meaningfully design interventions to foster acceptance . In order to be able to explain user acceptance and use , it is important to understand the antecedents of the key Technology Acceptance Model constructs , perceived ease of use and usefulness "" ( Venkatesh and Davis 1996 , pp . 472 - 473 ) .","The parsimony of TAM combined with its predictive power make it easy to apply to different situations. However, while parsimony is TAM's strength, it is also the model's key limitation. TAM is predictive but its generality does not provide sufficient understanding from the standpoint of providing system designers with the information necessary to create user acceptance for new systems (Mathieson 1991) . Specifically, it is important to emphasize that although perceived ease of use has been employed extensively in user acceptance research in general and TAM research in particular, very little has been done to understand the determinants of perceived ease of use. Davis' more recent 1 The development and testing of TAM was based on studies conducted to examine potential acceptance of products of IBM, Canada. work acknowledges this potential limitation: ""While being very powerful in helping us predict acceptance, one of the limitations of TAM is that it does not help understand and explain acceptance in ways that guide development beyond suggesting that system characteristics impact ease of use. . . . This places a damper on our ability to meaningfully design interventions to foster acceptance. In order to be able to explain user acceptance and use, it is important to understand the antecedents of the key TAM constructs, perceived ease of use and usefulness"" (Venkatesh and Davis 1996, pp. 472-473). Understanding the determinants of perceived ease of use is further underscored by the two mechanisms by which it influences intention: (1) perceived ease of use has a direct effect on intention, and an indirect effect on intention via perceived usefulness, and (2) it is an initial hurdle that users have to overcome for acceptance, adoption, and usage of a system (see Davis et al. 1989 ).","work acknowledges this potential limitation: ""While being very powerful in helping us predict acceptance, one of the limitations of TAM is that it does not help understand and explain acceptance in ways that guide development beyond suggesting that system characteristics impact ease of use. . . . This places a damper on our ability to meaningfully design interventions to foster acceptance. In order to be able to explain user acceptance and use, it is important to understand the antecedents of the key TAM constructs, perceived ease of use and usefulness"" (Venkatesh and Davis 1996, pp. 472-473).",related_work,2
meta-model,work DWM provides a meta - model that acts as a template to guide analysts in identifying the role of objects and relationships in the application domain .,"InConcert is an object-oriented client-server workflow management system. The InConcert object model consists of tasks, roles and references to data objects. It also provides an event and trigger model w x 43 . The Dynamic Workflow management frame-Ž . work DWM provides a meta-model that acts as a template to guide analysts in identifying the role of objects and relationships in the application domain. DWM defines the basic elements of a business process and provides an integrated view from four distinct perspectives: functional, informational, bew x havioral and organizational 27,28 .",work DWM provides a meta-model that acts as a template to guide analysts in identifying the role of objects and relationships in the application domain.,background_information,3
pretests,wording refined based on card sorting and pretests .,wording refined based on card sorting and pretests.,wording refined based on card sorting and pretests.,belongs_to_article,1
card sorting,wording refined based on card sorting and pretests .,wording refined based on card sorting and pretests.,wording refined based on card sorting and pretests.,belongs_to_article,1
framework,word2vec method is a self - supervised framework mapping every word in the vocabulary to a unique vector in the representation vector space so that similar words are located close to each other .,"The document representation method doc2vec derives directly from the word representation method word2vec [45, 46] . word2vec method is a self-supervised framework mapping every word in the vocabulary to a unique vector in the representation vector space so that similar words are located close to each other. This representation model can be trained by two ways, in which, each word either is predicted by the surrounding words (CBOW method) or used to predict other words in the same context (Skip-gram method). By using neural networks with new architectures, word2vec can preserve the linear regularities among words, and is able to capture syntactic and semantic word similarities better than other methods [45] . As a natural extension, doc2vec vectorization, introduced by Le and Mikolov [38] , represents a feedback document corpus according to a continuous distributed vector. Previous studies show that it is more robust and accurate in document classification than other baseline methods [38, 47, 48] . Similar to word2vec, there are two main architectures to train the doc2vec model: a distributed memory model of paragraph vectors (PV-DM) and the distributed bag of words version of the paragraph vector (PV-DBOW).",word2vec method is a self-supervised framework mapping every word in the vocabulary to a unique vector in the representation vector space so that similar words are located close to each other.,background_information,3
innovation,"word vectors without context and do not distinguish the nuanced meanings of the word "" managing . "" 21 global vectors for word representation 's innovation is to capture word co the hyphen occurrences in tweets or documents , as opposed to Word2vec 's focus on local contextual information for individual words .","word vectors without context and do not distinguish the nuanced meanings of the word ""managing."" 21 GloVe's innovation is to capture word co-occurrences in tweets or documents, as opposed to Word2vec's focus on local contextual information for individual words. Word vectors are high-dimensional and cannot be reduced to scalar metrics. The following is a sample tweet representing negative sentiment and a helpless attitude: @UNRefugeeAgency @Refugees @RedHourBen UNHCR and humanitarian organizations please ... We are Iraqi refugees in Turkey from 2014 until now and so far we have not got a homeland ... Put yourselves in our place. Our situation is bad and our children, their future is unknown.","word vectors without context and do not distinguish the nuanced meanings of the word ""managing."" 21 GloVe's innovation is to capture word co-occurrences in tweets or documents, as opposed to Word2vec's focus on local contextual information for individual words.",background_information,3
Word2vec,"word vectors without context and do not distinguish the nuanced meanings of the word "" managing . "" 21 global vectors for word representation 's innovation is to capture word co the hyphen occurrences in tweets or documents , as opposed to Word2vec 's focus on local contextual information for individual words .","word vectors without context and do not distinguish the nuanced meanings of the word ""managing."" 21 GloVe's innovation is to capture word co-occurrences in tweets or documents, as opposed to Word2vec's focus on local contextual information for individual words. Word vectors are high-dimensional and cannot be reduced to scalar metrics. The following is a sample tweet representing negative sentiment and a helpless attitude: @UNRefugeeAgency @Refugees @RedHourBen UNHCR and humanitarian organizations please ... We are Iraqi refugees in Turkey from 2014 until now and so far we have not got a homeland ... Put yourselves in our place. Our situation is bad and our children, their future is unknown.","word vectors without context and do not distinguish the nuanced meanings of the word ""managing."" 21 GloVe's innovation is to capture word co-occurrences in tweets or documents, as opposed to Word2vec's focus on local contextual information for individual words.",background_information,3
word of mouth,word of mouth context because the online environment permits more separation between an electronic word of mouth message and its source [ 59 ] .,"EWoM Message quality refers to the strength of the arguments contained within an eWoM message. It describes the extent to which a consumer perceives the content of an eWoM message as meaningful [19, 96, 124] , accurate [20, 34, 44, 46] , understandable [44] , rational [42, 96, 123] , timely [20, 29, 34, 44, 46] , relevant [20, 29, 44, 46] , and comprehensive [20, 29, 44, 96] . Prior HSM research has consistently identified quality of arguments within a message as the main antecedent of informational influence under conditions of systematic processing [15, 126] . Similarly, prior research on eWoM message adoption has shown the quality of arguments within an eWoM message as an important antecedent of a consumer's perceived helpfulness of an eWoM message [1, 86] . When elaborated carefully (i.e., systematic processing), higher argument quality reduces ambiguity surrounding the topic of discussion within an eWoM message for a consumer, which can help reduce decision-making uncertainty [126] . Therefore, higher eWoM message quality can significantly improve the helpfulness of an eWoM message for a consumer [20, 21] . Nonetheless, assessing the helpfulness of an eWoM message based on the quality of its arguments requires diligent scrutiny and systematic processing of the content of the eWoM message, which will be limited if a consumer is unwilling to or is cognitively unable to make sense of eWoM message arguments [13, 17, 107, 126] . Thus, eWoM message quality is a systematic cue that can determine a consumer's perceived helpfulness of an eWoM message. Hence, we postulate the following hypothesis. Message credibility refers to the extent to which a consumer perceives the information contained within a message as believable, trustworthy, and valid [19] . Prior studies (e.g., Wathen and Burkell [113] ) explain that ""a key early stage in the persuasion process is the receiver's judgment of the credibility of the information"" (p. 134). Although the emphasis in information credibility research has traditionally been on characteristics of the source, characteristics of the message have also gained distinct attention [78, 81] . Rosenthal [97] was among the early researchers who demonstrated the importance of message credibility in persuasion as a conceptually distinct factor than the credibility of the source. This conceptualization has since been adopted by many studies in the persuasion and eWoM literatures (e.g., Cheung et al. [19] , Erkans and Evans [40] , Fang [42] , Flanagin [78] [79] [80] [81] [82] , Walthan and Burkell [113] ). The importance of message credibility in determining its perceived helpfulness is more pronounced in an eWoM context as compared to a traditional (face-to-face) word of mouth context because the online environment permits more separation between an eWoM message and its source [59]. Face-to-face word of mouth is mainly transmitted between family members, friends, or close acquaintances. As such, consumers generally encounter less uncertainty about the credibility of a message because the source is well known. When the source is well known, consumers are able to extend source credibility (attributed to the familiarity with the source) to the credibility of the message provided by the source. However, in an online environment, information about the source is separated from an eWoM message because eWoM messages are mostly posted by strangers who are either anonymous or reveal little identification information [59, 68] . This places the burden on the consumer to assess the credibility of the message separately from the credibility of its source [59]. Therefore, eWoM message credibility is considered an important determinant of the helpfulness of an eWoM message [31, 40, 57] .",word of mouth context because the online environment permits more separation between an eWoM message and its source [59].,belongs_to_article,1
eWoM,word of mouth context because the online environment permits more separation between an electronic word of mouth message and its source [ 59 ] .,"EWoM Message quality refers to the strength of the arguments contained within an eWoM message. It describes the extent to which a consumer perceives the content of an eWoM message as meaningful [19, 96, 124] , accurate [20, 34, 44, 46] , understandable [44] , rational [42, 96, 123] , timely [20, 29, 34, 44, 46] , relevant [20, 29, 44, 46] , and comprehensive [20, 29, 44, 96] . Prior HSM research has consistently identified quality of arguments within a message as the main antecedent of informational influence under conditions of systematic processing [15, 126] . Similarly, prior research on eWoM message adoption has shown the quality of arguments within an eWoM message as an important antecedent of a consumer's perceived helpfulness of an eWoM message [1, 86] . When elaborated carefully (i.e., systematic processing), higher argument quality reduces ambiguity surrounding the topic of discussion within an eWoM message for a consumer, which can help reduce decision-making uncertainty [126] . Therefore, higher eWoM message quality can significantly improve the helpfulness of an eWoM message for a consumer [20, 21] . Nonetheless, assessing the helpfulness of an eWoM message based on the quality of its arguments requires diligent scrutiny and systematic processing of the content of the eWoM message, which will be limited if a consumer is unwilling to or is cognitively unable to make sense of eWoM message arguments [13, 17, 107, 126] . Thus, eWoM message quality is a systematic cue that can determine a consumer's perceived helpfulness of an eWoM message. Hence, we postulate the following hypothesis. Message credibility refers to the extent to which a consumer perceives the information contained within a message as believable, trustworthy, and valid [19] . Prior studies (e.g., Wathen and Burkell [113] ) explain that ""a key early stage in the persuasion process is the receiver's judgment of the credibility of the information"" (p. 134). Although the emphasis in information credibility research has traditionally been on characteristics of the source, characteristics of the message have also gained distinct attention [78, 81] . Rosenthal [97] was among the early researchers who demonstrated the importance of message credibility in persuasion as a conceptually distinct factor than the credibility of the source. This conceptualization has since been adopted by many studies in the persuasion and eWoM literatures (e.g., Cheung et al. [19] , Erkans and Evans [40] , Fang [42] , Flanagin [78] [79] [80] [81] [82] , Walthan and Burkell [113] ). The importance of message credibility in determining its perceived helpfulness is more pronounced in an eWoM context as compared to a traditional (face-to-face) word of mouth context because the online environment permits more separation between an eWoM message and its source [59]. Face-to-face word of mouth is mainly transmitted between family members, friends, or close acquaintances. As such, consumers generally encounter less uncertainty about the credibility of a message because the source is well known. When the source is well known, consumers are able to extend source credibility (attributed to the familiarity with the source) to the credibility of the message provided by the source. However, in an online environment, information about the source is separated from an eWoM message because eWoM messages are mostly posted by strangers who are either anonymous or reveal little identification information [59, 68] . This places the burden on the consumer to assess the credibility of the message separately from the credibility of its source [59]. Therefore, eWoM message credibility is considered an important determinant of the helpfulness of an eWoM message [31, 40, 57] .",word of mouth context because the online environment permits more separation between an eWoM message and its source [59].,belongs_to_article,1
econometric approach,wooldridge [ 39 ] provided an econometric approach to analyzing cluster sample .,"As shown in table 2, the standard deviations are not the same under different network structures. Potential problems arise with statistical inference in the presence of clustering effects. Default standard errors that ignore clustering can greatly understate true standard errors [8] . wooldridge [39] provided an econometric approach to analyzing cluster sample. following his approach, we compute the variance matrices that are robust to arbitrary cluster correlation and unknown heteroskedasticity. 3 In our context, the observations are clustered into different network topologies. Standard errors are adjusted for clusters in column 5 of table 3, and the result is similar. A practical limitation of inference with cluster-robust standard errors is the assumption that the number of clusters is large. Cameron et al. [8] show that cluster bootstraps can lead to considerable improvement in inference when there are few clusters. Column 6 shows that the results of the cluster bootstrap are robust. Because of the strong suspicion of heteroskedasticity, we also compute the heteroskedasticity-robust t-statistics using the Huber-white sandwich estimators in column 7 to check the robustness of our results. the robust t-statistics can deal with the concerns about the failure to meet standard regression assumptions, such as heteroskedasticity [38] . Our results are robust to the case when the modeling errors depend on the explanatory variables, such as degree and sdummy. Note that different network topologies can be linearly predicted from the variables degree and sdummy (dummy variables indicating network structures are redundant when we have the two explanatory variables, degree and sdummy, so adding additional dummy variables indicating network structures causes the problem of multicollinearity). thus, the results in column 7 are also robust to the case when the modeling errors depend on different network topologies.",wooldridge [39] provided an econometric approach to analyzing cluster sample.,related_work,2
cluster sample,wooldridge [ 39 ] provided an econometric approach to analyzing cluster sample .,"As shown in table 2, the standard deviations are not the same under different network structures. Potential problems arise with statistical inference in the presence of clustering effects. Default standard errors that ignore clustering can greatly understate true standard errors [8] . wooldridge [39] provided an econometric approach to analyzing cluster sample. following his approach, we compute the variance matrices that are robust to arbitrary cluster correlation and unknown heteroskedasticity. 3 In our context, the observations are clustered into different network topologies. Standard errors are adjusted for clusters in column 5 of table 3, and the result is similar. A practical limitation of inference with cluster-robust standard errors is the assumption that the number of clusters is large. Cameron et al. [8] show that cluster bootstraps can lead to considerable improvement in inference when there are few clusters. Column 6 shows that the results of the cluster bootstrap are robust. Because of the strong suspicion of heteroskedasticity, we also compute the heteroskedasticity-robust t-statistics using the Huber-white sandwich estimators in column 7 to check the robustness of our results. the robust t-statistics can deal with the concerns about the failure to meet standard regression assumptions, such as heteroskedasticity [38] . Our results are robust to the case when the modeling errors depend on the explanatory variables, such as degree and sdummy. Note that different network topologies can be linearly predicted from the variables degree and sdummy (dummy variables indicating network structures are redundant when we have the two explanatory variables, degree and sdummy, so adding additional dummy variables indicating network structures causes the problem of multicollinearity). thus, the results in column 7 are also robust to the case when the modeling errors depend on different network topologies.",wooldridge [39] provided an econometric approach to analyzing cluster sample.,related_work,2
system quality,wixom and todd [ 115 ] integrated satisfaction and the technology acceptance model ( technology acceptance model ) theory to show that information satisfaction - driven by information quality - can positively affect perceived usefulness and that system satisfaction - driven by system quality - can positively affect perceived ease of use .,"the Phenomenon of interest in this study is process satisfaction (PS) as a subset of general satisfaction. Marketing research has long noted the importance satisfaction plays in customer adoption and repurchase (e.g., [95, 96] ). research in e-commerce has shown that satisfaction is just as-if not more-critical for long-term growth and profitability in electronic markets [9, 69] . In information systems (IS) research, satisfaction and related affect have been shown to play a strong role in system adoption and, more importantly, in system continuance [9, 12, 58,84,85]. wixom and todd [115] integrated satisfaction and the technology acceptance model (taM) theory to show that information satisfaction-driven by information quality-can positively affect perceived usefulness and that system satisfaction-driven by system quality-can positively affect perceived ease of use. liao et al. [57] extended the theory of planned behavior (tPB) and found that satisfaction was the primary determinant of one's inten-communication interactivity influence communication fluency. the biggest limitation of kock's compensatory adaptation theory [53,54] is that it posits that lean media may result in paradoxical increases in team performance outcomes due to members compensating-often involuntarily-for obstacles posed by lean electronic media. while this is a useful start, such built-in limitations undermine the explanatory power of the theory.",wixom and todd [115] integrated satisfaction and the technology acceptance model (taM) theory to show that information satisfaction-driven by information quality-can positively affect perceived usefulness and that system satisfaction-driven by system quality-can positively affect perceived ease of use.,related_work,2
theory,wixom and todd [ 115 ] integrated satisfaction and the technology acceptance model ( technology acceptance model ) theory to show that information satisfaction - driven by information quality - can positively affect perceived usefulness and that system satisfaction - driven by system quality - can positively affect perceived ease of use .,"the Phenomenon of interest in this study is process satisfaction (PS) as a subset of general satisfaction. Marketing research has long noted the importance satisfaction plays in customer adoption and repurchase (e.g., [95, 96] ). research in e-commerce has shown that satisfaction is just as-if not more-critical for long-term growth and profitability in electronic markets [9, 69] . In information systems (IS) research, satisfaction and related affect have been shown to play a strong role in system adoption and, more importantly, in system continuance [9, 12, 58,84,85]. wixom and todd [115] integrated satisfaction and the technology acceptance model (taM) theory to show that information satisfaction-driven by information quality-can positively affect perceived usefulness and that system satisfaction-driven by system quality-can positively affect perceived ease of use. liao et al. [57] extended the theory of planned behavior (tPB) and found that satisfaction was the primary determinant of one's inten-communication interactivity influence communication fluency. the biggest limitation of kock's compensatory adaptation theory [53,54] is that it posits that lean media may result in paradoxical increases in team performance outcomes due to members compensating-often involuntarily-for obstacles posed by lean electronic media. while this is a useful start, such built-in limitations undermine the explanatory power of the theory.",wixom and todd [115] integrated satisfaction and the technology acceptance model (taM) theory to show that information satisfaction-driven by information quality-can positively affect perceived usefulness and that system satisfaction-driven by system quality-can positively affect perceived ease of use.,related_work,2
information quality,wixom and todd [ 115 ] integrated satisfaction and the technology acceptance model ( technology acceptance model ) theory to show that information satisfaction - driven by information quality - can positively affect perceived usefulness and that system satisfaction - driven by system quality - can positively affect perceived ease of use .,"the Phenomenon of interest in this study is process satisfaction (PS) as a subset of general satisfaction. Marketing research has long noted the importance satisfaction plays in customer adoption and repurchase (e.g., [95, 96] ). research in e-commerce has shown that satisfaction is just as-if not more-critical for long-term growth and profitability in electronic markets [9, 69] . In information systems (IS) research, satisfaction and related affect have been shown to play a strong role in system adoption and, more importantly, in system continuance [9, 12, 58,84,85]. wixom and todd [115] integrated satisfaction and the technology acceptance model (taM) theory to show that information satisfaction-driven by information quality-can positively affect perceived usefulness and that system satisfaction-driven by system quality-can positively affect perceived ease of use. liao et al. [57] extended the theory of planned behavior (tPB) and found that satisfaction was the primary determinant of one's inten-communication interactivity influence communication fluency. the biggest limitation of kock's compensatory adaptation theory [53,54] is that it posits that lean media may result in paradoxical increases in team performance outcomes due to members compensating-often involuntarily-for obstacles posed by lean electronic media. while this is a useful start, such built-in limitations undermine the explanatory power of the theory.",wixom and todd [115] integrated satisfaction and the technology acceptance model (taM) theory to show that information satisfaction-driven by information quality-can positively affect perceived usefulness and that system satisfaction-driven by system quality-can positively affect perceived ease of use.,related_work,2
information technology,without investigating the eflects of virtual product experience from an information technology ( information technology ) perspective .,"Exploratory research in marketing 116. 44,45. 46] has already suggested that VPE has the potential to improve consumer product knowledge and attitudes toward brands, while enhancing consumer purchase intentions. However, most previous studies have treated VPE as a black box. without investigating the eflects of VPE from an information technology (IT) perspective. ",without investigating the eflects of VPE from an information technology (IT) perspective.,related_work,2
firewall,"without disrupting normal operations of governments , universities and corporations ; and the increasing availability of enterpriseready , behind - the - firewall , private virtual worlds .","Industry commentators such as McKinsey (Richards 2008) and Gartner (2009) have classified virtual worlds as transformational technologies that will become mainstream within the next five years. These expectations may seem overly confident in the light of earlier predictions that by 2011, 80% of active Internet users (and Fortune 500 enterprises) would be engaged in some form of virtual world activity (Gartner 2007) . However, numerous signals indicate that virtual worlds are likely to become more relevant and productive in the near future, including: globalizing trends that require virtual work and distributed collaboration; green initiatives that seek to cut the carbon emissions generated by travel, including commutes to offices; cost-cutting measures in economically challenging times, motivating organizations to reduce real estate and the need for physical colocation; efforts to prevent the spread of communicable diseases (e.g., H1N1) without disrupting normal operations of governments, universities and corporations; and the increasing availability of enterpriseready, behind-the-firewall, private virtual worlds. ","without disrupting normal operations of governments, universities and corporations; and the increasing availability of enterpriseready, behind-the-firewall, private virtual worlds.",background_information,3
database,"without a database law , other means that database creators can use to protect their databases seem to be ineffective in most cases .","without a database law, other means that database creators can use to protect their databases seem to be ineffective in most cases. For example, a creator can use certain nonprice predation strategies-namely, by raising rivals' costs [50] -to deter entry or at least to soften competition from the reuser. In the past, creators attempted cost-raising strategies such as blocking the Internet protocol (IP) addresses used by reuser computers and frequently changing output format to make data extraction more difficult. this can be modeled by letting the creator choose a technology I, with which the marginal cost of the reuser becomes C 1 (I). the cost of installing such antiextraction technologies is often small enough to be negligible. using techniques similar to those in the proof of lemma 1, we can solve for firm profit maximization. when I is such that 0 ≤ C 1 (I) ≤ min{(3/2)(v -t), 3t, 2v -3t}, the creator profit becomes π 0 d = (t + C 1 (I)/3) 2 /2t > π d = t/2, that is, the creator profit is higher than when the technology is not used. the reuser profit is π 1 d = (t + 2C 1 (I)/3)(t -C 1 (I)/3)/2t, which could be greater or less than π d , depending on the level of C 1 (I). Obviously, if C 1 (I) is very high, the reuser will be deterred. However, antiextraction techniques have not been very effective in practice; 18 we suspect that C 1 (I) has been too small to have a substantial effect. therefore, we will assume no antiextraction is in place in the rest of the analysis. regardless of the effectiveness of antiextraction techniques, they are a socially wasteful investment because they merely help transfer consumer surplus and reuser profit to the creator. when database creators are also reusers, the cost-raising problem may not arise at all. 19 there can be a need for a database law from the reuser's point of view. Database reusers often face legal challenges from database creators. For example, reusers often receive legal threat and sometimes are sued by the creators. the uncertainty of various proposed database bills exacerbates the legal risks for the reusers, which are often small but innovative firms. as a result, some reusers have to exit the market, and certain value-added data reuses cannot occur. In this case, having a database law that clearly specifies the kinds of legal reuses will help to create and sustain a market of socially beneficial reuser databases. conditions and choices of Data reuse Policy a socially beneficial data reuse policy can correct market failure by restricting certain free riding in data reuse; the legal certainties it provides also help eliminate or reduce wasteful cost-raising investment by incumbent database creators. this can be done either by requiring the reuser to pay the creator for the data or by disallowing data reuse altogether. the creator can ask the reuser to license the data by paying a fee, r, which can be up to the reuser's profit π d ; asking a fee r > π d is equivalent to disallowing reuse because the reuser would make a negative net profit. like any other transaction, data reuse licenses inevitably incur a transaction cost (e.g., negotiating the fee schedule r, monitoring and enforcing the license, and the other activities of administrating data reuse policy often incur certain costs). this can be modeled with a transaction efficiency coefficient α; when the creator asks for r, it actually gets αr, and the transaction cost is (1 -α)r, where α ∈ [0, 1]. to simplify the analysis, we let r = π d ; that is, we assume that the creator has the negotiation skills or legal power to ask the reuser to disgorge all profits from reusing the data. 21 thus, in the duopoly case with a fee-paying reuser enforced by a data reuse policy, (π d + π d ) is the best the creator can get (assuming α = 1) to offset its fixed cost F if the creator ever allows someone to reuse its data. Before we develop the formal analysis, we describe the intuition by plotting this upper-bound condition along with the profit curves in Figure 2. ","without a database law, other means that database creators can use to protect their databases seem to be ineffective in most cases.",belongs_to_article,1
IT project management,"within the information technology project management literature , studies have attempted to asses measures similar to congruence using broader terms , such as alignment or social alignment , to capture the degree of consensus between the involved parties with respect to social aspects of projects ( e.g. , START_CITE [ 53 ] END_CITE CITE_b49 ) .","The congruence framework presented in this study offers a nuanced approach for examining fit between various stakeholders involved in ISD projects. within the IT project management literature, studies have attempted to asses measures similar to congruence using broader terms, such as alignment or social alignment, to capture the degree of consensus between the involved parties with respect to social aspects of projects (e.g., [53] ). However, most of these studies have found mixed or weak support. One of the major problems was associated with the approach used to evaluate alignment. This study used polynomial regression and response surface tests to examine an important aspect of alignment. The results for both the models displayed good fit and provided a clear understanding of how each component affected the dependent variable. using such robust approaches can provide a detailed understanding of relationships between combinations of two predictor variables and an outcome variable by graphing the results onto a three-dimensional space. ","within the IT project management literature, studies have attempted to asses measures similar to congruence using broader terms, such as alignment or social alignment, to capture the degree of consensus between the involved parties with respect to social aspects of projects (e.g., [53] ).",related_work,2
technostress,"within the information systems literature , research indicates that employee stress - related to the use information technology ( information technology ) ( literature, research indicates that employee stress-related to the use information technology (IT) , technostress ) influences a number of information technology and non - information technology - related cognitions and behaviors [ 56,67 ] .","we use coping theory to explore an underlying relationship between employee stress caused by burdensome, complex, and ambiguous information security requirements (termed ""security-related stress"" or SrS) and deliberate information security policy (ISP) violations. results from a survey of 539 employee users suggest that SrS engenders an emotion-focused coping response in the form of moral disengagement from ISP violations, which in turn increases one's susceptibility to this behavior. Our multidimensional view of SrS-comprised of security-related overload, complexity, and uncertainty-offers a new perspective on the workplace environment factors that foster noncompliant user behavior and inspire cognitive rationalizations of such behavior. The study extends technostress research to the information systems security domain and provides a theoretical framework for the influence of SrS on user behavior. For practitioners, the results highlight the incidence of SrS in organizations and suggest potential mechanisms to counter the stressful effects of information security requirements. D'Arcy, hErATh, AND ShOSSAcADeMics AnD prAcTiTioners AliKe recognize eMployees as a major threat to organizational information security efforts [14, 69]. To address this ""insider"" threat, organizations have devoted significant resources into behavioral security measures, such as policy development and education and training, in addition to continually updating their security technologies [54]. U.S. federal and state governments and certain industries have also introduced regulations and standards that mandate organizations' internal security measures [14]. Despite these initiatives, a class of employee security-related behaviors known as volitional (but not malicious) information security policy (ISP) violations [27, 71] (e.g., password sharing, failing to log off when leaving workstation) continue to plague organizations. At least some explanation for this predicament is that employees face a surfeit of rapidly expanding security requirements (i.e., policies, procedures, and technical controls), which they find to be constraining, inconvenient, and difficult to understand [51,53, 69]. Evidence of this comes from a recent survey of over 2,800 employees [16] in which ""too busy to think about policies"" and ""policies are inconvenient to follow"" were reported as chief reasons for ISP violations. Some authors have suggested that security requirements can backfire and bring about security-diminishing behavior due to the demands (e.g., time, effort, frustration) they impose on employees [51,60,64]. Although there is preliminary evidence to support this notion [51], the information systems (IS) literature lacks a systematic, theorydriven investigation of the potential adverse effects of organizational information security requirements (hereafter security requirements) on user behavior. A goal of this paper is to address this gap.Against this backdrop, we offer a new avenue for understanding employees' ISP violations-namely, workplace stress due to security requirements and its coping response. The topic of stress has a long history in the organizational and psychology literatures and empirical results have shown that negative work stressors 1 predict a variety of undesirable employee behaviors (e.g., [25,57]). within the IS literature, research indicates that employee stress-related to the use information technology (IT) (i.e., technostress) influences a number of IT and non-IT-related cognitions and behaviors [56,67]. In the present study, we extend the technostress concept to the domain of IS security and explain three conditions-overload, complexity, and uncertainty-in which security requirements can create stress in employees. we theorize that this form of employee stress, termed security-related stress (SRS), is a contributor to ISP violations.Using coping theory as a foundation [38], we develop and empirically test a model of ISP violation intention which predicts that employees engage in emotion-focused coping in response to SrS. we explicate this emotion-focused coping in the form of ","within the IS literature, research indicates that employee stress-related to the use information technology (IT) (i.e., technostress) influences a number of IT and non-IT-related cognitions and behaviors [56,67].",related_work,2
use information technology,"within the information systems literature , research indicates that employee stress - related to the use information technology ( information technology ) ( literature, research indicates that employee stress-related to the use information technology (IT) , technostress ) influences a number of information technology and non - information technology - related cognitions and behaviors [ 56,67 ] .","we use coping theory to explore an underlying relationship between employee stress caused by burdensome, complex, and ambiguous information security requirements (termed ""security-related stress"" or SrS) and deliberate information security policy (ISP) violations. results from a survey of 539 employee users suggest that SrS engenders an emotion-focused coping response in the form of moral disengagement from ISP violations, which in turn increases one's susceptibility to this behavior. Our multidimensional view of SrS-comprised of security-related overload, complexity, and uncertainty-offers a new perspective on the workplace environment factors that foster noncompliant user behavior and inspire cognitive rationalizations of such behavior. The study extends technostress research to the information systems security domain and provides a theoretical framework for the influence of SrS on user behavior. For practitioners, the results highlight the incidence of SrS in organizations and suggest potential mechanisms to counter the stressful effects of information security requirements. D'Arcy, hErATh, AND ShOSSAcADeMics AnD prAcTiTioners AliKe recognize eMployees as a major threat to organizational information security efforts [14, 69]. To address this ""insider"" threat, organizations have devoted significant resources into behavioral security measures, such as policy development and education and training, in addition to continually updating their security technologies [54]. U.S. federal and state governments and certain industries have also introduced regulations and standards that mandate organizations' internal security measures [14]. Despite these initiatives, a class of employee security-related behaviors known as volitional (but not malicious) information security policy (ISP) violations [27, 71] (e.g., password sharing, failing to log off when leaving workstation) continue to plague organizations. At least some explanation for this predicament is that employees face a surfeit of rapidly expanding security requirements (i.e., policies, procedures, and technical controls), which they find to be constraining, inconvenient, and difficult to understand [51,53, 69]. Evidence of this comes from a recent survey of over 2,800 employees [16] in which ""too busy to think about policies"" and ""policies are inconvenient to follow"" were reported as chief reasons for ISP violations. Some authors have suggested that security requirements can backfire and bring about security-diminishing behavior due to the demands (e.g., time, effort, frustration) they impose on employees [51,60,64]. Although there is preliminary evidence to support this notion [51], the information systems (IS) literature lacks a systematic, theorydriven investigation of the potential adverse effects of organizational information security requirements (hereafter security requirements) on user behavior. A goal of this paper is to address this gap.Against this backdrop, we offer a new avenue for understanding employees' ISP violations-namely, workplace stress due to security requirements and its coping response. The topic of stress has a long history in the organizational and psychology literatures and empirical results have shown that negative work stressors 1 predict a variety of undesirable employee behaviors (e.g., [25,57]). within the IS literature, research indicates that employee stress-related to the use information technology (IT) (i.e., technostress) influences a number of IT and non-IT-related cognitions and behaviors [56,67]. In the present study, we extend the technostress concept to the domain of IS security and explain three conditions-overload, complexity, and uncertainty-in which security requirements can create stress in employees. we theorize that this form of employee stress, termed security-related stress (SRS), is a contributor to ISP violations.Using coping theory as a foundation [38], we develop and empirically test a model of ISP violation intention which predicts that employees engage in emotion-focused coping in response to SrS. we explicate this emotion-focused coping in the form of ","within the IS literature, research indicates that employee stress-related to the use information technology (IT) (i.e., technostress) influences a number of IT and non-IT-related cognitions and behaviors [56,67].",related_work,2
emails,with w ik equal to the weight of term information because in email k and r equal to the number of emails belonging to the same observation .,"with w ik equal to the weight of term i in email k and r equal to the number of emails belonging to the same observation. Using each distinct term as a feature in the churnmodeling phase would lead to an unmanageable number of explanatory variables. Moreover, due to the high dimensionality of the feature space, most weights are zero for a single email. Thus, using a large and sparse term-by-email matrix would be counterproductive in the predictivemodeling context. ",with w ik equal to the weight of term i in email k and r equal to the number of emails belonging to the same observation.,belongs_to_article,1
email,with w ik equal to the weight of term information because in email k and r equal to the number of emails belonging to the same observation .,"with w ik equal to the weight of term i in email k and r equal to the number of emails belonging to the same observation. Using each distinct term as a feature in the churnmodeling phase would lead to an unmanageable number of explanatory variables. Moreover, due to the high dimensionality of the feature space, most weights are zero for a single email. Thus, using a large and sparse term-by-email matrix would be counterproductive in the predictivemodeling context. ",with w ik equal to the weight of term i in email k and r equal to the number of emails belonging to the same observation.,background_information,3
user involvement,"with users included measures of user involvement ( number of users , amount of user time involved , quality of user involvement , and problems with user involvement ) , measures of the number and types of user contacts , and measures of the amount of user education being provided and used .","with users included measures of user involvement (number of users, amount of user time involved, quality of user involvement, and problems with user involvement), measures of the number and types of user contacts, and measures of the amount of user education being provided and used. Also, measures of user attitudes were suggested -complaints and compliments, results of user attitude surveys, changes in budgetary commitments, and number of ""new customers."" Several viewed trends in purchases of outside DP services and acquisition of minicomputers by users as important measures of relationships with users. ","with users included measures of user involvement (number of users, amount of user time involved, quality of user involvement, and problems with user involvement), measures of the number and types of user contacts, and measures of the amount of user education being provided and used.",related_work,2
surveys,with three surveys among 64 knowledge workers ( and their supervisors ) from the research and advisory organization .,"with three surveys among 64 knowledge workers (and their supervisors) from the research and advisory organization. The results of this study show that temporal and spatial separation both influence knowledge awareness through distinct causal pathways, and that communication media may serve to bridge spatial and exacerbate temporal divides. Finally, we conclude with an in-depth discussion regarding the explanations and implications of these findings. ",with three surveys among 64 knowledge workers (and their supervisors) from the research and advisory organization.,background_information,3
databases,"with this balance , value creation through data reuse is maximally allowed to the extent that the creators still have enough incentives to create the databases .","we believe the solution to these challenges hinges upon finding a reasonable balance between protection of incentives and promotion of value creation through data reuse. with this balance, value creation through data reuse is maximally allowed to the extent that the creators still have enough incentives to create the databases. consensus can develop for international harmonization if we can determine the policy choices that maximize social welfare; 14 a database policy so formulated should survive the scrutiny of constitutionality and other inefficiencies can be avoided or mitigated. ","with this balance, value creation through data reuse is maximally allowed to the extent that the creators still have enough incentives to create the databases.",background_information,3
cognitive absorption,with the user 's cognitive absorption in the virtual world .,with the user's cognitive absorption in the virtual world.,with the user's cognitive absorption in the virtual world.,background_information,3
virtual world,with the user 's cognitive absorption in the virtual world .,with the user's cognitive absorption in the virtual world.,with the user's cognitive absorption in the virtual world.,background_information,3
security issues,"with the theory and empirical results in hand , we provide some answers to the question of whether the theoretical perspective oversimplifies security issues .","Our study makes practical and theoretical contributions to the literature on information security in several ways. First, our study provides a deep practical understanding of the security context, driven by both data protection and compliance. Our findings identify how security resources influence data protection and compliance based on an organization's security operational maturity. Thus, our analysis provides policy insights on effective security programs in complex environments. Second, this study makes a theoretical contribution by providing evidence that security resources do not always have positive effects on actual security and compliance in the health-care context, where customers or patients are more elastic to reputation than price. Since both data breaches and noncompliance can tarnish a hospital's reputation, health-care security decisions are more influenced by political or regulatory perspectives than economic perspectives. we empirically examine whether the health-care security context is in line with the predictions of the resource-based view. with the theory and empirical results in hand, we provide some answers to the question of whether the theoretical perspective oversimplifies security issues. ","with the theory and empirical results in hand, we provide some answers to the question of whether the theoretical perspective oversimplifies security issues.",related_work,2
advertising,with the rapid growth of consumer access to broadband and the shift of mass TV advertising to targeted streaming - video ads .,"with the rapid growth of consumer access to broadband and the shift of mass TV advertising to targeted streaming-video ads. arrives at the website following a Poisson distribution with rate ij (per unit of time). Let the period length be t j (number of days), then N ij follows a Poisson distribution with rate ij t j . Because ij > 0, we further posit that ln ij is a linear function of some observable Stage 1 drivers X ij in the period (the conventional log-linear specification). Therefore, Equation (3) in the ORD model is replaced by Pr N ij = n = ij t j n e − ij t j n! and ",with the rapid growth of consumer access to broadband and the shift of mass TV advertising to targeted streaming-video ads.,background_information,3
streaming-video,with the rapid growth of consumer access to broadband and the shift of mass TV advertising to targeted streaming - video ads .,"with the rapid growth of consumer access to broadband and the shift of mass TV advertising to targeted streaming-video ads. arrives at the website following a Poisson distribution with rate ij (per unit of time). Let the period length be t j (number of days), then N ij follows a Poisson distribution with rate ij t j . Because ij > 0, we further posit that ln ij is a linear function of some observable Stage 1 drivers X ij in the period (the conventional log-linear specification). Therefore, Equation (3) in the ORD model is replaced by Pr N ij = n = ij t j n e − ij t j n! and ",with the rapid growth of consumer access to broadband and the shift of mass TV advertising to targeted streaming-video ads.,background_information,3
clinical records,with the issue of paper clinical records and the difficulty of tracing their physical location in the unit .,"with the issue of paper clinical records and the difficulty of tracing their physical location in the unit. They proposed attaching a microchip that emitted a sound to each document whenever it was searched for. On the other hand, the physicists, who were in charge of managing the unit's technological infrastructures, did not want to discuss the issues with other groups, but wanted to be in charge of the new integrated MIS. Nurses and technicians were mostly afraid of the training that a new MIS would require. Through the discussion, however, it became clear that a new MIS, whose characteristics are tailored to each causal condition of the grounded model, would have helped the unit as a whole and each professional group in particular, if properly and collectively designed. ",with the issue of paper clinical records and the difficulty of tracing their physical location in the unit.,related_work,2
H5,"with the corresponding path coefficient for Electronic data interchange nonusers . The significance of difference was examined by /-test ( Venkatesh and Morris 2000 ) . It tums out that the only path that differs significantly between the two subsamples is from adoption costs to open - standard interorganizational systems adoption ( direct impact ^ 2.29 , p < 0.05 ) . This result provides further support for H5 . in summary , by fitting the structural model on the full sample and subsamples , we have found support for all of the five hypotheses . ' "" *","with the corresponding path coefficient for EDI nonusers. The significance of difference was examined by /-test (Venkatesh and Morris 2000). It tums out that the only path that differs significantly between the two subsamples is from adoption costs to open-standard IOS adoption (/ ^ 2.29, p < 0.05). This result provides further support for H5. in summary, by fitting the structural model on the full sample and subsamples, we have found support for all of the five hypotheses.'""* ","with the corresponding path coefficient for EDI nonusers. The significance of difference was examined by /-test (Venkatesh and Morris 2000). It tums out that the only path that differs significantly between the two subsamples is from adoption costs to open-standard IOS adoption (/ ^ 2.29, p < 0.05). This result provides further support for H5. in summary, by fitting the structural model on the full sample and subsamples, we have found support for all of the five hypotheses.'""*",belongs_to_article,1
p < 0.05,"with the corresponding path coefficient for Electronic data interchange nonusers . The significance of difference was examined by /-test ( Venkatesh and Morris 2000 ) . It tums out that the only path that differs significantly between the two subsamples is from adoption costs to open - standard interorganizational systems adoption ( direct impact ^ 2.29 , p < 0.05 ) . This result provides further support for H5 . in summary , by fitting the structural model on the full sample and subsamples , we have found support for all of the five hypotheses . ' "" *","with the corresponding path coefficient for EDI nonusers. The significance of difference was examined by /-test (Venkatesh and Morris 2000). It tums out that the only path that differs significantly between the two subsamples is from adoption costs to open-standard IOS adoption (/ ^ 2.29, p < 0.05). This result provides further support for H5. in summary, by fitting the structural model on the full sample and subsamples, we have found support for all of the five hypotheses.'""* ","with the corresponding path coefficient for EDI nonusers. The significance of difference was examined by /-test (Venkatesh and Morris 2000). It tums out that the only path that differs significantly between the two subsamples is from adoption costs to open-standard IOS adoption (/ ^ 2.29, p < 0.05). This result provides further support for H5. in summary, by fitting the structural model on the full sample and subsamples, we have found support for all of the five hypotheses.'""*",belongs_to_article,1
databases,"with the appropriate policy in place , both the creators and the reusers should focus on innovation that can increase the variety of databases and create value from database contents .","Abstract. : the availability of data on the web and new data extraction technologies have made it increasingly easy to reuse existing data to create new databases and provide value-added services. Meanwhile, database creators have been seeking legal protection for their data, such as the European union's Database Directive. the legislative development shows that there is significant difficulty in finding the right balance between protecting the incentives of creating publicly accessible databases (including semistructured web sites) and preserving adequate access to factual data for valuecreating activities. we address this issue using an extended spatial competition model that explicitly considers licensing provisions and inefficiencies in policy administration. the results show that, depending on the cost level of database creation, the degree of differentiation of the reuser database, and the efficiency of policy administration, there are different socially beneficial policy choices, such as protecting a legal monopoly, encouraging competition via compulsory licensing, discouraging voluntary licensing, or even allowing free riding. with the appropriate policy in place, both the creators and the reusers should focus on innovation that can increase the variety of databases and create value from database contents. key wordS and pHraSeS: database protection, data reuse, intellectual property, noncopyrightable data. ","with the appropriate policy in place, both the creators and the reusers should focus on innovation that can increase the variety of databases and create value from database contents.",background_information,3
database,"with the appropriate policy in place , both the creators and the reusers should focus on innovation that can increase the variety of databases and create value from database contents .","Abstract. : the availability of data on the web and new data extraction technologies have made it increasingly easy to reuse existing data to create new databases and provide value-added services. Meanwhile, database creators have been seeking legal protection for their data, such as the European union's Database Directive. the legislative development shows that there is significant difficulty in finding the right balance between protecting the incentives of creating publicly accessible databases (including semistructured web sites) and preserving adequate access to factual data for valuecreating activities. we address this issue using an extended spatial competition model that explicitly considers licensing provisions and inefficiencies in policy administration. the results show that, depending on the cost level of database creation, the degree of differentiation of the reuser database, and the efficiency of policy administration, there are different socially beneficial policy choices, such as protecting a legal monopoly, encouraging competition via compulsory licensing, discouraging voluntary licensing, or even allowing free riding. with the appropriate policy in place, both the creators and the reusers should focus on innovation that can increase the variety of databases and create value from database contents. key wordS and pHraSeS: database protection, data reuse, intellectual property, noncopyrightable data. ","with the appropriate policy in place, both the creators and the reusers should focus on innovation that can increase the variety of databases and create value from database contents.",background_information,3
information technologies,"with the advance of information technologies and the rise of social media , information exchange is ubiquitous these days .",". Several empirical studies have demonstrated the power of prediction markets in areas such as political science [5] , supply chain management [6, 22], marketing [11, 16, 34] , and finance [4] . In most of the previous literature, researchers have assumed that participants in prediction markets are isolated: they receive bits and pieces of independent information and cannot affect the decisions of other participants. However, in reality, people often mobilize their social networks to collect information and opinions on a variety of issues. CNBC reported an effective information exchange network through which tweeting with fellow farmers has become a way for participants in a far-flung and isolated business to compare notes on everything from weather conditions to new fertilizers. 1 these tweets are dramatically accelerating the flow of information that may give investors an edge in the commodities market. with the advance of information technologies and the rise of social media, information exchange is ubiquitous these days. Indeed, people can use their smartphones or computers to share information with their social network neighbors at almost any place, at any time. the ubiquity of information exchange on social networks and the lack of understanding about their effects on prediction markets motivate us to explore the following research question: ","with the advance of information technologies and the rise of social media, information exchange is ubiquitous these days.",background_information,3
United States,"with somE EstimatEs of only approximatEly 60 pErcEnt of the market for online search in the United States , Google would not appear to have a monopoly in search .","with somE EstimatEs of only approximatEly 60 pErcEnt of the market for online search in the United States, Google would not appear to have a monopoly in search. however, Google is actually quite close to the threshold for considering a company to have monopoly market share, regardless of the establishment of harm, particularly if Google's estimation of its market share in search engine advertising of 72 percent is accepted. The threshold is usually 70 percent, with numbers in the range of 40 percent to 70 percent being deemed worthy of attention. as Google's share continues to grow, it may surpass the 70 percent threshold. In HDC Medical, Inc., v. Minntech Corp., the court held that if a defendant has so large a market share as to constitute a predominant share, a rebuttable presumption of monopolization applies [48, p. 1103]. likewise, if Google search is no more than another form of advertising, then companies can advertise in the New York Times, in Fortune, on television, and, of course, with Yahoo! and Bing. ","with somE EstimatEs of only approximatEly 60 pErcEnt of the market for online search in the United States, Google would not appear to have a monopoly in search.",background_information,3
Bayesian inference,"with respect to the variational parameters θ v { λ , r } , we obtain the collapsed variational Bayesian inference algorithm for the Bayesian model with mixture of Bernoulli model .","with respect to the variational parameters θ v {λ, r}, we obtain the collapsed variational Bayesian inference algorithm for the BMB model. The entire procedure is summarized in Algorithm 1, where a dot indicates that the corresponding index is being summed over, Yin, Luo, and Brown: Learning from Crowdsourced Multi-labeling for example, u l 1· u l 1,1 + u l 1,0 , and we use the superscript ¬i and ¬(i, j) to indicate the corresponding variables being excluded from the counts, for example, n ¬i k,j,1 i ≠i I[x i k, z i ,j 1]. A more detailed derivation of the algorithm, including an application of the Gaussian approximation, is given in Online Appendix D. In practice, we run the inference algorithm with three random initializations of the variational parameters and retain the results with the highest value of ELBO. For each random initialization, we repeat the update rules (18) and (19) until the relative improvement in the ELBO falls below a small threshold (e.g., 0.0001) or the algorithm reaches the maximum number of iterations (e.g., 200 iterations). ","with respect to the variational parameters θ v {λ, r}, we obtain the collapsed variational Bayesian inference algorithm for the BMB model.",background_information,3
Facebook,"with over 955 million user by June 2012 START_CITE ( -Facebook and Friendster. One , 2012 ) END_CITE CITE_b35 .","Mark Zuckerberg founded Facebook at Harvard University in 2004 with his college roommates and fellow computer science students (Facebook, 2009 ). The website's membership was initially limited to Harvard students as a version of Hot or Not (Tabak, 2004) , but was soon expanded to other colleges, then high school students, and, finally in 2006, to anyone over the age over 13. Facebook demonstrated viral growth patterns and was ranked in 2009 as the most used social network worldwide by monthly active users (Compete, 2009) with over 955 million user by June 2012 (Facebook, 2012) . In 2008, the fastest growing demographic was 25+-year olds (Eldon, 2008) , but by 2009 it was an age cohort of 35-54 year olds (Corbett, 2009 ) and by 2012 it was 45-54 year-olds. 4 Facebook's retention has been strong with 23% of users checking their accounts more than 5 times per day. 5 In addition, mechanisms to invite friends automatically offered by Facebook have led to significant fan out rates: the average user in Facebook currently has over 130 friends. Facebook has been fervent about its growth and has operated a growth team which receives specific attention from Zuckerberg and his top management team (Johns, 2012) . ","with over 955 million user by June 2012 (Facebook, 2012) .",background_information,3
Airlines,"with other international airlines such as Qantas , Lufthansa , Scandinavian Airlines , British Airways , etc . for the purpose of exchanging frequent flyer points .","with other international airlines such as Qantas, Lufthansa, Scandinavian Airlines, British Airways, etc. for the purpose of exchanging frequent flyer points. ","with other international airlines such as Qantas, Lufthansa, Scandinavian Airlines, British Airways, etc. for the purpose of exchanging frequent flyer points.",belongs_to_article,1
British,"with other international airlines such as Qantas , Lufthansa , Scandinavian Airlines , British Airways , etc . for the purpose of exchanging frequent flyer points .","with other international airlines such as Qantas, Lufthansa, Scandinavian Airlines, British Airways, etc. for the purpose of exchanging frequent flyer points. ","with other international airlines such as Qantas, Lufthansa, Scandinavian Airlines, British Airways, etc. for the purpose of exchanging frequent flyer points.",background_information,3
online retailers,"with online retailers seeking to establish online communities and followings , and with manufactures and marketers increasingly desirous to influence the online reputation of their products , the implications of these findings will likely grow .","with online retailers seeking to establish online communities and followings, and with manufactures and marketers increasingly desirous to influence the online reputation of their products, the implications of these findings will likely grow. we recognize that unscrupulous marketers may try to leverage this work and generate more credible product reviews. however, we feel that the dramatic effect of simple word choices in anonymous online product reviews must be brought to light so that potential customers might understand how they make credibility attributions when reading anonymous product reviews. As we stated above, it is unknown how suitable expectancy violations are as an indication of credibility. however, it is likely that potential buyers may desire a firmer basis for making credibility attributions. Providing such a basis may constitute a valuable service that aggregators of online product reviews may consider supplying. ","with online retailers seeking to establish online communities and followings, and with manufactures and marketers increasingly desirous to influence the online reputation of their products, the implications of these findings will likely grow.",belongs_to_article,1
P5b,"with no further evidence , we concur with Cao and Everard 's START_CITE [ 11 ] END_CITE CITE_b12 proposition and thus propose that higher uncertainty avoidance scores will positively increase information privacy concerns ( P5b ) .","as is the case with masculinity, the literature has mixed results on the effect of uncertainty avoidance on information privacy concerns. Both Bellman et al. [9] and Milberg et al. [77] hypothesized that uncertainty avoidance would have a positive effect on information privacy concerns because high uncertainty avoidance scores are associated with high levels of anxiety, stress, and concern for security. However, Milberg et al. [77] found the effect to be negative while Bellman et al. [9] found it to be nonsignificant. another study found it to be positive [11] . Neither Bellman et al. [9] nor Milberg et al. [77] gave theoretical explanations for their results, but Cao and Everard [11] noted that individuals with high uncertainty avoidance scores would naturally avoid uncertainty about their personal information by limiting others' access. with no further evidence, we concur with Cao and Everard's [11] proposition and thus propose that higher uncertainty avoidance scores will positively increase information privacy concerns (P5b). ","with no further evidence, we concur with Cao and Everard's [11] proposition and thus propose that higher uncertainty avoidance scores will positively increase information privacy concerns (P5b).",belongs_to_article,1
IT cost,with lower IT cost synergies had neither high IT nor high business process relatedness .,"with lower IT cost synergies had neither high IT nor high business process relatedness. This pointed to independent positive effects of IT and business process relatedness on IT cost synergies (see Figures 3B and 4B). In a next step, we examined the cases within the respective groups for plausible explanations for these observed regularities (Eisenhardt, 1989) . We present these explanations in Section 4 when developing the respective propositions. ",with lower IT cost synergies had neither high IT nor high business process relatedness.,belongs_to_article,1
business process,with lower IT cost synergies had neither high IT nor high business process relatedness .,"with lower IT cost synergies had neither high IT nor high business process relatedness. This pointed to independent positive effects of IT and business process relatedness on IT cost synergies (see Figures 3B and 4B). In a next step, we examined the cases within the respective groups for plausible explanations for these observed regularities (Eisenhardt, 1989) . We present these explanations in Section 4 when developing the respective propositions. ",with lower IT cost synergies had neither high IT nor high business process relatedness.,belongs_to_article,1
data analysis,"with key issues described as : "" How to bring multiple sources and types of information together [ … ] to make better decisions [ … ] . "" , "" Integrating big data analysis with managerial decision making . "" and "" New approaches and sources of data - what are the roles of artificial intelligence , [ … ] , machine learning ? "" START_CITE [ 10 ] END_CITE CITE_b9 .","with key issues described as: ""How to bring multiple sources and types of information together […] to make better decisions […]."", ""Integrating big data analysis with managerial decision making."" and ""New approaches and sources of data-what are the roles of artificial intelligence, […], machine learning?"" [10] . According to Lilien [11] , there is also a spiking interest of B2B selling firms for machine learning and predictive analytics, driven by new data sources that become available. In summary, several authors have stated the need to explore the added value of big data applications and analytics in business environments, thereby taking into account the data, tools and algorithms that can be used (e.g., [9, 12] ). Recently, Chen et al. [13] showed that the use of big data analytics was responsible for 8.5% explained variance in asset productivity and 9.2% explained variance in business growth, which indicates the relevance of big data for value creation. ","with key issues described as: ""How to bring multiple sources and types of information together […] to make better decisions […]."", ""Integrating big data analysis with managerial decision making."" and ""New approaches and sources of data-what are the roles of artificial intelligence, […], machine learning?"" [10] .",background_information,3
government agencies,with key government leaders and prominent CEOs of government agencies invited to give the opening speech .,with key government leaders and prominent CEOs of government agencies invited to give the opening speech. EDI success stories from other countries were highlighted.,with key government leaders and prominent CEOs of government agencies invited to give the opening speech.,related_work,2
business intelligence,"with its predominantly direct sales model , Dell 's sales and marketing division has a much better grip on the customer demand trend than its suppliers , and tightly guards such business intelligence against its competitors START_CITE [ 36 ] END_CITE CITE_b31 .","This demand and order sequence is based on practices observed across many industries, including the fashion apparel, computer, electronics, and automobile industries where it is quite common to place customized orders. 3 For instance, after receiving customized computer orders from its customers, Dell, as an online retailer, will start the computer production by placing component orders to its suppliers who must in turn fill the orders promptly from its finished component inventory [36] . with its predominantly direct sales model, Dell's sales and marketing division has a much better grip on the customer demand trend than its suppliers, and tightly guards such business intelligence against its competitors [36] . general Motors (gM), for example, relies heavily on forecast information provided by its dealers, who have better knowledge of the future demand to decide on component capacity and production in gM's assembly lines [47] . 4 what these industries also have in common is that suppliers face relatively long lead times and/or short product life cycles. we assume our model deals with situations where a combination of long lead times and/or short product life cycles creates enormous pressures for a manufacturer to achieve on-time delivery and necessitates the start of production before receiving the final orders. 5 The process implies that the manufacturer could take ownership of an exception and be accountable for its resolution [41] . The decision on Q is made after the forecasting process is finished and the demand update is accepted. This sequence strictly follows the CPFR framework that requires order replenishment decisions and collaboration to be made after demand forecasting collaboration [13, 61] , which is highlighted in Figure 1. Specifically, ""Collaborative Replenishment"" in Figure 1, which includes initial order forecast and collaboration, is the step following ""Collaborative Forecasting,"" which is the focus of this study, and is followed by order execution. Solving the above problem backward, since the manufacturer believes the demand is now distributed as f (x | u[ ), the manufacturer expects his profit for a given production quantity Q and a demand update claim u[ without the collaboration-related costs: ","with its predominantly direct sales model, Dell's sales and marketing division has a much better grip on the customer demand trend than its suppliers, and tightly guards such business intelligence against its competitors [36] .",background_information,3
IT security,"with increased risks and more regulations , health - care organizations have increasingly implemented security resources , including IT security applications , prevention , and audit procedures .","According to the resource-based view, large organizations are likely to have more resources than small ones [43] , such as IT security applications, procedures, and IT security staff. These differences produce heterogeneous outcomes such as regulatory compliance and breach occurrences [16] . with limited security budgets, organizations need to understand the most effective approach to achieving security and compliance goals while saving time and resources. historically, organizations have followed technically focused strategies for designing effective information security solutions because security has been perceived to be a technical problem [66] . IT security equipment and applications are generally believed to improve an organization's ability to monitor suspicious activities and prevent data breaches. however, security is not just a technical but also a managerial problem. Thus, our study further includes the strategic importance of security procedural resources. Security education programs, policies, and procedures are available and useful in detecting and responding to information threats. Security procedural resources involve preventing security breaches and auditing all information flow controls [69]. In an analogous area of operations management, Ittner et al. [33] studied defect rates in manufacturing organizations and showed the impact of prevention and audit activities on quality performance. Likewise, the security context requires prevention and audit processes to detect and respond to information threats. with increased risks and more regulations, health-care organizations have increasingly implemented security resources, including IT security applications, prevention, and audit procedures. ","with increased risks and more regulations, health-care organizations have increasingly implemented security resources, including IT security applications, prevention, and audit procedures.",background_information,3
advertising,"with coupons and sweepstakes ; 5 and an advertising database with active TV , print , and radio data .","Single-source data is collected at various stages in the product flow. Shipment data is collected on items from the factory to the warehouse, withdrawal data between the warehouse and retailer, consumer data at take-away time, and data about promotional activity. Five primary databases on which single-source data Ž . are accumulated: 1 a household database with con-Ž . sumer data; 2 a store database with sales and local Ž . promotions; 3 a retail factors database with pricing, Ž . display and features; 4 a promotion factors database Ž . with coupons and sweepstakes; 5 and an advertising database with active TV, print, and radio data. The power of a single-source system comes from a complex set of interactions among these five. Put Ž simply, the system tracks what products are sold the . Ž trade environment ; who bought these products the . consumer environment ; and why these products were Ž . w x bought the promotion environment 5 . Table 1 presents an example framework which could represent a single-source dataset as a four dimensional Cartesian product. That is, a single- ","with coupons and sweepstakes; 5 and an advertising database with active TV, print, and radio data.",background_information,3
constructs,with analyzing the business model concept to further decompose it into its fundamental constructs .,"with analyzing the business model concept to further decompose it into its fundamental constructs. The specification of business model components ranks second in research popularity, as shown in Table 2. However, business model components assume the leading position when only recent studies are considered. This is somewhat expected and indicates a maturation of research in the field that naturally shifts from earlier definitional research to more detailed ontological analyses. (c) Taxonomies. Research in this domain relates to possible categorizations of business models into a number of typologies based on various criteria. The primary purpose is to produce a list of generic business model types that can then be analysed based on their unique features. A relatively significant portion of work, deriving mainly from early authors, has been performed in this field. (d) Conceptual models. Research in this domain aims at organizing information about a business model around a number of different perspectives. While research on components focuses on identifying the constituent elements of a business model, research in this sub-domain focuses on identifying and describing the relationship between these elements in an Abstract.  but rational way. As part of research in this field, a number of possible representational formalisms (usually graphical) for visualizing the main elements of a business model, as well as their Framework for analysing eBusiness models inter-relationships, under a specific aspect, have been produced. This is a domain of growing research interest, as demonstrated by the increasing number of publications in the field. (e) Design methods and tools. Research in this field concerns the development and use of methods, languages, standards, and software (e.g., simulation tools), typically referred to as business modeling tools, to automate and leverage the process of designing a business model. Although research interest in this sub-domain has been identified relatively early, it still remains as a timely research challenge, possibly due to the clear need of organizations to design, experiment, and change business models in an easy and cost-effective fashion. (f) Adoption factors. It involves research on factors that affect the organizational adoption of business models, as well as research on socio-economic implications of business model innovation. Compared to other sub-domains, a relatively smaller segment of the research community is pursuing this type of research. (g) Evaluation models. This domain is concerned with identifying criteria for either assessing the feasibility, viability, and profitability of new business models or evaluating them against alternative or best practice cases. This is also a relatively recent research domain with few researchers having pursued focused work on it. (h) Change methodologies. This domain includes research efforts that focus on formulating guidelines, describing steps, and specifying actions to be taken for either changing existing business models or choosing new ones to adapt to a business or technology innovation. This is also a relatively new area with intense interest for further investigation but only a few studies currently addressing it. ",with analyzing the business model concept to further decompose it into its fundamental constructs.,related_work,2
pricing,"with a rich array of theories regarding information asymmetry , network effects , switching cost , bundling , pricing , and transaction costs , economics has been the dominant reference discipline in this line of research in information systems .","anitesh BaRua is the william F. wright Centennial Professor of Information technology in the Department of Information, Risk and Operations Management at the McCombs School of Business, university of texas at Austin. he received his Ph.D. from Carnegie Mellon university. his research interests are in the areas of economics of information systems, outsourcing governance, and social media. he has published over 75 articles in academic journals and refereed conference proceedings. he has served as associate editor for Management Science and Information Systems Research and as senior editor for Information Systems Research. he serves on the editorial boards of the Journal of Management Information Systems and International Journal of Electronic Commerce. andRew B. whinston is the hugh Roy Cullen Centennial Chair in Business Administration, professor of information systems, computer science and economics, and director of the Center for Research in Electronic Commerce at the university of texas at Austin. he received his Ph.D. in economics from Carnegie Mellon university. Dr. whinston is the co-author or co-editor of 23 books and over 300 articles. electRonic commeRce has come a long way since the advent of web-based business in the early to mid-1990s. with a rich array of theories regarding information asymmetry, network effects, switching cost, bundling, pricing, and transaction costs, economics has been the dominant reference discipline in this line of research in information systems. By encouraging and publishing high-quality electronic commerce articles for two decades, the Journal of Management Information Systems has played a key role in the development of the academic literature in this domain. this special issue showcases a wide variety and richness of problems, as well as the sophistication of methodology and insights representing the cutting edge in contemporary electronic commerce research. ","with a rich array of theories regarding information asymmetry, network effects, switching cost, bundling, pricing, and transaction costs, economics has been the dominant reference discipline in this line of research in information systems.",background_information,3
transaction costs,"with a rich array of theories regarding information asymmetry , network effects , switching cost , bundling , pricing , and transaction costs , economics has been the dominant reference discipline in this line of research in information systems .","anitesh BaRua is the william F. wright Centennial Professor of Information technology in the Department of Information, Risk and Operations Management at the McCombs School of Business, university of texas at Austin. he received his Ph.D. from Carnegie Mellon university. his research interests are in the areas of economics of information systems, outsourcing governance, and social media. he has published over 75 articles in academic journals and refereed conference proceedings. he has served as associate editor for Management Science and Information Systems Research and as senior editor for Information Systems Research. he serves on the editorial boards of the Journal of Management Information Systems and International Journal of Electronic Commerce. andRew B. whinston is the hugh Roy Cullen Centennial Chair in Business Administration, professor of information systems, computer science and economics, and director of the Center for Research in Electronic Commerce at the university of texas at Austin. he received his Ph.D. in economics from Carnegie Mellon university. Dr. whinston is the co-author or co-editor of 23 books and over 300 articles. electRonic commeRce has come a long way since the advent of web-based business in the early to mid-1990s. with a rich array of theories regarding information asymmetry, network effects, switching cost, bundling, pricing, and transaction costs, economics has been the dominant reference discipline in this line of research in information systems. By encouraging and publishing high-quality electronic commerce articles for two decades, the Journal of Management Information Systems has played a key role in the development of the academic literature in this domain. this special issue showcases a wide variety and richness of problems, as well as the sophistication of methodology and insights representing the cutting edge in contemporary electronic commerce research. ","with a rich array of theories regarding information asymmetry, network effects, switching cost, bundling, pricing, and transaction costs, economics has been the dominant reference discipline in this line of research in information systems.",background_information,3
technology adoption,with a methodological individualist model developed from the aggregation of individual - level theories of technology adoption such as TAM .,"In light of our second objective, we sought to compare our non-reductionist model of technology adoption by groups (described above) with a methodological individualist model developed from the aggregation of individual-level theories of technology adoption such as TAM. ",with a methodological individualist model developed from the aggregation of individual-level theories of technology adoption such as TAM.,background_information,3
r 2,"with a mass point u / 2 − u at V h , and G r 2 P = Prob P r 2 > P = u 2 1 − u V h − P P − V l • Infomediary fee :","with a mass point u / 2 − u at V h , and G r 2 P = Prob P r 2 > P = u 2 1 − u V h − P P − V l • Infomediary fee: ","with a mass point u / 2 − u at V h , and G r 2 P = Prob P r 2 > P = u 2 1 − u V h − P P − V l • Infomediary fee:",background_information,3
IT,"with a large - scale , government - funded Information Technology program over an extended period .","To illustrate how one can use Figure 1 to depict stakeholder groups' dynamic movement, we summarize our findings in three diagrams covering the two important phases of health IT policy implementation. The program was initially called the NHSNet and was renamed after the launch of the NPfIT in 2002 as the NHS N3. Each diagram presents our eight main stakeholder groups and illustrates how their respective positions altered over time. Our theorization does not extend to detailed causal explanations about why a particular stakeholder group shifted its position. Rather, we demonstrate the importance of identifying different key stakeholder groups and how they engaged (or disengaged) with a large-scale, government-funded IT program over an extended period. This methodological point is important since research enquiry that covers a limited period (e.g., the launch of a program) may not reveal the potential for some stakeholder groups who are initially not engaged to become actively vocal in their concerns about a program. Figure 2 presents the stakeholder groups at the launch of the NHS-wide networking project in phase 1, which covered the 1993-1995 period. Here, key stakeholders, notably the government agencies who spearheaded the NPfIT, engaged the services of the various technology suppliers to bid for large contracts to develop health IT networks. The NHS-wide networking project promoted health IT as a means to improve health service delivery. Not surprisingly, its outlook was optimistic and the relevant NHS literature formally articulated the project as enabling the NHS to communicate with each other efficiently, securely and costeffectively (NHS Executive, 1994). At the time, the intended users of the system, notably GPs, adopted a ""wait-and-see"" attitude, which our interviewees' mixed views show: GPs recognized the need to improve the exchange of information in the NHS while also maintaining some skepticism about a new initiative ""led from the center"". ","with a large-scale, government-funded IT program over an extended period.",background_information,3
retailer,with a j N 0 and b j N 0 . Parameter θ jl denotes the degree of substitutability / complementarity between retailer j and retailer l.,"with a j N 0 and b j N 0. Parameter θ jl denotes the degree of substitutability/complementarity between retailer j and retailer l. Because the product is assumed to be substitutable for customers, following [9] , we have θ jl ≥ 0 for all l, j ∈ {1, 2, …, J}, l ≠ j and ∑ J l = 1;l≠j θ jl ≤1. Parameter θ jl = 0 if demand of retailer j is independent of demand of retailer l; θ jl N 0 if the product is substitutable and there is competition on the market demand between retailer j and retailer l. The larger the value of θ jl , the more intensive the competition between retailer j and retailer l [26] . Therefore, parameter θ jl can be interpreted as a measure of horizontal competition between retailer j and retailer l. ",with a j N 0 and b j N 0. Parameter θ jl denotes the degree of substitutability/complementarity between retailer j and retailer l.,belongs_to_article,1
comparative fit index,"with a cutoff value of 3 START_CITE [ 160 ] END_CITE CITE_b159 ; ( 2 ) the values of goodness - of - fit index , comparative fit index , and normed fit index with the cutoff value of 0.90 recommended by Hu and Bentler START_CITE [ 161 ] END_CITE CITE_b160 and Anderson and Gerbing START_CITE [ 148 ] END_CITE CITE_b147 ; and ( 3 ) root mean square error of approximation with the cutoff value of 0.08 recommended by Hu and Bentler START_CITE [ 161 ] END_CITE CITE_b160 .","Next, we assessed the overall model fit by using (1) the value of χ 2 /d. f. with a cutoff value of 3 [160] ; (2) the values of goodness-of-fit index, comparative fit index, and normed fit index with the cutoff value of 0.90 recommended by Hu and Bentler [161] and Anderson and Gerbing [148] ; and (3) root mean square error of approximation with the cutoff value of 0.08 recommended by Hu and Bentler [161] . The values of multiple indices in this study suggest the model fits the data satisfactorily (X 2 /d.f. = 1.98; the values of goodness-of-fit index = 0.91; comparative fit index = 0.94; normed fit index = 0.92; root mean square error of approximation = 0.05). ","with a cutoff value of 3 [160] ; (2) the values of goodness-of-fit index, comparative fit index, and normed fit index with the cutoff value of 0.90 recommended by Hu and Bentler [161] and Anderson and Gerbing [148] ; and (3) root mean square error of approximation with the cutoff value of 0.08 recommended by Hu and Bentler [161] .",belongs_to_article,1
goodness-of-fit index,"with a cutoff value of 3 START_CITE [ 160 ] END_CITE CITE_b159 ; ( 2 ) the values of goodness - of - fit index , comparative fit index , and normed fit index with the cutoff value of 0.90 recommended by Hu and Bentler START_CITE [ 161 ] END_CITE CITE_b160 and Anderson and Gerbing START_CITE [ 148 ] END_CITE CITE_b147 ; and ( 3 ) root mean square error of approximation with the cutoff value of 0.08 recommended by Hu and Bentler START_CITE [ 161 ] END_CITE CITE_b160 .","Next, we assessed the overall model fit by using (1) the value of χ 2 /d. f. with a cutoff value of 3 [160] ; (2) the values of goodness-of-fit index, comparative fit index, and normed fit index with the cutoff value of 0.90 recommended by Hu and Bentler [161] and Anderson and Gerbing [148] ; and (3) root mean square error of approximation with the cutoff value of 0.08 recommended by Hu and Bentler [161] . The values of multiple indices in this study suggest the model fits the data satisfactorily (X 2 /d.f. = 1.98; the values of goodness-of-fit index = 0.91; comparative fit index = 0.94; normed fit index = 0.92; root mean square error of approximation = 0.05). ","with a cutoff value of 3 [160] ; (2) the values of goodness-of-fit index, comparative fit index, and normed fit index with the cutoff value of 0.90 recommended by Hu and Bentler [161] and Anderson and Gerbing [148] ; and (3) root mean square error of approximation with the cutoff value of 0.08 recommended by Hu and Bentler [161] .",background_information,3
False Positives,"with TP : True Positives , FN : False Negatives , FP : False Positives , TN : True Negatives , P : Positives ( event ) , and N : Negatives ( non - event ) .","with TP: True Positives, FN: False Negatives, FP: False Positives, TN: True Negatives, P: Positives (event), and N: Negatives (non-event). AUC is restricted between the values of 0.5 and 1, where the former denotes that the model does not perform better than random and the latter indicates a perfect prediction [43] . If the AUC is below 0.5 in the test set, this is a strong indication of overfitting. ","with TP: True Positives, FN: False Negatives, FP: False Positives, TN: True Negatives, P: Positives (event), and N: Negatives (non-event).",background_information,3
R = 0.817,"with R = 0.817 , F = 8.909 , t X 2 = 3.135 , t X 4 = 2.790 , t X 6 = − 4.772 , t X 7 = 4.334 ;","with R = 0.817, F = 8.909, t X 2 = 3.135, t X 4 = 2.790, t X 6 = − 4.772, t X 7 = 4.334; ","with R = 0.817, F = 8.909, t X 2 = 3.135, t X 4 = 2.790, t X 6 = − 4.772, t X 7 = 4.334;",belongs_to_article,1
IoT,with innovation capability - IC .,with IoT-IC.,with IoT-IC.,background_information,3
developing countries,"with Internet access , such as the OLPC Project 's XO , Asus 's eeePC , and Intel 's Classmate are others that are gaining ground in developing countries .","Beyond the PC itself, there is a variety of low-cost personal computing technologies that can connect to the Internet. Technologies such as ""network PCs"" that failed to gain traction in the United States might well be ideal for the developing world, as also noted by James (2002) . with Internet access, such as the OLPC Project's XO, Asus's eeePC, and Intel's Classmate are others that are gaining ground in developing countries. Over one million of these devices were deployed in 2007 and five million are projected for 2008 (Sharma and Kraemer 2008) . Even these devices are too expensive for some countries and deployment in other countries frequently requires government subsidy not only for the hardware but also for user training and ongoing support. ","with Internet access, such as the OLPC Project's XO, Asus's eeePC, and Intel's Classmate are others that are gaining ground in developing countries.",background_information,3
affordances,"with Instagram stories , indicating their similarities to the affordances associated with Snapchat stories .","with Instagram stories, indicating their similarities to the affordances associated with Snapchat stories. ","with Instagram stories, indicating their similarities to the affordances associated with Snapchat stories.",background_information,3
developing countries,"with Internet access , such as the OLPC Project 's XO , Asus 's eeePC , and Intel 's Classmate are others that are gaining ground in developing countries .","Beyond the PC itself, there is a variety of low-cost personal computing technologies that can connect to the Internet. Technologies such as ""network PCs"" that failed to gain traction in the United States might well be ideal for the developing world, as also noted by James (2002) . with Internet access, such as the OLPC Project's XO, Asus's eeePC, and Intel's Classmate are others that are gaining ground in developing countries. Over one million of these devices were deployed in 2007 and five million are projected for 2008 (Sharma and Kraemer 2008) . Even these devices are too expensive for some countries and deployment in other countries frequently requires government subsidy not only for the hardware but also for user training and ongoing support. ","with Internet access, such as the OLPC Project's XO, Asus's eeePC, and Intel's Classmate are others that are gaining ground in developing countries.",background_information,3
Instagram,"with Instagram stories , indicating their similarities to the affordances associated with Snapchat stories .","with Instagram stories, indicating their similarities to the affordances associated with Snapchat stories.","with Instagram stories, indicating their similarities to the affordances associated with Snapchat stories.",background_information,3
rational choice theory,"with information systems security procedures include deterrence theory , protection motivation theory , rational choice theory , and theory of reasoned action","Theories frequently applied to examine employee compliance (or noncompliance) with ISSPs include deterrence theory, protection motivation theory, rational choice theory, and theory of reasoned action (D'Arcy and Herath 2011, Karjalainen and Siponen 2011 , Crossler et al. 2013 , Siponen and Vance 2014 . In applying these theories and others, by and large, previous researchers have focused on identifying various (sets of) ""independent variables"" related to ISSBs that are presumed to be independent of (or the same) over time and security contexts. Yet, as the opening scenario suggests, the dynamics of change, including employees' shifting reasoning, are clearly relevant to understanding ISSBs, because individuals' priorities for enacting these behaviors may continue to change over time as the individuals' circumstances change. For instance, a user may routinely lock her computer when she becomes suspicious of her environment regarding potential security breaches, and later, this routine can be disrupted, either as an exception or as a repeating pattern, when new task or social priorities emerge. ","with ISSPs include deterrence theory, protection motivation theory, rational choice theory, and theory of reasoned action",background_information,3
digital platforms,with Chinese entrepreneurs on digital platforms .,"Second, as Sequeira and Rasheed (2006) suggested, the family plays a significant role in giving convenient and low-cost sources of information support. Therefore, kinship members and close friends are willing to share inspiration, creative ideas on application development, unique insights into user requirement and valuable information regarding ""permits, laws, management practices, reliable suppliers, and promising business lines"" (Aldrich and Waldinger, 1990, p. 127) with Chinese entrepreneurs on digital platforms. ",with Chinese entrepreneurs on digital platforms.,background_information,3
research model,"with Alternative Models To increase the validity of our research model , besides theoretically grounding the model in the absorptive capacity theory , we further tested two alternative models and compared their model fit with our research model .","with Alternative Models To increase the validity of our research model, besides theoretically grounding the model in the ACAP theory, we further tested two alternative models and compared their model fit with our research model. The results corresponding to these three models are summarized in Table 3. We use both Akaike information criterion (AIC) (Akaike 1973) and Bayesian information criterion (BIC) (Schwarz 1978 , Wasserman 2000 to compare these three models. When two models fit on the same data, the model with the smaller value of the information criterion is considered to Table 2 Results Summary for Research Model be better. The BIC approach attempts to identify the model with the highest probabilities of being the ""true"" model for the data, assuming that one of the models under consideration is true. While AIC is defined without reference to a ""true model."" Instead, AIC uses expected prediction of future data as the key criterion of the adequacy of a model (for more discussion, see Sileshi 2006 , Acquah 2010 . Both indices of AIC and BIC are commonly used to assess the relative fit of models with count variables, although they differ in their definitions of good model fit. ","with Alternative Models To increase the validity of our research model, besides theoretically grounding the model in the ACAP theory, we further tested two alternative models and compared their model fit with our research model.",belongs_to_article,1
survey,"with 78 percent already adopting the standards or planning to implemetit an XML strategy during the year 2002 [ 40| . According to the survey , the reason that companies adopt is to support systems - integration strategies . Financial institutions specifically need the ability to access multiple systems in real time to allow them to offer specific services to customers , business partners , and suppliers on the Web - such as verifying credit , placing orders , or performing a trade . XML and Web services offer an economical solution for integrated business services to be delivered via the Web .","with 78 percent already adopting the standards or planning to implemetit an XML strategy during the year 2002 [40|. According to the survey, the reason that companies adopt is to support systems-integration strategies. Financial institutions specifically need the ability to access multiple systems in real time to allow them to offer specific services to customers, business partners, and suppliers on the Web-such as verifying credit, placing orders, or performing a trade. XML and Web services offer an economical solution for integrated business services to be delivered via the Web.","with 78 percent already adopting the standards or planning to implemetit an XML strategy during the year 2002 [40|. According to the survey, the reason that companies adopt is to support systems-integration strategies. Financial institutions specifically need the ability to access multiple systems in real time to allow them to offer specific services to customers, business partners, and suppliers on the Web-such as verifying credit, placing orders, or performing a trade. XML and Web services offer an economical solution for integrated business services to be delivered via the Web.",related_work,2
degrees of freedom,"with 1 , ( "" -k ) degrees of freedom where n is the sample size and k is the number of constructs in the model START_CITE [ 32 ] END_CITE CITE_b29 .","where Var(f,) = 1 -X^^-, X is the item loading, and E is the error. 6. The formula for computing/^ is (/?'partial mediation -R^ full mediation)/{l -/?^ partial mediation). The pseudo F-statistic is computed using the formula/-* (« -A -1). with 1, ("" -k) degrees of freedom where n is the sample size and k is the number of constructs in the model [32] . ","with 1, ("" -k) degrees of freedom where n is the sample size and k is the number of constructs in the model [32] .",belongs_to_article,1
R < 1,with 0 < q θ R < 1 2 oeθ 0 Θ may be uniquely optimal for the supply side function SH,with 0 < q θ R < 1 oeθ 0 Θ may be uniquely optimal for the supply side.,with 0 < q θ R < 1 oeθ 0 Θ may be uniquely optimal for the supply side.,belongs_to_article,1
P 21j,"with ( j − 10 ) being the number of years since the firm 's last closure or audit event , and h k the lower five elements of the firm 's state vector . Furthermore , if P 21j ≥ θ(j , h k ) as well ( where P 21j is the probability of transitioning to an audit Markov state given that closure was not available ) , then saying "" yes "" to closure is preferable to not having the option at all . Because the audit probability is increased if a firm is given the option and declines it ( relative to the case where there was no option at all ) , i.e. , P 11j ( 2 ) > P 21j , it will always be advantageous for the firm to have ( and use ) the option if it would be willing to do so under the "" usual "" lower audit probabilities . On the other hand , if P 11j ( 2 ) is below the threshold θ in ( 14 ) then so is P 21j , meaning that the firm 's reward would be lower under the option if taking the option is more expensive than discarding it . In general , the threshold θ that makes the option desirable for the firm depends on the firm 's state vector ( i.e. , on the Markov state , j , it is in , and on its past history of decisions , h k . However , there are choices for β , ' , and r ( including those in use today , discussed in the next section ) which are of practical interest and lead to the option being to the firm 's advantage uniformly , for any j and h k .","with (j − 10) being the number of years since the firm's last closure or audit event, and h k the lower five elements of the firm's state vector. Furthermore, if P 21j ≥ θ(j, h k ) as well (where P 21j is the probability of transitioning to an audit Markov state given that closure was not available), then saying ""yes"" to closure is preferable to not having the option at all. Because the audit probability is increased if a firm is given the option and declines it (relative to the case where there was no option at all), i.e., P 11j (2) > P 21j , it will always be advantageous for the firm to have (and use) the option if it would be willing to do so under the ""usual"" lower audit probabilities. On the other hand, if P 11j (2) is below the threshold θ in (14) then so is P 21j , meaning that the firm's reward would be lower under the option if taking the option is more expensive than discarding it. In general, the threshold θ that makes the option desirable for the firm depends on the firm's state vector (i.e., on the Markov state, j, it is in, and on its past history of decisions, h k . However, there are choices for β, ', and r (including those in use today, discussed in the next section) which are of practical interest and lead to the option being to the firm's advantage uniformly, for any j and h k . Based on the above discussion, if P 21j > θ(j, h k ) along the optimal trajectory, then the firm will always say yes to closure, and having the option to do so is better than the alternative. In that case, the value functions J k will be increasing in the closure probability, p o , for all k. To see why that is, notice that the reward function (9) is independent of p o and of the actual availability of closure, c k . Also, in terms of the state transitions (4), p o only affects Pr( = 1) (the probability of arriving at a state where the firm has the option to use closure) and none of the remaining elements of the state vector (in particular, s k ) on which the reward function depends. Thus, the term EJ kþ1 Ax k þ Bu k þ n k ð Þ in (11) will be increasing in p o , because an increase in p o simply corresponds to a higher probability of a more favorable outcome (namely closure). These facts imply [17] that the long-term optimal reward function J ∞ will be increasing in p o as well. ","with (j − 10) being the number of years since the firm's last closure or audit event, and h k the lower five elements of the firm's state vector. Furthermore, if P 21j ≥ θ(j, h k ) as well (where P 21j is the probability of transitioning to an audit Markov state given that closure was not available), then saying ""yes"" to closure is preferable to not having the option at all. Because the audit probability is increased if a firm is given the option and declines it (relative to the case where there was no option at all), i.e., P 11j (2) > P 21j , it will always be advantageous for the firm to have (and use) the option if it would be willing to do so under the ""usual"" lower audit probabilities. On the other hand, if P 11j (2) is below the threshold θ in (14) then so is P 21j , meaning that the firm's reward would be lower under the option if taking the option is more expensive than discarding it. In general, the threshold θ that makes the option desirable for the firm depends on the firm's state vector (i.e., on the Markov state, j, it is in, and on its past history of decisions, h k . However, there are choices for β, ', and r (including those in use today, discussed in the next section) which are of practical interest and lead to the option being to the firm's advantage uniformly, for any j and h k .",belongs_to_article,1
eWOM,will go beyond prior work by examining the differing motivations for creating positive electronic word-of-mouth or negative electronic word-of-mouth .,"will go beyond prior work by examining the differing motivations for creating positive eWOM or negative eWOM. Second, the six motives listed in Table 1 represent an inclusive list, one that contains inconsistent results. The first three motives -enjoyment, altruism, and attachment -are found to be consistently significant in affecting writers' behavior. Each are included in the present study. In the case of the fourth motive, utilitarian motivation, results are mixed. This variable is included in the present study because it is frequently reported that people intentionally write eWOM for monetary purposes [39, 40] . Indeed, of the motivations that have been identified, the potential to enhance one's own self-worth, concern for others, the desire for social interaction, and economic incentives are the most influential [7] . And the last two motives -product involvement and message involvement -are the least investigated and results across previous studies are not consistent. Given the inconsistent results and only sporadic interest in the final two items in the table -product involvement and message involvement -we have excluded the last two motives without further consideration.",will go beyond prior work by examining the differing motivations for creating positive eWOM or negative eWOM.,belongs_to_article,1
business models,will also require a fundamental transformation of existing strategies and business models . START_CITE Chesbrough ( 2003 ) END_CITE CITE_b21 states that an understanding of the ecosystem is critical to developing effective business models .,"will also require a fundamental transformation of existing strategies and business models. Chesbrough (2003) states that an understanding of the ecosystem is critical to developing effective business models. By visualizing the structure and potential dynamics of interfirm relations of the converging mobile ecosystem, the network approach presented in this study will thus help firms construct more valuable business models.",will also require a fundamental transformation of existing strategies and business models. Chesbrough (2003) states that an understanding of the ecosystem is critical to developing effective business models.,belongs_to_article,1
IT,"wiTh The adVenT of whaT is colloquially TeRmed The network , computer , or information age , there is growing recognition among business and information systems ( information systems ) executives that data , or more precisely , information , may be the only inimitable information technology resource that can create a sustainable competitive advantage START_CITE [ 27 , END_CITE CITE_b22 START_CITE 39 , END_CITE CITE_b33 START_CITE 48 ] END_CITE CITE_b42 .","wiTh The adVenT of whaT is colloquially TeRmed The network, computer, or information age, there is growing recognition among business and information systems (IS) executives that data, or more precisely, information, may be the only inimitable IT resource that can create a sustainable competitive advantage [27, 39, 48] . Industry surveys show that the volume of raw data stored in corporate data centers is doubling in size every other year, with some industries, most notably health care, doubling in size annually [25, 44] . As data is transformed into information for decision making, practitioners describe how an organization's ability to realize value from its information is based, in part, on how information is governed over its life cycle [24]. Based on prior studies by Khatri and Brown [18] , Kooper et al. [23] , and Weber et al. [51] , we define information governance as a collection of capabilities or practices for the creation, capture, valuation, storage, usage, control, access, archival, and deletion of information over its life cycle. 1 Exponential data growth, most notably in recent years when the pace of data growth has begun to outpace the rate of decline in hardware costs causing total storage spending to climb, has transformed information governance and its related practices into a top issue for senior business and IS management [24, 32, 42] . 2 The IS practitioner literature sees information governance as having two goals: (1) to maximize the value of information to the organization by ensuring that information is reliable, secure, and accessible for decision making and (2) to protect information so that its value to the organization is not diminished through technology or human error, loss of timely access, inappropriate use, or misadventure [18, 35, 44] . Achieving these goals has inspired organizations to consider a variety of different information governance practices. As shown in our literature review, much of what we know about information governance comes from a handful of case studies and framework analyses that offer insights into the role and composition of information governance. Some insights can also be gleaned from the mainstream IT governance literature, principally as to physical infrastructure or personnel skills needed to support data center management, data security, application use, and access control. however, since the mainstream IT governance literature has tended to highlight the management, use, and control of physical IT artifacts-defined by Orlikowski and Iacono [30] as bundles of properties (functions or capabilities) packaged in hardware or software, and objectified by the IT governance literature in the context of IT infrastructure, IT resource allocation, locus of IT decision making, IT planning/control, and project management-researchers may need to exercise caution in extrapolating what we know about governing physical IT artifacts to governing nonphysical or information artifacts. hence, a goal of this study is to motivate consideration of the information artifact within the confines of IT governance but with the knowledge that a more inclusive view of IT governance-one that expressly includes information-may need to consider issues that are exclusive to information. For example, unlike physical artifacts, information can be easily and quickly replicated and shared across vast distances. Information is not a wasting asset whose value declines as a function of increased use as is the case with IT hardware. Paradoxically, the value of information can increase with greater use [44] .","wiTh The adVenT of whaT is colloquially TeRmed The network, computer, or information age, there is growing recognition among business and information systems (IS) executives that data, or more precisely, information, may be the only inimitable IT resource that can create a sustainable competitive advantage [27, 39, 48] .",background_information,3
collaborative technologies,"why would the Groups that used a collaboration technoloGy be more likely to follow the stage model of group development while groups that did not use the technology follow the punctuated equilibrium model ? clearly , one plausible explanation lies in the nature of the technology . The new work processes enabled by the technology 's features might have induced different patterns of group development or have changed the ways a group adapted . As mentioned above , these collaborative technologies are designed to influence the process by which groups interact . However , we do not believe that the changes we observed are purely due to the nature of the technology per se . Note : for percentage fit , behaviors that were possibly , but not clearly , observed were counted as half .","why would the Groups that used a collaboration technoloGy be more likely to follow the stage model of group development while groups that did not use the technology follow the punctuated equilibrium model? clearly, one plausible explanation lies in the nature of the technology. The new work processes enabled by the technology's features might have induced different patterns of group development or have changed the ways a group adapted. As mentioned above, these collaborative technologies are designed to influence the process by which groups interact. However, we do not believe that the changes we observed are purely due to the nature of the technology per se. Note: for percentage fit, behaviors that were possibly, but not clearly, observed were counted as half.","why would the Groups that used a collaboration technoloGy be more likely to follow the stage model of group development while groups that did not use the technology follow the punctuated equilibrium model? clearly, one plausible explanation lies in the nature of the technology. The new work processes enabled by the technology's features might have induced different patterns of group development or have changed the ways a group adapted. As mentioned above, these collaborative technologies are designed to influence the process by which groups interact. However, we do not believe that the changes we observed are purely due to the nature of the technology per se. Note: for percentage fit, behaviors that were possibly, but not clearly, observed were counted as half.",belongs_to_article,1
standard deviation,"whose time elapsed was at least one and a half 1.5 times greater than the average duration , as calculated Ž in step 1 the ' 1.5 ' multiplying factor is based on the distribution of execution times and allows for the identification of those execution times which are more than one standard deviation away from the .","Time delays in execution were identified and tabulated as follows. First, for each individual, the average time required to perform an individual ac-Ž . tion was computed step 1 . Second, time delays were identified by denoting each individual action Ž . whose time elapsed was at least one and a half 1.5 times greater than the average duration, as calculated Ž in step 1 the '1.5' multiplying factor is based on the distribution of execution times and allows for the identification of those execution times which are more than one standard deviation away from the . mean . Such delays correspond to those instances when individuals had to pause in order to determine what action to take next and are indicative of implementation difficulties.","whose time elapsed was at least one and a half 1.5 times greater than the average duration, as calculated Ž in step 1 the '1.5' multiplying factor is based on the distribution of execution times and allows for the identification of those execution times which are more than one standard deviation away from the .",belongs_to_article,1
evaluation of Internet,whose primary goal is to produce theory driven criteria to guide the development and evaluation of Internet - based medical courses .,"Besides the generic, yet valuable approach proposed by Webster and Watson (2002) , there exist specific methods which serve the purpose of theory development. Paré and colleagues (2015) identified one particular theory-driven approach which is highly relevant to the IS field, that is, the realist review (Pawson, Greenhalgh, Harvey, & Walshe, 2005) . The main goal of a realist review is to unpack the mechanisms of how ""complex interventions"" work in particular contexts. The basic research question -what works? -which is usually associated with qualitative systematic reviews changes to: what is it about this intervention that works, for whom, in what circumstances, in what respects and why? Realist reviews have no particular preference for either quantitative or qualitative evidence. As a theory-building approach, a realist review usually starts by articulating likely underlying mechanisms and then scrutinises available evidence to find out whether and where these mechanisms are applicable (Shepperd et al., 2009 ). Primary studies found in the extant literature are viewed as case studies which can test and modify the initial theories (Rousseau, Manning, & Denyer, 2008 ). An example of IS-related realist review is the article by Wong and colleagues (2010) whose primary goal is to produce theory driven criteria to guide the development and evaluation of Internet-based medical courses. The authors identified two main theories of the course-in-context that explained variation in learners' satisfaction and outcomes. Results indicated that learners were more likely to accept an online course if it offered a perceived advantage over available non-Internet alternatives, was easy to use technically, and compatible with their values and norms. Importantly, interactivity led to effective learning only if learners were able to enter into a dialogue with a tutor, fellow students or virtual tutorials and gain formative feedback. ",whose primary goal is to produce theory driven criteria to guide the development and evaluation of Internet-based medical courses.,background_information,3
innovations,"who were responsible for innovations in skateboarding and windsurfing equipment START_CITE ( Shah , 2003 ) END_CITE CITE_b38",,,background_information,3
WeChat,who was considering whether or not to send a message in WeChat after online dating .,"who was considering whether or not to send a message in WeChat after online dating. Here, ""A,"" rather than a specific name, was used for 2 reasons. First, with a specific name, participants may not become involved in the scenario where the used name is not the same as their real name. The gender of participants may also not be consistent with that of the specific name. When he/she should be used in the scenario, ""TA"" (pronounced as ""he/she"" in Chinese, equivalent to ""they"" being used as a gender-neutral singular pronoun in English) was used instead. The interface of the hypothetical scenario is shown in Appendix B. ",who was considering whether or not to send a message in WeChat after online dating.,background_information,3
environmental sustainability,"who investigated the topic of environmental sustainability and IT with the goal to develop a holistic , transdisciplinary , integrative framework for business transformation .","We found a few narrative and theory development reviews in our sample that provide good illustrations of the application of this reporting guideline. The exemplar we selected is provided by Elliot (2011) who investigated the topic of environmental sustainability and IT with the goal to develop a holistic, transdisciplinary, integrative framework for business transformation. Consistent with Webster and Watson (2002) , the author adopted a highly structured and transparent approach to identifying and conceptualising relevant materials. Considering the huge volume of literature on this topic, ""the initial or scoping phase sought to examine authoritative sources to determine a problem specification, propose a focus and scope, and suggest a structure for the review to meet the aims of this article"" (p. 201). Seventeen authoritative sources, including seminal, intergovernmental, governmental, practitioner, and social activist reports, were identified by the author during several years of work in this domain. According to the author, purposive sampling was necessary as the authoritative sources were not necessarily identifiable through searches of academic literature. ","who investigated the topic of environmental sustainability and IT with the goal to develop a holistic, transdisciplinary, integrative framework for business transformation.",background_information,3
IT,who found that the number of information technology signals varied across different industries depending on the information technology 's role .,"(2010) who found that the number of IT signals varied across different industries depending on the IT's role. Consequently, we could expect varying market reactions to announcements across industries.",who found that the number of IT signals varied across different industries depending on the IT's role.,background_information,3
survey,who completed the survey on Mechanical Turk for a small monetary payment .,"To confirm the high prosocial nature of the project description used in this study, we conducted a post-test with forty-nine participants (21 females",,belongs_to_article,1
technology infrastructure,who are able to implement and support the technology infrastructure and business process choreography .,"who are able to implement and support the technology infrastructure and business process choreography. Overall, firms with a greater technology readiness are in a position conducive to adopt and implement IBPS.",who are able to implement and support the technology infrastructure and business process choreography.,belongs_to_article,1
reflective indicators,while y 1 -y 4 are specified as reflective indicators of a separate construct η 2 ( ETA2 ) .,"while y 1 -y 4 are specified as reflective indicators of a separate construct η 2 (ETA2). Note the fixing of the error variance of ETA1 to zero and also the inclusion of the SO option in the ""Options"" line (the latter option is necessary when using a formative indicator as a marker variable for scaling purposes",,belongs_to_article,1
event studies,"while typical event studies use very short event windows ( typically less than 10 days ) , risk measures are known to persist over a longer period .","Downside risk we operationalize downside risk as firm performance relative to industry performance. the relevant industry classification was the three-code North American Industry Classification System classification. to calculate the downside risk metric and its determinants, we employed a couple of measures: one market based and the other accounting based. for the market-based measure, two contiguous 60-day periods (-70 to -10 days and 10 to 70 days) were used. while typical event studies use very short event windows (typically less than 10 days), risk measures are known to persist over a longer period. Specifically, our choice of the 60-day window length is based on empirical findings in the finance and accounting literature indicating that there is persistence in post-earnings-announcement drift up to 60 trading days subsequent to the earnings announcement (and even up to 120 trading days for large firms) [5] . we elected to exclude a 10-day window surrounding the announcement date to provide a reasonable assurance that any downside risk captured by our calculations would not be biased by the announcement effect and that any volatility associated with the short-term announcement would not be included in our measure of downside risk. 1 for the accounting-based measure (return on assets [rOA]) of downside risk, two contiguous three-year periods (-4 to -1 years and 1 to 4 years) were used. the two three-year periods were selected to obtain sufficient data to construct the downside risk measure. we excluded the year of announcement to provide a higher degree of assurance that any changes in downside risk would not be biased by the announcement effect and that any volatility associated with the announcement would not be included in our measure of downside risk.","while typical event studies use very short event windows (typically less than 10 days), risk measures are known to persist over a longer period.",background_information,3
theorizing,"while this simultaneous need for alignment and adaptation is pervasive in theorizing about interfirm control mechanisms [ 56 ] , few empirical studies have directly and simultaneously assessed both facets .","recognizing the Dual role of Control Mechanisms in Systems Development the role of controls in facilitating alignment of vendor activities with client objectives is explicitly recognized, whereas their role in facilitating adaptation in systems development projects is pervasive but implicit in prior research. For example, Henderson and lee [40] have shown that formal controls are associated with project efficiency and effectiveness. Similarly, tiwana and Keil [90] have recently shown how different controls differ in their impact on performance in internal and outsourced projects. However, besides alignment, both the interfirm governance literature [16, 17, 44, 50] and the tCE and relational contracting perspectives (e.g., [52, 95] ) implicitly recognize the need for adaptiveness to correct misalignments with evolving client imperatives. while this simultaneous need for alignment and adaptation is pervasive in theorizing about interfirm control mechanisms [56], few empirical studies have directly and simultaneously assessed both facets. Similarly, the requirements analysis literature has also emphasized the need for software projects to meet requirements that are representative of actual user needs [58] , which themselves might evolve over the course of systems development. Inattention to flexibility in the systems development process is also an explicitly recognized theoretical gap in the IS controls literature [90]. Systems development ambidexterity is therefore a theoretically useful representation of software development performance, which is subsequently demonstrated in this paper as also being associated with classical efficiency and effectiveness measures of performance.","while this simultaneous need for alignment and adaptation is pervasive in theorizing about interfirm control mechanisms [56], few empirical studies have directly and simultaneously assessed both facets.",background_information,3
survey,"while this concern can be alleviated by collecting data from more than one respondent per firm , it can be pragmatically difficult to do so due to the data attrition that occurs when only one of the contacted persons responds to their portion of the survey .","One of the major concerns with self-report data is common method bias. while this concern can be alleviated by collecting data from more than one respondent per firm, it can be pragmatically difficult to do so due to the data attrition that occurs when only one of the contacted persons responds to their portion of the survey. Similarly, while the concern can be alleviated by collecting information about the dependent variable from a secondary source, it is difficult to obtain objective process-level performance data. given these practical constraints, we took steps to design the instrument to safeguard against common method bias by using concise and clear items, employing a mix of likert, guttman, and ratio scales to measure independent and dependent variables, and controlling for scale length [53] .","while this concern can be alleviated by collecting data from more than one respondent per firm, it can be pragmatically difficult to do so due to the data attrition that occurs when only one of the contacted persons responds to their portion of the survey.",background_information,3
database,"while they consider fairness to the database creators , they are also concerned with social welfare , which is the value to all stakeholders .","Policymakers tend to approach the issue from a different perspective. while they consider fairness to the database creators, they are also concerned with social welfare, which is the value to all stakeholders. when the European union introduced the Database Directives, one of the main objectives was to stimulate the database production industry by providing protection to the investment in database creation. when a database is created mainly for the purpose of supporting the firm's core business, increased legal protection has little impact on the incentives of creating the database. thus we focus our analysis on databases created for sale.","while they consider fairness to the database creators, they are also concerned with social welfare, which is the value to all stakeholders.",background_information,3
strategic management,"while the traditional financial economics and decision theory perspectives on risk provide valuable insights and have been the cornerstones of empirical analysis , they obscure an understanding of risk in a strategic management context START_CITE [ 6 ] END_CITE CITE_b5 .","while the traditional financial economics and decision theory perspectives on risk provide valuable insights and have been the cornerstones of empirical analysis, they obscure an understanding of risk in a strategic management context [6] . the strategic management perspective on risk attempts to provide broader conceptualization of risk that is consistent with managerial views. March and Shapira [40] contend that managers have a substantially different conceptualization of risk than do financial economists and decision theorists. Managerial concepts on risk seem to focus more on organizational losses than organizational gains. Managers tend to be more concerned about downside risk than about outcome variability [30, 57] . Some empirical evidence from the management literature also suggests that downside risk is more relevant to decision makers than the total variance of outcomes (e.g., [54] ). therefore, variance measures may be less relevant to managerial decision-making behavior than measures focusing on downside risk characteristics. In contrast to decision theory, which views a firm with consistent performance to be less risky, a strategic management perspective would suggest that considerable risk is associated with a firm performing consistently below the profitability of its rivals [3, 40] . the strategic management perspective of risk is rooted in the concept of competitive advantage. In a competitive environment, a better-performing firm gains competitive advantage over a poorer-performing firm [44, 49]. Over an extended period, the firm with consistently lower returns than its rivals is in danger of failing. thus, the strategic management perspective defines risk in terms of relative performance [40, 47].","while the traditional financial economics and decision theory perspectives on risk provide valuable insights and have been the cornerstones of empirical analysis, they obscure an understanding of risk in a strategic management context [6] .",background_information,3
organization size,"while the percentage of organization size categories varied in each region , overall about 39 percent of our responding organizations had fewer than 500 employees , 43 percent of them had between 500 and 5,000 employees , and 18 percent had more than 5,000 employees .","table 2 shows the number of employees in our sample across all the five regions. while the percentage of organization size categories varied in each region, overall about 39 percent of our responding organizations had fewer than 500 employees, 43 percent of them had between 500 and 5,000 employees, and 18 percent had more than 5,000 employees. table 3 shows the number of organizations and the percentage of all the organizations in each of the five regions that had assimilated cIts. the overall percentages indicate a wide variation in the assimilation patterns of individual cIts. at the top end, almost 99 percent of the organizations in our sample had assimilated e-mail, 70 percent had assimilated teleconferencing, and 56 percent had assimilated videoconferencing. the reduction in overall assimilation of teleconferencing and videoconferencing can be attributed to their relatively low assimilation rates in Norway and Switzerland as compared to the other three regions. at the bottom end, only about 29 percent of organizations had assimilated electronic meeting systems in the five regions. table 4 shows the perceived organizational benefits for the six cIts analyzed in this paper. In our study, respondents found that teleconferencing resulted in the most benefits. For every benefit mentioned, teleconferencing had the highest number of respondents agreeing that it did indeed result in that benefit, averaging over 50 percent across all benefits. Videoconferencing was second highest, averaging nearly 47 percent. proprietary groupware followed with 40 percent, and data conferencing and web-based tools both had an average of about 33 percent agreement across all benefits. Finally, for every benefit mentioned, electronic meeting systems had the lowest number of respondents agreeing that it resulted in the benefit, for an average of about 25 percent. this may be one important reason that electronic meeting systems was least likely to be assimilated and used by the respondents in our study.","while the percentage of organization size categories varied in each region, overall about 39 percent of our responding organizations had fewer than 500 employees, 43 percent of them had between 500 and 5,000 employees, and 18 percent had more than 5,000 employees.",belongs_to_article,1
prior research,"while the main benefit of using real options analysis stems from the ability to reduce downside risk of investments , most prior research has discussed this generally in relation to managerial flexibility in making the investment .","we grounded our research in real options and rBV logic, and the results are largely consistent with our theoretical reasoning. from the theoretical perspective, our study alerts researchers especially on the use of real options thinking to analyze the business value of It investments made under uncertainty. while the main benefit of using real options analysis stems from the ability to reduce downside risk of investments, most prior research has discussed this generally in relation to managerial flexibility in making the investment. we draw attention to endogenous uncertainty, whose resolution requires active learning and engagement in business experiments through r&D. this not only contributes to real options thinking but also ties it to organizational learning and absorptive capacity literature. It has been pointed out that those It investments that contribute to exploitable absorptive capacity can lead to higher payoffs in the future [22] . Our analysis also reinforces the utility of the rBV as a theoretical lens to illuminate competitive advantage-related research. from a practical perspective, our study alerts managers to the need to understand the nature of uncertainty surrounding an It investment. If such uncertainty is exogenous (e.g., uncertainty surrounding an emerging technology that is not yet widely utilized) or does not require extensive learning, it may be prudent to ""wait and see."" However, if the uncertainty is endogenous (due to the resulting business process transformation) and extensive learning is required, it may be prudent to adopt the technology early and engage in active learning by conducting business experiments required to resolve this type of uncertainty. Of course, this will be influenced by the level of absorptive capacity of the organization. thus, forward-looking organizations should continuously build their absorptive capacity, say, through investment in r&D. furthermore, managers also need to evaluate whether a given It investment contributes to competitive advantage. for It investments that are largely competitive necessities, it is advisable to undertake such investments at the same time with industry participants to maintain competitive parity. limitations althOuGh Our StuDy iS theOretically GrOunDeD, it has some limitations. first, we used a cross-sectional data set limiting our ability to uncover dynamic processes and sequential investment strategy, which are needed to contain downside risk. for example, the resolution of uncertainty through learning and business experimentation is a dynamic process that extends, perhaps, through several periods. future research should undertake a longitudinal study to examine the effects investigated in this study. Despite this limitation, we still found considerable support for our hypotheses. Consistent with our theorizing, transformational and informate It investments lead to a reduction in downside risk only if they are made early. for automate It investments, the hypothesized effect of parity timing is supported. ","while the main benefit of using real options analysis stems from the ability to reduce downside risk of investments, most prior research has discussed this generally in relation to managerial flexibility in making the investment.",background_information,3
real options,"while the main benefit of using real options analysis stems from the ability to reduce downside risk of investments , most prior research has discussed this generally in relation to managerial flexibility in making the investment .","we grounded our research in real options and rBV logic, and the results are largely consistent with our theoretical reasoning. from the theoretical perspective, our study alerts researchers especially on the use of real options thinking to analyze the business value of It investments made under uncertainty. while the main benefit of using real options analysis stems from the ability to reduce downside risk of investments, most prior research has discussed this generally in relation to managerial flexibility in making the investment. we draw attention to endogenous uncertainty, whose resolution requires active learning and engagement in business experiments through r&D. this not only contributes to real options thinking but also ties it to organizational learning and absorptive capacity literature. It has been pointed out that those It investments that contribute to exploitable absorptive capacity can lead to higher payoffs in the future [22] . Our analysis also reinforces the utility of the rBV as a theoretical lens to illuminate competitive advantage-related research. from a practical perspective, our study alerts managers to the need to understand the nature of uncertainty surrounding an It investment. If such uncertainty is exogenous (e.g., uncertainty surrounding an emerging technology that is not yet widely utilized) or does not require extensive learning, it may be prudent to ""wait and see."" However, if the uncertainty is endogenous (due to the resulting business process transformation) and extensive learning is required, it may be prudent to adopt the technology early and engage in active learning by conducting business experiments required to resolve this type of uncertainty. Of course, this will be influenced by the level of absorptive capacity of the organization. thus, forward-looking organizations should continuously build their absorptive capacity, say, through investment in r&D. furthermore, managers also need to evaluate whether a given It investment contributes to competitive advantage. for It investments that are largely competitive necessities, it is advisable to undertake such investments at the same time with industry participants to maintain competitive parity. limitations althOuGh Our StuDy iS theOretically GrOunDeD, it has some limitations. first, we used a cross-sectional data set limiting our ability to uncover dynamic processes and sequential investment strategy, which are needed to contain downside risk. for example, the resolution of uncertainty through learning and business experimentation is a dynamic process that extends, perhaps, through several periods. future research should undertake a longitudinal study to examine the effects investigated in this study. Despite this limitation, we still found considerable support for our hypotheses. Consistent with our theorizing, transformational and informate It investments lead to a reduction in downside risk only if they are made early. for automate It investments, the hypothesized effect of parity timing is supported. ","while the main benefit of using real options analysis stems from the ability to reduce downside risk of investments, most prior research has discussed this generally in relation to managerial flexibility in making the investment.",related_work,2
B2B,"while much of the electronic commerce literature has focused on business - toconsumer ( B2C ) or consumer - to - consumer ( C2C ) issues , business - to - business ( B2B ) commerce has been the dominant player in terms of the volume of money involved .","while much of the electronic commerce literature has focused on business-toconsumer (B2C) or consumer-to-consumer (C2C) issues, business-to-business (B2B) commerce has been the dominant player in terms of the volume of money involved. Yan Dong, Xiaowen huang, Kingshuk K. Sinha, and Kefeng Xu, in ""Collaborative Demand Forecasting: toward the Design of an Exception-Based Forecasting Mechanism,"" analyze an exception-based mechanism in collaborative planning, forecasting, and replenishment (CPFR) to induce truthful sharing of information among supply chain partners. they demonstrate that in the presence of information asymmetry between B2B partners, incentive-based contracts involving revenue-sharing, cost-sharing, transfer payment, and CPFR-specific exception resolution can all be used for truthful information sharing, though with different business outcomes. the authors recommend exercising caution in designing and customizing B2B collaboration mechanisms. ","while much of the electronic commerce literature has focused on business-toconsumer (B2C) or consumer-to-consumer (C2C) issues, business-to-business (B2B) commerce has been the dominant player in terms of the volume of money involved.",related_work,2
"uninsured service web portals are primarily provided to consumers to increase convenience and reduce transaction costs associated with physical service encounters .""","we consider service contingencies to be contingencies associated with the unique nature of the relationship between the ambulatory-care clinic and the patient. while many cases of self-service portals being offered to customers exist-such as instances of online banking portals and e-commerce portals-such self-service web portals are primarily provided to consumers to increase convenience and reduce transaction costs associated with physical service encounters. Ambulatory-care clinics, however, are representative of a class of targeted, localized businesses that cater to a wider variety of customer (patient) needs, ranging from one-time, emergent needs to longer-term repeated coordination of care and relationship building. relationships have been considered in the business-to-business context, especially in supply chain management, where strategic technology adoption can increase provider-supplier value and relationship quality through collaboration and information sharing. For instance, Iacovou et al. [34] found that electronic data interchange (EDI) adoption is more likely between partners who are dependent on each other. This finding suggests that an ongoing relationship where information exchange is needed can motivate adoption of technology designed to streamline the flow of information. In addition, the co-creation of value research stream suggests mutual benefits for firms that embrace the potential value of their consumers (e.g., [45] ), and collaborative efforts are often at the core of health provider and patient relationships. however, to our knowledge, the nature of such lasting relationships between a firm and the firm's core customers has not been identified in other studies as a key predictor of supply-side adoption.",while many cases of self-service portals being offered to customers exist-such as instances of online banking portals and e-commerce portals-such self-service web portals are primarily provided to consumers to increase convenience and reduce transaction costs associated with physical service encounters.,,background_information,3
experiment,"while linguistic analysis has produced raw accuracy rates that exceed 80 percent START_CITE [ 62 ] END_CITE CITE_b51 , the recommendations provided by the linguistic analysis component during this experiment characterized 6 out of the 10 interviews correctly .","Decision aid this work presents a partially functional decision aid called the hybrid-behavioral analysis system (h-BaS), which has two major components. the first component presents linguistic analysis results from message feature mining. while all of the steps in message feature mining are automated, the transcription of the interaction to be analyzed is not. therefore, for this experiment, transcriptions were manually created and message feature mining was then applied. while linguistic analysis has produced raw accuracy rates that exceed 80 percent [62] , the recommendations provided by the linguistic analysis component during this experiment characterized 6 out of the 10 interviews correctly. the decision variable was formed using lexical diversity, verb quantity, affect ratio, group references, and activation as per Zhou et al. [61] . a more detailed description of message feature mining is provided in appendix a.","while linguistic analysis has produced raw accuracy rates that exceed 80 percent [62] , the recommendations provided by the linguistic analysis component during this experiment characterized 6 out of the 10 interviews correctly.",belongs_to_article,1
accuracy rates,"while linguistic analysis has produced raw accuracy rates that exceed 80 percent START_CITE [ 62 ] END_CITE CITE_b51 , the recommendations provided by the linguistic analysis component during this experiment characterized 6 out of the 10 interviews correctly .","Decision aid this work presents a partially functional decision aid called the hybrid-behavioral analysis system (h-BaS), which has two major components. the first component presents linguistic analysis results from message feature mining. while all of the steps in message feature mining are automated, the transcription of the interaction to be analyzed is not. therefore, for this experiment, transcriptions were manually created and message feature mining was then applied. while linguistic analysis has produced raw accuracy rates that exceed 80 percent [62] , the recommendations provided by the linguistic analysis component during this experiment characterized 6 out of the 10 interviews correctly. the decision variable was formed using lexical diversity, verb quantity, affect ratio, group references, and activation as per Zhou et al. [61] . a more detailed description of message feature mining is provided in appendix a.","while linguistic analysis has produced raw accuracy rates that exceed 80 percent [62] , the recommendations provided by the linguistic analysis component during this experiment characterized 6 out of the 10 interviews correctly.",belongs_to_article,1
information sharing,"while information sharing is becoming START_ANNEX START_PARAGRAPH_TAG ubiquitous online phenomenon , how to ensure information quality or induce quality content remains START_ANNEX START_PARAGRAPH_TAG challenge because of the anonymity of commentators .","Online communities provide a social sphere for people to share information and knowledge. while information sharing is becoming a ubiquitous online phenomenon, how to ensure information quality or induce quality content remains a challenge because of the anonymity of commentators. this paper introduces moderation into reputation systems. we show that moderation directly affects strategic commentators' incentive to generate useful information, and moderation is generally desirable to improve information quality. we find that when being moderated with different probabilities based on their reputations, commentators might display a pattern of reputation oscillation, in which they generate useful content to build up high","while information sharing is becoming a ubiquitous online phenomenon, how to ensure information quality or induce quality content remains a challenge because of the anonymity of commentators.",background_information,3
risk management,"while good compliance does not always result in good data protection , good data protection based on risk management leads to good compliance [ 14 ] .","while good compliance does not always result in good data protection, good data protection based on risk management leads to good compliance [14]. Organizations using a risk-based approach can reap the benefits of uniting compliance and risk management in a single solution. Nonetheless, organizations have to describe how whatever they are doing helps meet regulatory requirements",,background_information,3
common method variance,"while formal tests revealed that common method variance was not prevalent , the research would be strengthened by a longitudinal design with a lag between the collection of the dependent and independent variables or through measures of actual information security policy violations obtained from independent sources .","iT is iMporTAnT To consiDer The following liMiTATions To This sTuDy, some of which point to opportunities for future research. The first limitation is the single source for both the dependent and independent variables, which could introduce common method variance. while formal tests revealed that common method variance was not prevalent, the research would be strengthened by a longitudinal design with a lag between the collection of the dependent and independent variables or through measures of actual ISP violations obtained from independent sources.","while formal tests revealed that common method variance was not prevalent, the research would be strengthened by a longitudinal design with a lag between the collection of the dependent and independent variables or through measures of actual ISP violations obtained from independent sources.",related_work,2
innovation,"while focusing on a single innovation helps us to understand issues that are specific to an innovation , it does not account for the complexities of managing the assimilation of multiple innovations , especially when they are moderately complementary .","Despite growing interest, there is limited scientific understanding about EPI assimilation due to four reasons. First, in general, there is scant theory-based research on the assimilation of classes of IT innovations for business processes [13, 67] . Second, past procurement studies have focused on a single innovation (e.g., reverse auctions) and factors specific to it (e.g., buyer-supplier relationships for reverse auctions) [28, 35] . while focusing on a single innovation helps us to understand issues that are specific to an innovation, it does not account for the complexities of managing the assimilation of multiple innovations, especially when they are moderately complementary. Indeed, the assimilation of moderately complementary innovations is much more challenging than the assimilation of those innovations that are unrelated or those that are full complements and must be concurrently adopted because of technical constraints [21]. Third, by focusing on one stage of the assimilation life cycle, such as the decision to adopt a specific innovation, past procurement studies have overlooked the fact that technology assimilation is an ongoing process. Fourth, and finally, a majority of past studies on procurement innovation have been anecdotal [43] .","while focusing on a single innovation helps us to understand issues that are specific to an innovation, it does not account for the complexities of managing the assimilation of multiple innovations, especially when they are moderately complementary.",background_information,3
collaboration technologies,"while early collaboration technology research initiatives were centered on decision room environments START_CITE [ 18 ] END_CITE CITE_b15 , attention has more recently turned to collaboration technologies that support virtual teams and distributed work ( e.g. , e - mail , instant messaging , asynchronous discussion tools ) .","Collaboration technology has been the subject of formal research at least since the 1970s, although its emergence as a key domain of research did not occur until the 1980s [18] . Many reviews of collaboration technology research have been published over the years outlining the development of research and highlighting trends in the empirical results [21, 24, 31, 32] . while early collaboration technology research initiatives were centered on decision room environments [18] , attention has more recently turned to collaboration technologies that support virtual teams and distributed work (e.g., e-mail, instant messaging, asynchronous discussion tools).","while early collaboration technology research initiatives were centered on decision room environments [18] , attention has more recently turned to collaboration technologies that support virtual teams and distributed work (e.g., e-mail, instant messaging, asynchronous discussion tools).",background_information,3
Internet,"while e - sourcing and e - coordination are technologies to improve the procurement process , e - communities are alternate Internet - enabled channels for buyer - supplier exchange that leverage e - sourcing and e - coordination technologies to facilitate this exchange .","Previous reseArch idenTifies Three clAsses of e-procurement: e-sourcing, such as online auctions, online bidding, and online tendering",,background_information,3
hypotheses,"while controlling for select "" dominant paradigm "" characteristics ( e.g. , ambulatorycare clinic size , structure , management support , and competition ) , in the following section , we present specific arguments for our hypotheses related to the impact of demand contingencies , service contingencies , and learning externality contingencies on patient portal adoption by ambulatory ",,,belongs_to_article,1
ERP,"while companies might use an application service provider for small applications or to support peripheral operations , they should not commit to an application service provider version of highly customized , central systems such as Enterprise Resource Planning or CRM ( customer relationship management ) , which are central to the business and would be very difficult to move ' ' START_CITE [ 1 ] END_CITE CITE_b0 Writing a contract Duration Short vs. long .","Easy for customers to duplicate vs. difficult. ''Companies that ultimately build sustainable ASP related businesses should think about offering value-added components to their service that is simultaneously difficult for customers to replicate and customers to replace. In the long run, it might be the proprietary nature of the value-added components that will distance the winters from the loser in the space'' [68] Vendor target market Large firm vs. small. ''One prescription for survival is for ASPs to rethink their model of catering to startups and small businesses. 'ASPs that can't penetrate the Global 2000 won't survive,' says Bob Stimson, a senior analyst with Merrill Lynch'' [30] Selection of application Discreteness of the application High vs. low. ''A good opportunity to fill a specialized niche in a company's portfolio'' [36] {This specialized niche could be a discrete system. } Complexity of application High vs. low. ''SAP was a very bad fit for this company. It's very complex and it was Pandesic's business plan to mask that complexity'' [23] Difficulty of customizing the application High vs. low. Commodity systems can be easily outsourced to an ASP '' . . . Office-On-Line, a Web version of its flagship office applications suite. Small businesses will be able to ''rent'' the suite . . . '' [18] . ''But supply chain and industry apps hosting will have another strike against them-these apps contain complex business rules and unique configurations, making replication across firms virtually impossible'' [2] . ''Standardization and robustness help dictate the choice of software vendors. USi doesn't do a lot of customization because it doesn't want to manage 100 versions of PeopleSoft. Big developers such as PeopleSoft and Siebel have invested millions in building functionality into their products so that they can be configured with a few switches, not thousands of lines of source code'' [58] . ''For companies that want to keep adding new systems and functionality . . . without the ability to integrate new functions, it {ASP} could prove too static to meet the needs of fastgrowing organizations'' [31] Criticality of the application High vs. low. ''I do not think we'd be interested in relinquishing control over mission-critical applications'' [4] . '' . . . while companies might use an ASP for small applications or to support peripheral operations, they should not commit to an ASP version of highly customized, central systems such as ERP or CRM (customer relationship management), which are central to the business and would be very difficult to move'' [1] Writing a contract Duration Short vs. long. ''Contracts generally run between one and three years'' [36] . ''Brassring offers . . . a month to month contract, which lets companies leave whenever they choose without penalty'' [51] Pricing","while companies might use an ASP for small applications or to support peripheral operations, they should not commit to an ASP version of highly customized, central systems such as ERP or CRM (customer relationship management), which are central to the business and would be very difficult to move'' [1] Writing a contract Duration Short vs. long.",background_information,3
B2B,"while busesinss - to - business σ / B2B ) technology solutions , such as Collaborative planning, forecasting, and replenishment σ / collaborative planning , forecasting , and replenishment ) , facilitate the sharing of historical information σ / e.g. , transaction records ) , business intelligence σ / e.g. , potential customer demand ) is considered private .","Abstract. : Sharing of truthful information involving business intelligence between supply chain partners is a challenge on account of the asymmetric nature of the information, where one party possesses information such as market intelligence that is neither available in the public domain nor verifiable through third parties. while busesinss-to-business (B2B) technology solutions, such as CPFR (collaborative planning, forecasting, and replenishment), facilitate the sharing of historical information (e.g., transaction records), business intelligence (e.g., potential customer demand) is considered private. Central to CPFR is collaborative demand forecasting (CDF) that allows supply chain partners to share private demand information and incorporate the jointly derived demand forecast into production planning and product replenishment decisions. Implementing CDF, however, is a challenge because of the high costs of the laborious collaboration effort (e.g., to resolve forecast differences). Hence, companies are unable to realize the benefits of CDF and, in turn, the full potential of CPFR. Typically, the issues of information truthfulness and collaboration cost are addressed through an exception management mechanism that defines a range of forecast updates within which collaboration is automated without any human intervention in B2B trading partners. In this paper, we develop incentive-based contracts that explicitly consider the truth-telling behavior and exception resolution in decisions related to the threshold values of demand information. Our first contribution to B2B information management is in establishing the strategic value of exception management and resolution mechanisms in B2B relationships, leading to truthful revelation of demand information. Our second contribution is in developing exception-based incentive contracts, especially in light of the advances in today's business practices and technology, to address issues associated with unobservable and asymmetric demand information. Specifically, we propose a resolution contract to coordinate the supply chain that directly incorporates both exceptions and resolution in an incentive mechanism. we show that these alternative contracts are all viable solutions in assuring truthful exchange of demand information but excel individually in specific situations and, thus, provide practitioners with alternative demand collaboration tools when price negotiation is not an option.","while busesinss-to-business (B2B) technology solutions, such as CPFR (collaborative planning, forecasting, and replenishment), facilitate the sharing of historical information (e.g., transaction records), business intelligence (e.g., potential customer demand) is considered private.",background_information,3
product reviews,"while acknowledging that product reviews can certainly influence purchase decisions , we adopt a more nuanced approach for understanding the effects of review characteristics on purchase decisions that considers the level of credibility attributed to the reviewer .","In past research concerning online product reviews, researchers have made a theoretical leap from properties of product reviews to purchase decisions with mixed results [15, 22] . while acknowledging that product reviews can certainly influence purchase decisions, we adopt a more nuanced approach for understanding the effects of review characteristics on purchase decisions that considers the level of credibility attributed to the reviewer. Such a conceptualization may assist in moving past previous, sometimes conflicting research findings. It is the additional step of attributing credibility to a reviewer that affects how a product review will influence perceptions and intentions about a product. characteristics of the reviewer and the review may appear to directly influence potential buyer intentions and perceptions, but without considering the effect of credibility, these apparent relationships may be unstable.","while acknowledging that product reviews can certainly influence purchase decisions, we adopt a more nuanced approach for understanding the effects of review characteristics on purchase decisions that considers the level of credibility attributed to the reviewer.",background_information,3
effect sizes,"which we 11 For interesting examples of small effect sizes that tell a "" big story "" and of large effect sizes that do not , see START_CITE Cortina and Landis","As shown in Table 5, the 27 MIS Quarterly papers are evaluated in terms of seven aspects (i.e., seven columns) which we 11 For interesting examples of small effect sizes that tell a ""big story"" and of large effect sizes that do not, see Cortina and Landis (2009) . (2015) is not counted. ","which we 11 For interesting examples of small effect sizes that tell a ""big story"" and of large effect sizes that do not, see Cortina and Landis",related_work,2
database,"which was compiled from two different sources , namely , the membership list of Life Office Management Association ( Life Office Management Association ) , an industry organization , and the Dun & Bradstreet database , consisted of 800 companies comprising the life and health insurance companies operating in North America with over 100 employees .","The questionnaire was administered to managers in the life and health insurance industry during the first quarter of the year 2000. Data was collected for the year 1999. The target respondent list, 13 The only substantive difference was that infrastructure flexibility had a significant and positive impact on customer retention. which was compiled from two different sources, namely, the membership list of Life Office Management Association (LOMA), an industry organization, and the Dun & Bradstreet database, consisted of 800 companies comprising the life and health insurance companies operating in North America with over 100 employees. The questionnaire was divided into two parts: (1) customer service and (2) information systems. Components were to be completed and returned independently by the customer service and the IT manager respectively. 14 Responses from 104 distinct firms were obtained. Responses from both the customer service and IT manager were obtained from 72 firms. This generated a matched sample response rate of 9 percent. This response rate is comparable to other studies with matched surveys from senior executives (Sabherwal and Chan 2001) . A variety of analyses indicated that respondents were representative of the target population, and thus nonresponse bias does not appear to be a problem in the data. 15 Descriptive statistics for the firms in the sample are shown in Table 1. 16","which was compiled from two different sources, namely, the membership list of Life Office Management Association (LOMA), an industry organization, and the Dun & Bradstreet database, consisted of 800 companies comprising the life and health insurance companies operating in North America with over 100 employees.",belongs_to_article,1
100 employees,"which was compiled from two different sources , namely , the membership list of Life Office Management Association ( Life Office Management Association ) , an industry organization , and the Dun & Bradstreet database , consisted of 800 companies comprising the life and health insurance companies operating in North America with over 100 employees .","The questionnaire was administered to managers in the life and health insurance industry during the first quarter of the year 2000. Data was collected for the year 1999. The target respondent list, 13 The only substantive difference was that infrastructure flexibility had a significant and positive impact on customer retention. which was compiled from two different sources, namely, the membership list of Life Office Management Association (LOMA), an industry organization, and the Dun & Bradstreet database, consisted of 800 companies comprising the life and health insurance companies operating in North America with over 100 employees. The questionnaire was divided into two parts: (1) customer service and (2) information systems. Components were to be completed and returned independently by the customer service and the IT manager respectively. 14 Responses from 104 distinct firms were obtained. Responses from both the customer service and IT manager were obtained from 72 firms. This generated a matched sample response rate of 9 percent. This response rate is comparable to other studies with matched surveys from senior executives (Sabherwal and Chan 2001) . A variety of analyses indicated that respondents were representative of the target population, and thus nonresponse bias does not appear to be a problem in the data. 15 Descriptive statistics for the firms in the sample are shown in Table 1. 16","which was compiled from two different sources, namely, the membership list of Life Office Management Association (LOMA), an industry organization, and the Dun & Bradstreet database, consisted of 800 companies comprising the life and health insurance companies operating in North America with over 100 employees.",belongs_to_article,1
North America,"which was compiled from two different sources , namely , the membership list of Life Office Management Association ( Life Office Management Association ) , an industry organization , and the Dun & Bradstreet database , consisted of 800 companies comprising the life and health insurance companies operating in North America with over 100 employees .","The questionnaire was administered to managers in the life and health insurance industry during the first quarter of the year 2000. Data was collected for the year 1999. The target respondent list, 13 The only substantive difference was that infrastructure flexibility had a significant and positive impact on customer retention. which was compiled from two different sources, namely, the membership list of Life Office Management Association (LOMA), an industry organization, and the Dun & Bradstreet database, consisted of 800 companies comprising the life and health insurance companies operating in North America with over 100 employees. The questionnaire was divided into two parts: (1) customer service and (2) information systems. Components were to be completed and returned independently by the customer service and the IT manager respectively. 14 Responses from 104 distinct firms were obtained. Responses from both the customer service and IT manager were obtained from 72 firms. This generated a matched sample response rate of 9 percent. This response rate is comparable to other studies with matched surveys from senior executives (Sabherwal and Chan 2001) . A variety of analyses indicated that respondents were representative of the target population, and thus nonresponse bias does not appear to be a problem in the data. 15 Descriptive statistics for the firms in the sample are shown in Table 1. 16","which was compiled from two different sources, namely, the membership list of Life Office Management Association (LOMA), an industry organization, and the Dun & Bradstreet database, consisted of 800 companies comprising the life and health insurance companies operating in North America with over 100 employees.",background_information,3
e-mail,which they evaluate using Enron 's e - mail database and multiple online discussion forums .,"Overview: Abbasi and Chen's (2008) CyberGate study develops a design framework for text analysis for computer-mediated communication (CMC) systems such as e-mail, discussion forums, and chat. This framework embodies a set of general design principles for a class of systems that support ideational, textual, and interpersonal analysis of computermediated text. The researchers also provide guidelines for the selection of the features and visualization techniques for CMC text analysis systems. The main contribution builds upon the Walls et al. (1992) model for the formulation of an information system design theory (ISDT), with the kernel theory being systemic functional linguistic theory (SFLT) (Halliday 1994 ). Abbasi and Chen instantiate their design theory in a material instance artifact (CyberGate) which they evaluate using Enron's e-mail database and multiple online discussion forums.",which they evaluate using Enron's e-mail database and multiple online discussion forums.,related_work,2
retailers,which suggests that retailers that chose to personalize have above average customer loyalty .,"which suggests that retailers that chose to personalize have above average customer loyalty. This suggests the presence of unobserved characteristics that not only positively influence the selection into the personalization option but also enable customer loyalty that is above average. Now consider Equation (2). This equation suggests that when retailers are randomly assigned to the nopersonalization option, the average customer loyalty is W i 0 . However, when retailers self-select themselves into the no-personalization option, their customer loyalty is given by",which suggests that retailers that chose to personalize have above average customer loyalty.,background_information,3