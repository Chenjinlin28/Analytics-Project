{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Setfit model \n",
    "\n",
    "Training a model on the given dataset. Then deploy it on the manually labeled data to check the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/setfit.git\n",
      "  Cloning https://github.com/huggingface/setfit.git to /private/var/folders/ld/mczz0xmx243grk32lg665c_c0000gn/T/pip-req-build-wj_vyje4\n",
      "  Running command git clone --quiet https://github.com/huggingface/setfit.git /private/var/folders/ld/mczz0xmx243grk32lg665c_c0000gn/T/pip-req-build-wj_vyje4\n",
      "  Resolved https://github.com/huggingface/setfit.git to commit 327a3b64ca38dd688a7f92b67e7bf88f9db66aa9\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: datasets>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from setfit==1.1.0.dev0) (2.19.2)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from setfit==1.1.0.dev0) (3.0.0)\n",
      "Requirement already satisfied: evaluate>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from setfit==1.1.0.dev0) (0.4.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from setfit==1.1.0.dev0) (0.22.2)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from setfit==1.1.0.dev0) (1.3.2)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from setfit==1.1.0.dev0) (23.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets>=2.3.0->setfit==1.1.0.dev0) (3.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets>=2.3.0->setfit==1.1.0.dev0) (1.26.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets>=2.3.0->setfit==1.1.0.dev0) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets>=2.3.0->setfit==1.1.0.dev0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets>=2.3.0->setfit==1.1.0.dev0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets>=2.3.0->setfit==1.1.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.32.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets>=2.3.0->setfit==1.1.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets>=2.3.0->setfit==1.1.0.dev0) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets>=2.3.0->setfit==1.1.0.dev0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets>=2.3.0->setfit==1.1.0.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.3.0->setfit==1.1.0.dev0) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets>=2.3.0->setfit==1.1.0.dev0) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets>=2.3.0->setfit==1.1.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface_hub>=0.13.0->setfit==1.1.0.dev0) (4.8.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers>=2.2.1->setfit==1.1.0.dev0) (4.39.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers>=2.2.1->setfit==1.1.0.dev0) (2.3.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers>=2.2.1->setfit==1.1.0.dev0) (1.11.3)\n",
      "Requirement already satisfied: Pillow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence-transformers>=2.2.1->setfit==1.1.0.dev0) (10.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->setfit==1.1.0.dev0) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->setfit==1.1.0.dev0) (3.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets>=2.3.0->setfit==1.1.0.dev0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets>=2.3.0->setfit==1.1.0.dev0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets>=2.3.0->setfit==1.1.0.dev0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets>=2.3.0->setfit==1.1.0.dev0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets>=2.3.0->setfit==1.1.0.dev0) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.1->datasets>=2.3.0->setfit==1.1.0.dev0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.1->datasets>=2.3.0->setfit==1.1.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.1->datasets>=2.3.0->setfit==1.1.0.dev0) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.32.1->datasets>=2.3.0->setfit==1.1.0.dev0) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.1->setfit==1.1.0.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.1->setfit==1.1.0.dev0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.1->setfit==1.1.0.dev0) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.2.1->setfit==1.1.0.dev0) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.2.1->setfit==1.1.0.dev0) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.2.1->setfit==1.1.0.dev0) (0.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets>=2.3.0->setfit==1.1.0.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets>=2.3.0->setfit==1.1.0.dev0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets>=2.3.0->setfit==1.1.0.dev0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.3.0->setfit==1.1.0.dev0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.1->setfit==1.1.0.dev0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.2.1->setfit==1.1.0.dev0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/huggingface/setfit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments, sample_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "df = pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>entity</th>\n",
       "      <th>ent_id</th>\n",
       "      <th>sentence_original</th>\n",
       "      <th>class_ID</th>\n",
       "      <th>class_name</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence_preprocessed</th>\n",
       "      <th>sentence_stemmed</th>\n",
       "      <th>sentence_lemmas</th>\n",
       "      <th>sentence_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90812</td>\n",
       "      <td>interviews</td>\n",
       "      <td>qualitative interview</td>\n",
       "      <td>Especially for the problem of reduced identity...</td>\n",
       "      <td>1</td>\n",
       "      <td>belongs_to_article</td>\n",
       "      <td>507_6563_6594</td>\n",
       "      <td>especially problem reduced identity efforts vt...</td>\n",
       "      <td>especi problem reduc ident effort vt identifi ...</td>\n",
       "      <td>especially problem reduce identity effort vt i...</td>\n",
       "      <td>['especially', 'problem', 'reduced', 'identity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90770</td>\n",
       "      <td>knowledge management</td>\n",
       "      <td>knowledge management</td>\n",
       "      <td>This makes it challenging for them to organize...</td>\n",
       "      <td>3</td>\n",
       "      <td>background_information</td>\n",
       "      <td>507_4787_4813</td>\n",
       "      <td>makes challenging organize meetings conversati...</td>\n",
       "      <td>make challeng organ meet convers well perform ...</td>\n",
       "      <td>make challenge organize meeting conversation w...</td>\n",
       "      <td>['makes', 'challenging', 'organize', 'meetings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90500</td>\n",
       "      <td>reference model</td>\n",
       "      <td>reference modelling</td>\n",
       "      <td>It can be foreseen that managing such a compre...</td>\n",
       "      <td>3</td>\n",
       "      <td>background_information</td>\n",
       "      <td>505_7541_7567</td>\n",
       "      <td>foreseen managing comprehensive reference mode...</td>\n",
       "      <td>foreseen manag comprehens refer model challeng...</td>\n",
       "      <td>foresee manage comprehensive reference model c...</td>\n",
       "      <td>['fore', '##see', '##n', 'managing', 'comprehe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90417</td>\n",
       "      <td>UML</td>\n",
       "      <td>unified modeling language</td>\n",
       "      <td>In particular, redefinition in UML allows one ...</td>\n",
       "      <td>3</td>\n",
       "      <td>background_information</td>\n",
       "      <td>505_4085_4121</td>\n",
       "      <td>particular redefinition uml allows one modify ...</td>\n",
       "      <td>particular redefinit uml allow on modifi data ...</td>\n",
       "      <td>particular redefinition uml allow one modify d...</td>\n",
       "      <td>['particular', 'red', '##ef', '##ini', '##tion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90361</td>\n",
       "      <td>reference modeling</td>\n",
       "      <td>reference modelling</td>\n",
       "      <td>At the same time, redundancy in a reference mo...</td>\n",
       "      <td>2</td>\n",
       "      <td>related_work</td>\n",
       "      <td>505_2464_2508</td>\n",
       "      <td>time redundancy reference model typically avoi...</td>\n",
       "      <td>time redund refer model typic avoid see e g co...</td>\n",
       "      <td>time redundancy reference model typically avoi...</td>\n",
       "      <td>['time', 'red', '##unda', '##ncy', 'reference'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                entity                     ent_id  \\\n",
       "0  90812            interviews      qualitative interview   \n",
       "1  90770  knowledge management       knowledge management   \n",
       "2  90500       reference model        reference modelling   \n",
       "3  90417                   UML  unified modeling language   \n",
       "4  90361    reference modeling        reference modelling   \n",
       "\n",
       "                                   sentence_original  class_ID  \\\n",
       "0  Especially for the problem of reduced identity...         1   \n",
       "1  This makes it challenging for them to organize...         3   \n",
       "2  It can be foreseen that managing such a compre...         3   \n",
       "3  In particular, redefinition in UML allows one ...         3   \n",
       "4  At the same time, redundancy in a reference mo...         2   \n",
       "\n",
       "               class_name    sentence_id  \\\n",
       "0      belongs_to_article  507_6563_6594   \n",
       "1  background_information  507_4787_4813   \n",
       "2  background_information  505_7541_7567   \n",
       "3  background_information  505_4085_4121   \n",
       "4            related_work  505_2464_2508   \n",
       "\n",
       "                               sentence_preprocessed  \\\n",
       "0  especially problem reduced identity efforts vt...   \n",
       "1  makes challenging organize meetings conversati...   \n",
       "2  foreseen managing comprehensive reference mode...   \n",
       "3  particular redefinition uml allows one modify ...   \n",
       "4  time redundancy reference model typically avoi...   \n",
       "\n",
       "                                    sentence_stemmed  \\\n",
       "0  especi problem reduc ident effort vt identifi ...   \n",
       "1  make challeng organ meet convers well perform ...   \n",
       "2  foreseen manag comprehens refer model challeng...   \n",
       "3  particular redefinit uml allow on modifi data ...   \n",
       "4  time redund refer model typic avoid see e g co...   \n",
       "\n",
       "                                     sentence_lemmas  \\\n",
       "0  especially problem reduce identity effort vt i...   \n",
       "1  make challenge organize meeting conversation w...   \n",
       "2  foresee manage comprehensive reference model c...   \n",
       "3  particular redefinition uml allow one modify d...   \n",
       "4  time redundancy reference model typically avoi...   \n",
       "\n",
       "                                  sentence_tokenized  \n",
       "0  ['especially', 'problem', 'reduced', 'identity...  \n",
       "1  ['makes', 'challenging', 'organize', 'meetings...  \n",
       "2  ['fore', '##see', '##n', 'managing', 'comprehe...  \n",
       "3  ['particular', 'red', '##ef', '##ini', '##tion...  \n",
       "4  ['time', 'red', '##unda', '##ncy', 'reference'...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>sentence_original</th>\n",
       "      <th>class_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>interviews</td>\n",
       "      <td>Especially for the problem of reduced identity...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knowledge management</td>\n",
       "      <td>This makes it challenging for them to organize...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reference model</td>\n",
       "      <td>It can be foreseen that managing such a compre...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UML</td>\n",
       "      <td>In particular, redefinition in UML allows one ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reference modeling</td>\n",
       "      <td>At the same time, redundancy in a reference mo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 entity                                  sentence_original  \\\n",
       "0            interviews  Especially for the problem of reduced identity...   \n",
       "1  knowledge management  This makes it challenging for them to organize...   \n",
       "2       reference model  It can be foreseen that managing such a compre...   \n",
       "3                   UML  In particular, redefinition in UML allows one ...   \n",
       "4    reference modeling  At the same time, redundancy in a reference mo...   \n",
       "\n",
       "   class_ID  \n",
       "0         1  \n",
       "1         3  \n",
       "2         3  \n",
       "3         3  \n",
       "4         2  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract needed columns\n",
    "df = df[['entity', 'sentence_original', 'class_ID']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine entity and sentence into one text column\n",
    "df['text'] = df.apply(lambda row: f\"Entity: {row['entity']}, Sentence: {row['sentence_original']}\", axis=1)\n",
    "\n",
    "# Rename 'class_ID' to 'label' to match SetFit expectations\n",
    "df = df.rename(columns={'class_ID': 'label'})\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the DataFrame to a Dataset object\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['text', 'label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', '__index_level_0__'],\n",
       "    num_rows: 800\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from setfit import SetFitModel, Trainer, sample_dataset\n",
    "# Initialize the SetFit model\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ld/mczz0xmx243grk32lg665c_c0000gn/T/ipykernel_60539/1935454956.py:7: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "  trainer = SetFitTrainer(\n",
      "Map: 100%|██████████| 800/800 [00:00<00:00, 34955.81 examples/s]\n",
      "***** Running training *****\n",
      "  Num unique pairs = 32000\n",
      "  Batch size = 16\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 2000\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.2584, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.2409, 'learning_rate': 5e-06, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.2342, 'learning_rate': 1e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.1495, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.1606, 'learning_rate': 2e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.029, 'learning_rate': 1.9444444444444445e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.1479, 'learning_rate': 1.888888888888889e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0613, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0143, 'learning_rate': 1.7777777777777777e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0618, 'learning_rate': 1.7222222222222224e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0056, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0112, 'learning_rate': 1.6111111111111115e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0016, 'learning_rate': 1.555555555555556e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0002, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0006, 'learning_rate': 1.4444444444444446e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 1.388888888888889e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0002, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0002, 'learning_rate': 1.2777777777777777e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 1.2222222222222224e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 1.1111111111111113e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 1.0555555555555557e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 1e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 9.444444444444445e-06, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 8.888888888888888e-06, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0003, 'learning_rate': 7.77777777777778e-06, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 7.222222222222223e-06, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 6.111111111111112e-06, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 5.555555555555557e-06, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0, 'learning_rate': 5e-06, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0, 'learning_rate': 4.444444444444444e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 3.88888888888889e-06, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 2.7777777777777783e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 2.222222222222222e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0, 'learning_rate': 1.111111111111111e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0, 'learning_rate': 5.555555555555555e-07, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0001, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2000/2000 [31:37<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1898.2832, 'train_samples_per_second': 16.857, 'train_steps_per_second': 1.054, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import SetFitTrainer\n",
    "\n",
    "\n",
    "\n",
    "# Create a Trainer\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    loss_class=CosineSimilarityLoss\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the trained model\n",
    "trainer.model.save_pretrained('/Users/jinlinchen/Documents/Study/HWR Berlin/Semester 2/Analytics Lab/Analytics Project/Dataset part/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'accuracy': 0.885}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ld/mczz0xmx243grk32lg665c_c0000gn/T/ipykernel_60539/2239685966.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"f1\")\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8858547631671969\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.86      0.88        84\n",
      "           2       0.98      0.94      0.96        47\n",
      "           3       0.81      0.88      0.85        69\n",
      "\n",
      "    accuracy                           0.89       200\n",
      "   macro avg       0.90      0.89      0.89       200\n",
      "weighted avg       0.89      0.89      0.89       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from datasets import load_metric\n",
    "\n",
    "# Load the metric\n",
    "metric = load_metric(\"f1\")\n",
    "\n",
    "# Get predictions from the model\n",
    "preds = model(test_dataset[\"text\"])\n",
    "labels = test_dataset[\"label\"]\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_score = metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "print(f\"F1 score: {f1_score['f1']}\")\n",
    "\n",
    "# Define class names \n",
    "class_names = [\"1\", \"2\", \"3\"]  \n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(labels, preds, target_names=class_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Part2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Manual Labeleled Data\n",
    "df_manual = pd.read_csv('/Users/jinlinchen/Documents/Study/HWR Berlin/Semester 2/Analytics Lab/Analytics Project/Database Part/manual_label - consolidated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>sentence</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>sentence_original</th>\n",
       "      <th>label</th>\n",
       "      <th>class_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>retailer</td>\n",
       "      <td>which gives the supplier 's profit as and the ...</td>\n",
       "      <td>which gives the supplier's profit as and the r...</td>\n",
       "      <td>which gives the supplier's profit as and the r...</td>\n",
       "      <td>background_information</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Proposition 1</td>\n",
       "      <td>which gives the supplier 's profit as and the ...</td>\n",
       "      <td>which gives the supplier's profit as and the r...</td>\n",
       "      <td>which gives the supplier's profit as and the r...</td>\n",
       "      <td>belongs_to_article</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Supply-Chain</td>\n",
       "      <td>which gives the supplier 's profit as and the ...</td>\n",
       "      <td>which gives the supplier's profit as and the r...</td>\n",
       "      <td>which gives the supplier's profit as and the r...</td>\n",
       "      <td>background_information</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Petri net</td>\n",
       "      <td>which can be equivalently represented with a P...</td>\n",
       "      <td>which can be equivalently represented with a P...</td>\n",
       "      <td>which can be equivalently represented with a P...</td>\n",
       "      <td>background_information</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business processes</td>\n",
       "      <td>which are sent by business processes accounts ...</td>\n",
       "      <td>A simple example of a synthesis process for IA...</td>\n",
       "      <td>which are sent by business processes accounts ...</td>\n",
       "      <td>background_information</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               entity                                           sentence  \\\n",
       "0            retailer  which gives the supplier 's profit as and the ...   \n",
       "1       Proposition 1  which gives the supplier 's profit as and the ...   \n",
       "2        Supply-Chain  which gives the supplier 's profit as and the ...   \n",
       "3           Petri net  which can be equivalently represented with a P...   \n",
       "4  business processes  which are sent by business processes accounts ...   \n",
       "\n",
       "                                           paragraph  \\\n",
       "0  which gives the supplier's profit as and the r...   \n",
       "1  which gives the supplier's profit as and the r...   \n",
       "2  which gives the supplier's profit as and the r...   \n",
       "3  which can be equivalently represented with a P...   \n",
       "4  A simple example of a synthesis process for IA...   \n",
       "\n",
       "                                   sentence_original                   label  \\\n",
       "0  which gives the supplier's profit as and the r...  background_information   \n",
       "1  which gives the supplier's profit as and the r...      belongs_to_article   \n",
       "2  which gives the supplier's profit as and the r...  background_information   \n",
       "3  which can be equivalently represented with a P...  background_information   \n",
       "4  which are sent by business processes accounts ...  background_information   \n",
       "\n",
       "   class_ID  \n",
       "0         3  \n",
       "1         1  \n",
       "2         3  \n",
       "3         3  \n",
       "4         3  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>sentence_original</th>\n",
       "      <th>class_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>retailer</td>\n",
       "      <td>which gives the supplier's profit as and the r...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Proposition 1</td>\n",
       "      <td>which gives the supplier's profit as and the r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Supply-Chain</td>\n",
       "      <td>which gives the supplier's profit as and the r...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Petri net</td>\n",
       "      <td>which can be equivalently represented with a P...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business processes</td>\n",
       "      <td>which are sent by business processes accounts ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               entity                                  sentence_original  \\\n",
       "0            retailer  which gives the supplier's profit as and the r...   \n",
       "1       Proposition 1  which gives the supplier's profit as and the r...   \n",
       "2        Supply-Chain  which gives the supplier's profit as and the r...   \n",
       "3           Petri net  which can be equivalently represented with a P...   \n",
       "4  business processes  which are sent by business processes accounts ...   \n",
       "\n",
       "   class_ID  \n",
       "0         3  \n",
       "1         1  \n",
       "2         3  \n",
       "3         3  \n",
       "4         3  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual = df_manual[['entity', 'sentence_original', 'class_ID']]\n",
    "df_manual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine entity and sentence into one text column\n",
    "df_manual['text'] = df_manual.apply(lambda row: f\"Entity: {row['entity']}, Sentence: {row['sentence_original']}\", axis=1)\n",
    "labeled_text = df_manual['text'].tolist()\n",
    "# Rename 'class_ID' to 'label' to match SetFit expectations\n",
    "df_manual = df_manual.rename(columns={'class_ID': 'label'})\n",
    "manual_labels = df_manual['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the labeled dataset\n",
    "predicted_labels = trainer.model.predict(labeled_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5889328063241107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(manual_labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
